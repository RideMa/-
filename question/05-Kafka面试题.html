<!DOCTYPE html>
<html>
 <head>
 </head>
 <body>
  <meta content="relax" name="paragraphSpacing"/>
  <blockquote class="lake-alert lake-alert-info">
   <p style="text-align: center">
    <strong>
     整理作者：蓦然                                     知识星球、公众号（同名）：旧时光大数据
    </strong>
   </p>
  </blockquote>
  <h3>
   介绍下Kafka，Kafka的作用？Kafka的组件？适用场景？
  </h3>
  <p>
   可回答：1）Kafka基本原理；2）Kafka你知道的东西；3）Kafka的优点；4）Kafka的特点
  </p>
  <p>
   问过的一些公司：阿里云(2022.10)(2019.03)，招银网络(2022.09)，贝壳(2022.09)，奇瑞(2022.09)，腾讯(2022.08)，科大讯飞(2022.08)，长江存储(2022.08)，思科cisco(2021.08)，海康威视(2021.08)，字节(2021.08)(2019.09)，58同城(2021.08)，大华(2021.07)，Shopee(2021.07)，阿里社招(2021.03)，腾讯TEG实习(2021.03)，字节日常实习(2020.11)，小米(2020.10)(2020.09)x2，快手提前批(2020.09)，跟谁学(2020.09)，转转(2020.09)，昆仑万维提前批(2020.08)，京东(2020.08)，网易(2020.08)，京东提前批(2020.07)，阿里(2020.04)，阿里新零售实习(2020.03)，蘑菇街实习(2020.03)，阿里淘系(2019.11)，美团(2019.11)，猿辅导(2019.10)，马蜂窝(2019.10)，深信服(2019.10)，招银网络(2019.09)，趋势科技(2019.08)，滴滴(2019.04)，乐言科技(2019.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka是一种分布式、高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据，主要应用于大数据实时处理领域。简单地说，Kafka就相比是一个邮箱，生产者是发送邮件的人，消费者是接收邮件的人，Kafka就是用来存东西的，只不过它提供了一些处理邮件的机制。
  </p>
  <p>
   <strong>
    1、作用
   </strong>
  </p>
  <p>
   1）发布和订阅消息流
  </p>
  <p>
   2）以容错的方式记录消息流，Kafka以文件的方式来存储消息流
  </p>
  <p>
   3）实时处理消息流
  </p>
  <p>
   <strong>
    2、优缺点
   </strong>
  </p>
  <p>
   <strong>
    1）优点
   </strong>
  </p>
  <p>
   <strong>
    高吞吐量、低延迟：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    Kafka 可解耦数据流，因此延迟非常低，速度极快。即使在非常廉价的机器上，Kafka也能做到每秒处理几十万条消息，而它的延迟最低只有几毫秒。
   </span>
  </p>
  <p>
   <strong>
    可扩展性：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    Kafka 集群支持热扩展。Kafka 的分区日志模型允许跨多个服务器分发数据，使其可扩展性超越了在单服务器上应用的情况。
   </span>
  </p>
  <p>
   <strong>
    持久性、可靠性：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    分区可以跨多个服务器分发和和复制，数据全都写入到磁盘。这有助于防止服务器发生故障，使数据获得出色的容错能力和耐久性。
   </span>
  </p>
  <p>
   <strong>
    容错性：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    允许集群中节点故障（若副本数量为n，则允许n-1个节点故障）。
   </span>
  </p>
  <p>
   <strong>
    高并发：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    支持数千个客户端同时读写。
   </span>
  </p>
  <p>
   <strong>
    2）缺点
   </strong>
  </p>
  <p>
   <strong>
    无法弹性扩容：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    对partition的读写都在partition leader所在的broker，如果该broker压力过大，也无法通过新增broker来解决问题；
   </span>
  </p>
  <p>
   <strong>
    扩容成本高：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    集群中新增的broker只会处理新topic，如果要分担老topic-partition的压力，需要手动迁移partition，这时会占用大量集群带宽；
   </span>
  </p>
  <p>
   <strong>
    消费者新加入和退出会造成整个消费组rebalance：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    导致数据重复消费，影响消费速度，增加e2e延迟；
   </span>
  </p>
  <p>
   <strong>
    partition过多会使得性能显著下降：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    Zookeeper压力大，broker上partition过多让磁盘顺序写几乎退化成随机写。
   </span>
  </p>
  <p>
   <strong>
    3、组件
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136271845-88fc9cad-c844-4d64-b4ca-87dcb68eec71.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_13%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   Topic
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：可以理解为一个队列，生产者和消费者面向的都是一个 topic；
   </span>
  </p>
  <p>
   Producer
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：消息生产者，就是向 kafka broker 发消息的客户端；
   </span>
  </p>
  <p>
   Consumer
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：消息消费者，向 kafka broker 取消息的客户端；
   </span>
  </p>
  <p>
   Broker
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：一台 kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker可以容纳多个 topic。
   </span>
  </p>
  <p>
   <strong>
    4、使用场景
   </strong>
  </p>
  <p>
   1）在系统或应用程序之间构建可靠的用于传输实时数据的管道，消息队列功能
  </p>
  <p>
   2）构建实时的流数据处理程序来变换或处理数据流，数据处理功能
  </p>
  <p>
   通俗一点来说
  </p>
  <p>
   <strong>
    日志收集：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer；
   </span>
  </p>
  <p>
   <strong>
    消息系统：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    解耦生产者和消费者、缓存消息等；
   </span>
  </p>
  <p>
   <strong>
    用户活动跟踪：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后消费者通过订阅这些topic来做实时的监控分析，亦可保存到数据库；
   </span>
  </p>
  <p>
   <strong>
    运营指标：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告；
   </span>
  </p>
  <p>
   <strong>
    流式处理：
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    比如Spark streaming和Flink。
   </span>
  </p>
  <h3>
   为什么使用Kafka
  </h3>
  <p>
   问过的一些公司：蘑菇街(2020.04)，美团日常实习(2020.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   缓冲和削峰：上游数据时有突发流量，下游可能扛不住，或者下游没有足够多的机器去保证冗余 （数据冗余是指同一个数据在系统中多次重复出现）。Kafka可以起到一个缓冲的作用，把消息暂存到Kafka中，下游服务就可以按照自己的节奏慢慢处理。
  </p>
  <p>
   解耦和扩展性：项目开始时，不确定具体的需求，消息队列可以作为一个接口层，解耦重要业务流程，只需要遵守约定，针对数据编程即可获取扩展能力。
  </p>
  <p>
   冗余：采用一对多方式，一个生产者生产消息，可以被多个订阅topic的消费服务到，供多个毫无关联的业务应用。
  </p>
  <p>
   健壮性：消息队列可以堆积请求，所以消费者端业务即使短时间死掉，也不会影响主要业务的正常运行。
  </p>
  <p>
   异步通信：很多时候，用户也不需要立即处理消息，消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不想立即处理它，想在队列放多少就放多少，然后在需要的时候，再去处理他们。
  </p>
  <h3>
   说下Kafka架构
  </h3>
  <p>
   可回答：1）Kafka的相关结构；2）Kafka的组成部分
  </p>
  <p>
   问过的一些公司：字节(2022.11)(2022.08)，多益(2022.11)x2，百度提前批(2022.09)，soul(2021.09)，触宝(2021.07)，转转(2020.09)，美团(2020.09)(2020.08)，昆仑万维提前批(2020.08)，阿里淘系(2020.08)(2019.11)，京东提前批(2020.07)，阿里云(2020.07)，阿里新零售实习(2020.03)，网易有道(2019.08)，网易严选(2019.08)，今日头条(2018.12)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka基础架构
  </p>
  <ul list="uc3a9448e">
   <li fid="u5493e583">
    为方便扩展，并提高吞吐量，一个topic分为多个partition
   </li>
   <li fid="u5493e583">
    配合分区的设计，提出消费者组的概念，组内每个消费者并行消费
   </li>
   <li fid="u5493e583">
    为提高可用性，为每个partition增加若干副本，类似NameNode HA
    <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136272637-5ef4068f-0609-4355-a289-ee63089cdef6.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_29%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
   </li>
  </ul>
  <p>
   1）Producer：消息生产者，负责向Broker发送消息
  </p>
  <p>
   2）Consumer：消息消费者，从负责Broker读取并消费消息
  </p>
  <p>
   3）Consumer Group（CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。
  </p>
  <p>
   4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。
  </p>
  <p>
   5）Topic ：主题，与RocketMQ的Topic类似，使用Topic对消息进行分类，Kafka接收到的每条消息都会放入到一个Topic中。
  </p>
  <ul list="u0c79d4c1">
   <li fid="ua9c25c67">
    Topic代表发布和消费记录的端点。生产者向主题发布消息，消费者订阅主题进行消费消息；
   </li>
   <li fid="ua9c25c67">
    每条记录有一个键，一个值，一个时间戳和一些元数据组成；
   </li>
   <li fid="ua9c25c67">
    在未指定分区的情况下发布消息时，将使用键的散列选择分区。
   </li>
  </ul>
  <p>
   6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，对Topic的数据进行分布式存储的最小单位。；
  </p>
  <p>
   7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个Follower。
  </p>
  <p>
   8）Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。
  </p>
  <p>
   9）Follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个Follower会成为新的leader。
  </p>
  <h3>
   Kafka的工作原理？
  </h3>
  <p>
   可回答：1）Kafka的工作流程
  </p>
  <p>
   问过的一些公司：字节(2022.03)，字节社招(2021.05)，网易(2021.05)，美团实习(2021.04)，腾讯(2021.04)，腾讯社招(2020.09)，京东(2020.08)，阿里云(2020.07)，七牛云实习(2020.04)，今日头条(2018.12)
  </p>
  <p>
   <strong>
    先来看个简单版本的
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136276414-41a8d398-7fdd-4148-86f5-f6e0fd1cd913.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_28%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。
  </p>
  <p>
   topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。
  </p>
  <p>
   <strong>
    再来看个详细版本的
   </strong>
  </p>
  <p>
   关于Kafka的架构就不多说了，前面有相关介绍。
  </p>
  <p>
   <strong>
    1、发送数据
   </strong>
  </p>
  <p>
   Producer是生产者，是数据的入口。注意看图中的箭头，Producer 在写入数据的时候永远在找 Leader，不会直接将数据写入 Follower。
  </p>
  <p>
   关于Leader的寻找以及写入流程可以参考下图：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136276723-553763d0-2ac1-4f69-9e95-c1997daa0f38.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_29%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   消息写入Leader后，Follower是主动的去找Leader进行同步的！
  </p>
  <p>
   Producer 采用 Push 模式将数据发布到 Broker，每条消息追加到分区中，顺序写入磁盘，所以保证同一分区内的数据是有序的。
  </p>
  <p>
   写入示意图如下：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136272633-c5bfc936-d82c-40d8-b6da-dc4aa8fda1f3.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_25%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   上面说到数据会写入到不同的分区，那 Kafka 为什么要做分区呢？
  </p>
  <p>
   分区的主要目的是：
  </p>
  <ul list="uf75098b7">
   <li fid="ub82530b6">
    方便扩展。因为一个 Topic 可以有多个 Partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。
   </li>
   <li fid="ub82530b6">
    提高并发。以 Partition 为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。
   </li>
  </ul>
  <p>
   熟悉负载均衡的应该知道，当我们向某个服务器发送请求的时候，服务端可能会对请求做一个负载，将流量分发到不同的服务器。
  </p>
  <p>
   那在 Kafka 中，如果某个 Topic 有多个 Partition，Producer 又怎么知道该将数据发往哪个 Partition 呢？
  </p>
  <p>
   Kafka 中有几个原则：
  </p>
  <ul list="u26e3f602">
   <li fid="u175517d5">
    Partition 在写入的时候可以指定需要写入的 Partition，如果有指定，则写入对应的 Partition。
   </li>
   <li fid="u175517d5">
    如果没有指定 Partition，但是设置了数据的 Key，则会根据 Key 的值 Hash 出一个 Partition。
   </li>
   <li fid="u175517d5">
    如果既没指定 Partition，又没有设置 Key，则会轮询选出一个 Partition。
   </li>
  </ul>
  <p>
   保证消息不丢失是一个消息队列中间件的基本保证，那 Producer 在向 Kafka 写入消息的时候，怎么保证消息不丢失呢？
  </p>
  <p>
   其实上面的写入流程图中有描述出来，那就是通过 ACK 应答机制！在生产者向队列写入数据的时候可以设置参数来确定是否确认 Kafka 接收到数据，这个参数可设置的值为 0、1、all：
  </p>
  <ul list="u878068f4">
   <li fid="u07551e9d">
    0 代表 Producer 往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。
   </li>
   <li fid="u07551e9d">
    1 代表 Producer 往集群发送数据只要 Leader 应答就可以发送下一条，只确保 Leader 发送成功。
   </li>
   <li fid="u07551e9d">
    all 代表 Producer 往集群发送数据需要所有的 Follower 都完成从 Leader 的同步才会发送下一条，确保 Leader 发送成功和所有的副本都完成备份。安全性最高，但是效率最低。
   </li>
  </ul>
  <p>
   最后要注意的是，如果往不存在的 Topic 写数据，能不能写入成功呢？Kafka 会自动创建 Topic，分区和副本的数量根据默认配置都是 1。
  </p>
  <p>
   <strong>
    2、保存数据
   </strong>
  </p>
  <p>
   Producer 将数据写入 Kafka 后，集群就需要对数据进行保存了！Kafka 将数据保存在磁盘，可能在我们的一般的认知里，写入磁盘是比较耗时的操作，不适合这种高并发的组件。Kafka 初始会单独开辟一块磁盘空间，顺序写入数据（效率比随机写入高）。
  </p>
  <p>
   1）Partition结构
  </p>
  <p>
   前面说过了每个 Topic 都可以分为一个或多个 Partition，如果你觉得 Topic 比较抽象，那 Partition 就是比较具体的东西了！
  </p>
  <p>
   Partition 在服务器上的表现形式就是一个一个的文件夹，每个 Partition 的文件夹下面会有多组 Segment 文件。
  </p>
  <p>
   每组 Segment 文件又包含 .index 文件、.log 文件、.timeindex 文件（早期版本中没有）三个文件。
  </p>
  <p>
   Log 文件就是实际存储 Message 的地方，而 Index 和 Timeindex 文件为索引文件，用于检索消息。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136276617-ef11e735-3866-4796-a225-c6f700cc2532.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_12%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   如上图，这个 Partition 有三组 Segment 文件，每个 Log 文件的大小是一样的，但是存储的 Message 数量是不一定相等的（每条的 Message 大小不一致）。
  </p>
  <p>
   文件的命名是以该 Segment 最小 Offset 来命名的，如 000.index 存储 Offset 为 0~368795 的消息，Kafka 就是利用分段+索引的方式来解决查找效率的问题。
  </p>
  <p>
   2）Message结构
  </p>
  <p>
   上面说到 Log 文件就实际是存储 Message 的地方，我们在 Producer 往 Kafka 写入的也是一条一条的 Message。
  </p>
  <p>
   那存储在 Log 中的 Message 是什么样子的呢？消息主要包含消息体、消息大小、Offset、压缩类型……等等！
  </p>
  <p>
   我们重点需要知道的是下面三个：
  </p>
  <ul list="ubb6a5318">
   <li fid="uc8751ac5">
    Offset：Offset 是一个占 8byte 的有序 id 号，它可以唯一确定每条消息在 Parition 内的位置！
   </li>
   <li fid="uc8751ac5">
    消息大小：消息大小占用 4byte，用于描述消息的大小。
   </li>
   <li fid="uc8751ac5">
    消息体：消息体存放的是实际的消息数据（被压缩过），占用的空间根据具体的消息而不一样。
   </li>
  </ul>
  <p>
   3）存储策略
  </p>
  <p>
   无论消息是否被消费，Kafka 都会保存所有的消息。那对于旧数据有什么删除策略呢？
  </p>
  <ul list="u11dcfcc0">
   <li fid="ucd162b9b">
    基于时间，默认配置是 168 小时（7 天）。
   </li>
   <li fid="ucd162b9b">
    基于大小，默认配置是 1073741824。
   </li>
  </ul>
  <p>
   需要注意的是，Kafka 读取特定消息的时间复杂度是 O(1)，所以这里删除过期的文件并不会提高 Kafka 的性能！
  </p>
  <p>
   <strong>
    3、消费数据
   </strong>
  </p>
  <p>
   消息存储在 Log 文件后，消费者就可以进行消费了。Kafka 采用的是点对点的模式，消费者主动的去 Kafka 集群拉取消息，与 Producer 相同的是，消费者在拉取消息的时候也是找 Leader 去拉取。多个消费者可以组成一个消费者组（Consumer Group），每个消费者组都有一个组 id！
  </p>
  <p>
   同一个消费组者的消费者可以消费同一 Topic 下不同分区的数据，但是不会组内多个消费者消费同一分区的数据！
  </p>
  <p>
   详情如下图：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136276887-06b9e5f3-f0f9-46a3-8326-df86ac9e1d6c.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_27%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   图示是消费者组内的消费者小于 Partition 数量的情况，所以会出现某个消费者消费多个 Partition 数据的情况，消费的速度也就不及只处理一个 Partition 的消费者的处理速度！
  </p>
  <p>
   如果是消费者组的消费者多于 Partition 的数量，那会不会出现多个消费者消费同一个 Partition 的数据呢？
  </p>
  <p>
   上面已经提到过不会出现这种情况！多出来的消费者不消费任何 Partition 的数据。
  </p>
  <p>
   所以在实际的应用中，建议消费者组的 Consumer 的数量与 Partition 的数量一致！
  </p>
  <p>
   在保存数据的小节里面，我们聊到了 Partition 划分为多组 Segment，每个 Segment 又包含.log、.index、.timeindex 文件，存放的每条 Message 包含 Offset、消息大小、消息体……
  </p>
  <p>
   <strong>
    前面多次提到 Segment 和 Offset，查找消息的时候是怎么利用 Segment+Offset 配合查找的呢？
   </strong>
  </p>
  <p>
   假如现在需要查找一个 Offset 为 368801 的 Message 是什么样的过程呢？我们先看看下面的图：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136277043-2534cac2-d439-4df6-847c-9053d8bdb254.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_31%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   ① 先找到 Offset 的 368801message 所在的 Segment 文件（利用二分法查找），这里找到的就是在第二个 Segment 文件。
  </p>
  <p>
   ② 打开找到的 Segment 中的 .index 文件（也就是 368796.index 文件，该文件起始偏移量为 368796+1。
  </p>
  <p>
   我们要查找的 Offset 为 368801 的 Message 在该 Index 内的偏移量为 368796+5=368801，所以这里要查找的相对 Offset 为 5）。
  </p>
  <p>
   由于该文件采用的是稀疏索引的方式存储着相对 Offset 及对应 Message 物理偏移量的关系，所以直接找相对 Offset 为 5 的索引找不到。
  </p>
  <p>
   这里同样利用二分法查找相对 Offset 小于或者等于指定的相对 Offset 的索引条目中最大的那个相对 Offset，所以找到的是相对 Offset 为 4 的这个索引。
  </p>
  <p>
   ③ 根据找到的相对 Offset 为 4 的索引确定 Message 存储的物理偏移位置为 256。
  </p>
  <p>
   打开数据文件，从位置为 256 的那个地方开始顺序扫描直到找到 Offset 为 368801 的那条 Message。
  </p>
  <p>
   这套机制是建立在 Offset 为有序的基础上，利用 Segment+有序 Offset+稀疏索引+二分查找+顺序查找等多种手段来高效的查找数据！
  </p>
  <p>
   至此，消费者就能拿到需要处理的数据进行处理了。那
   <strong>
    每个消费者又是怎么记录自己消费的位置
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    呢？
   </span>
  </p>
  <p>
   在早期的版本中，消费者将消费到的 Offset 维护在 Zookeeper 中，Consumer 每间隔一段时间上报一次，这里容易导致重复消费，且性能不好！
  </p>
  <p>
   在新的版本中消费者消费到的 Offset 已经直接维护在 Kafka 集群的 __consumer_offsets 这个 Topic 中！
  </p>
  <h3>
   Kafka实现高吞吐的原理？
  </h3>
  <p>
   可回答：1）Kafka高吞吐的原因；2）Kafka如何保证高吞吐量；3）Kafka为什么低延迟高吞吐？有哪些特点？4）Kafka为什么高可用、高吞吐？如何保证高可用？5）Kafka高性能的实现机制？6）Kafka高性能的原因？7）Kafka零拷贝的实现原理。8）Kafka的数据存储在磁盘但是为什么速度依旧很快？9）Kafka为什么那么快；10）Kafka如何高效读写数据；11）Kafka读也很快，怎么实现的；12）什么是零拷贝，什么是顺序读写，为什么比随机读写快；13）Kafka的数据存储在磁盘但是为什么速度依旧很快？
  </p>
  <p>
   问过的一些公司：Lazada(2022.11)，阿里云(2022.10)，顺丰(2022.09)，OPPO(2022.09)，大疆(2022.09)，字节(2022.08)，OPPO(2022.07)，字节(2022.06)(2020.08)，海康威视(2021.08)x2，远景智能(2021.08)，陌陌(2021.07)x2，网易(2021.05)，腾讯PCG实习(2021.04)，农行广州(2020.09)，转转(2020.09)，猿辅导(2020.09)，滴滴(2020.09)，有赞(2020.08)x2，贝壳(2020.08)，京东(2020.08)x2，小鹏汽车(2020.07)，京东提前批(2020.07)，字节提前批(2020.06)，腾讯CSIG(2020.03)，美团日常实习(2020.03)，阿里新零售实习(2020.03)，蘑菇街实习(2020.03)，腾讯互娱(2020.03)，美团点评(2019.10)，网易严选(2019.08)，阿里(2018.09)，网易考拉(2018.09)，美团新到店(2018.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka 是分布式消息系统，需要处理海量的消息，Kafka 的设计是把所有的消息都写入速度低容量大的硬盘，以此来换取更强的存储能力，但实际上，使用硬盘并没有带来过多的性能损失。
  </p>
  <p>
   kafka 主要使用了以下几个方式实现了超高的吞吐率
  </p>
  <p>
   <strong>
    1）顺序读写
   </strong>
  </p>
  <p>
   kafka 的消息是不断追加到文件中的，这个特性使 kafka 可以充分利用磁盘的顺序读写性能，顺序读写不需要硬盘磁头的寻道时间，只需很少的扇区旋转时间，所以速度远快于随机读写。
  </p>
  <p>
   Kafka 官方给出了测试数据(Raid-5，7200rpm)：
  </p>
  <ul list="uc34cfb4d">
   <li fid="ub204a149">
    顺序 I/O：600MB/s
   </li>
   <li fid="ub204a149">
    随机 I/O：100KB/s
   </li>
  </ul>
  <p>
   <strong>
    2）零拷贝
   </strong>
  </p>
  <p>
   先简单了解下文件系统的操作流程，例如一个程序要把文件内容发送到网络。
  </p>
  <p>
   这个程序是工作在用户空间，文件和网络socket属于硬件资源，两者之间有一个内核空间。
  </p>
  <p>
   在操作系统内部，整个过程为：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136278391-9bf3ae7c-e66b-4df3-a005-402f53566aa1.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_17%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   在Linux kernel2.2之后出现了一种叫做"零拷贝(zero-copy)"系统调用机制，就是跳过“用户缓冲区”的拷贝，建立一个磁盘空间和内存的直接映射，数据不再复制到“用户态缓冲区”。
  </p>
  <p>
   系统上下文切换减少为2次，可以提升一倍的性能
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136278370-cf8f16e9-d034-47c6-997d-fb01887ca3f8.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_17%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    3）文件分段
   </strong>
  </p>
  <p>
   kafka 的队列topic被分为了多个区partition，每个partition又分为多个段segment，所以一个队列中的消息实际上是保存在N多个片段文件中。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136279447-20f3ecd7-35a2-43d6-8664-34973f000cd6.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_20%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   通过分段的方式，每次文件操作都是对一个小文件的操作，非常轻便，同时也增加了并行处理能力。
  </p>
  <p>
   <strong>
    4）批量发送
   </strong>
  </p>
  <p>
   Kafka 允许进行批量发送消息，先将消息缓存在内存中，然后一次请求批量发送出去，比如可以指定缓存的消息达到某个量的时候就发出去，或者缓存了固定的时间后就发送出去，如100条消息就发送，或者每5秒发送一次，这种策略将大大减少服务端的I/O次数。
  </p>
  <p>
   <strong>
    5）数据压缩
   </strong>
  </p>
  <p>
   Kafka 还支持对消息集合进行压缩，Producer可以通过GZIP或Snappy格式对消息集合进行压缩，压缩的好处就是减少传输的数据量，减轻对网络传输的压力，Producer压缩之后，在Consumer需进行解压，虽然增加了CPU的工作，但在对大数据处理上，瓶颈在网络上而不是CPU，所以这个成本很值得。
  </p>
  <h3>
   怎么提高Kafka消息发送的吞吐量
  </h3>
  <p>
   问过的一些公司：星环科技(2022.10)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   batch.size
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：适当加大，增加一次发送消息的大小。如果设置得太小，因为生产者需要更频繁地发送消息，会增加一些额外的开销。如果batch太大，会导致一条消息需要等待很久才能被发送出去，而且会让内存缓冲区有很大压力，过多数据缓冲在内存里。
   </span>
  </p>
  <p>
   buffer.memeory
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：适当加大，防止sender阻塞。该参数用来设置生产者内存缓冲区的大小，生产者用它缓冲要发送到服务器的消息。如果应用程序发送消息的速度超过发送到服务器的速度，会导致生产者空间不足。
   </span>
  </p>
  <p>
   compression.type
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：默认情况下，消息发送时不会被压缩。该参数可以设置为 snappy 、 gzp 或 lz4 ，它指定了消息被发送给 broker 之前使用哪一种压缩算法进行压缩，降低了消息的size，也能增加一次发送消息的数量。
   </span>
  </p>
  <p>
   linger.ms
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：等待时间，默认为0，修改为5-100ms。或者说设置短一点。
   </span>
  </p>
  <h3>
   Kafka如何保证消息顺序性？
  </h3>
  <p>
   可回答：1）Kafka怎么保证数据顺序读；2）Kafka生产者写入怎么保证有序；3）Kafka中Partition如何保证有序
  </p>
  <p>
   问过的一些公司：阿里云(2022.10)，顺丰(2022.09)，大疆(2022.09)，字节(2022.08)，重庆富民银行(2021.09)，多益(2020.11)，蘑菇街(2020.04)，快手(2020.03)x2，中邮消费金融科技(2020.03)，安恒信息(2020.03)，360社招(2020.01)，快手(2019.11)，京东(2019.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   要想实现消息有序，需要从Producer和Consumer两方面来考虑。在Kafka中Partition是真正保存消息的地方，发送的消息都存放在这里。Partition又存在于Topic中，并且一个Topic可以指定多个Partition。
  </p>
  <p>
   针对消息有序的业务需求，还分为全局有序和局部有序。
  </p>
  <ul list="u284c47b3">
   <li fid="ud2b19c3f">
    全局有序：一个Topic下的所有消息都需要按照生产顺序消费。
   </li>
   <li fid="ud2b19c3f">
    局部有序：一个Topic下的消息，只需要满足同一业务字段的要按照生产顺序消费。例如：Topic消息是订单的流水表，包含订单orderId，业务要求同一个orderId的消息需要按照生产顺序进行消费。
   </li>
  </ul>
  <p>
   在Kafka中，只保证Partition内有序，不保证Topic所有分区都是有序的。
  </p>
  <p>
   <strong>
    Kafka 要保证消息的消费顺序，可以有2种方法：
   </strong>
  </p>
  <p>
   <strong>
    1、实现全局有序
   </strong>
  </p>
  <p>
   <strong>
    1个Topic只创建（对应）1个Partition，这样生产者的所有数据都发送到了一个Partition，保证了消息的消费顺序。
   </strong>
  </p>
  <p>
   由于Kafka的一个Topic可以分为了多个Partition，Producer发送消息的时候，是分散在不同 Partition的。当Producer按顺序发消息给Broker，但进入Kafka之后，这些消息就不一定进到哪个Partition，会导致顺序是乱的。
  </p>
  <p>
   因此要满足全局有序，需要1个Topic只能对应1个Partition。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136279458-5b903abe-2b4c-411a-ad3a-36b853a88775.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_17%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   而且对应的consumer也要使用单线程或者保证消费顺序的线程模型，否则会出现下图所示，消费端造成的消费乱序。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136279470-e15fb35b-04a7-4cc7-86c6-ed0eff433ee8.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_18%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    2、实现局部有序
   </strong>
  </p>
  <p>
   <strong>
    生产者在发送消息的时候指定要发送到哪个Partition。
   </strong>
  </p>
  <p>
   要满足局部有序，只需要在发消息的时候指定Partition Key，Kafka对其进行Hash计算，根据计算结果决定放入哪个Partition。这样Partition Key相同的消息会放在同一个Partition。此时，Partition的数量仍然可以设置多个，提升Topic的整体吞吐量。
  </p>
  <p>
   如下图所示，在不增加partition数量的情况下想提高消费速度，可以考虑再次hash唯一标识（例如订单orderId）到不同的线程上，多个消费者线程并发处理消息（依旧可以保证局部有序）。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136279619-75ad8c58-c0f4-4356-ab22-917e11fde9d6.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_18%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <h3>
   指定partition的话，如果发生了数据倾斜，一个key的数据全发到了一个partition会出现什么问题
  </h3>
  <p>
   注意：可以接着“Kafka如何实现消息的有序消费？”这个问题的后面问到了回答
  </p>
  <p>
   问过的一些公司：字节(2022.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   partition倾斜就是热点倾斜，会导致消息堆积，可以通过hash算法或者亲缘性线程池去解决，但是亲缘性线程池也有热点倾斜问题，最本质的方式是对每个message的hash处理。
  </p>
  <h3>
   Kafka Producer Batch
  </h3>
  <p>
   问过的一些公司：星环科技(2022.10)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka Producer Batch是指Kafka生产者在向Broker发送消息时可以一次性发送多条消息，减少网络传输的开销和提高消息发送的效率。
  </p>
  <p>
   在新建ProducerBatch时需要评估这条消息的大小是否超过batch.size，如果不超过，就以batch.size的大小来创建这个ProducerBatch，这样在使用完后还可以通过BufferPool的管理进行复用。若果超过，则以消息的大小来创建ProducerBatch，此内存区域不会被复用。
  </p>
  <h3>
   Kafka分区策略
  </h3>
  <p>
   可回答：1）partition分区策略有几种
  </p>
  <p>
   问过的一些公司：星环科技(2022.10)，远景智能(2021.08)，小米(2020.08)，阿里云(2019.03)，今日头条(2018.12)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   对于消费者来说，一个consumer group中有多个consumer，一个 topic有多个partition，所以肯定会涉及到partition的分配问题，即确定每个partition由哪个consumer来消费，这就是分区分配策略（Partition Assignment Strategy）。
  </p>
  <p>
   Kafka有三种分配策略，一是RoundRobin，一是Range。最新还有一个StickyAssignor策略。
  </p>
  <p>
   <strong>
    1、分配分区的前提条件
   </strong>
  </p>
  <p>
   首先Kafka设定了默认的消费逻辑：一个分区只能被同一个消费组（ConsumerGroup）内的一个消费者消费。
  </p>
  <p>
   在这个消费逻辑设定下，假设目前某消费组内只有一个消费者C0，订阅了一个topic，这个topic包含6个分区，也就是说这个消费者C0订阅了6个分区，这时候可能会发生下列三种情况：
  </p>
  <ul list="u5e44c90f">
   <li fid="u8eb6a06c">
    如果这时候消费者组内
    <strong>
     新增
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     了一个
    </span>
    <strong>
     消费者
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     C1，这个时候就需要把之前分配给C0的6个分区拿出来3个分配给C1；
    </span>
   </li>
   <li fid="u8eb6a06c">
    如果这时候这个topic
    <strong>
     多了一些分区
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ，就要按照某种策略，把多出来的分区分配给C0和C1；
    </span>
   </li>
   <li fid="u8eb6a06c">
    如果这时候C1
    <strong>
     消费者挂掉了或者退出
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     了，不在消费者组里了，那所有的分区需要再次分配给C0。
    </span>
   </li>
  </ul>
  <p>
   这三种情况其实就是Kafka进行分区分配的前提条件：
  </p>
  <ul list="ua67d2506">
   <li fid="u130c7049">
    同一个 Consumer Group 内新增消费者；
   </li>
   <li fid="u130c7049">
    订阅的主题新增分区；
   </li>
   <li fid="u130c7049">
    消费者离开当前所属的Consumer Group，包括shuts down 或 crashes。
   </li>
  </ul>
  <p>
   只有满足了这三个条件的任意一个，才会进行分区分配 。
   <strong>
    分区的所有权从一个消费者移到另一个消费者称为重新平衡（rebalance），如何rebalance就涉及到本节提到的分区分配策略。Kafka提供了消费者客户端参数partition.assignment.strategy用来设置消费者与订阅主题之间的分区分配策略。默认情况下，此参数的值为：org.apache.kafka.clients.consumer.RangeAssignor，即采用range分配策略。除此之外，Kafka中还提供了roundrobin分配策略和sticky分区分配策略。消费者客户端参数partition.asssignment.strategy可以配置多个分配策略，把它们以逗号分隔就可以了。
   </strong>
  </p>
  <p>
   <strong>
    2、Range分配策略
   </strong>
  </p>
  <p>
   Range分配策略是
   <strong>
    面向每个主题
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    的，首先会对同一个主题里面的分区按照序号进行排序，并把消费者线程按照字母顺序进行排序。然后用分区数除以消费者线程数量来判断每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。
   </span>
  </p>
  <p>
   我们假设有个名为T1的主题，包含了7个分区，它有两个消费者（C0和C1），其中C0的num.streams(消费者线程) = 1，C1的num.streams = 2。排序后的分区是：0，1，2，3，4，5，6；消费者线程排序后是：C0-0，C1-0，C1-1；一共有7个分区，3个消费者线程，进行计算7/3=2…1，商为2余数为1，则每个消费者线程消费2个分区，并且前面1个消费者线程多消费一个分区，结果会是这样的：
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="375"/>
    <col width="375"/>
   </colgroup>
   <tbody>
    <tr>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        消费者线程
       </strong>
      </p>
     </td>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        对应消费的分区序号
       </strong>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C0-0
      </p>
     </td>
     <td>
      <p style="text-align: left">
       0，1，2
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       C1-0
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       3，4
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C1-1
      </p>
     </td>
     <td>
      <p style="text-align: left">
       5，6
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   这样看好像还没什么问题，但是一般在咱们实际生产环境下，会有多个主题，我们假设有3个主题（T1，T2，T3），都有7个分区，那么按照咱们上面这种Range分配策略分配后的消费结果如下：
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="375"/>
    <col width="375"/>
   </colgroup>
   <tbody>
    <tr>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        消费者线程
       </strong>
      </p>
     </td>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        对应消费的分区序号
       </strong>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C0-0
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T1（0，1，2），T2（0，1，2），T3（0，1，2）
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       C1-0
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       T1（3，4），T2（3，4），T3（3，4）
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C1-1
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T1（5，6），T2（5，6），T3（5，6）
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   <strong>
    我们可以发现，在这种情况下，C0-0消费线程要多消费3个分区，这显然是不合理的，其实这就是Range分区分配策略的缺点。
   </strong>
  </p>
  <p>
   <strong>
    3、RoundRobin分配策略
   </strong>
  </p>
  <p>
   RoundRobin策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询算法逐个将分区以此分配给每个消费者。
  </p>
  <p>
   使用RoundRobin分配策略时会出现两种情况：
  </p>
  <ul list="u42b6dbaa">
   <li fid="u31e45fe2">
    如果同一消费组内，所有的消费者订阅的消息都是相同的，那么 RoundRobin 策略的分区分配会是均匀的。
   </li>
   <li fid="u31e45fe2">
    如果同一消费者组内，所订阅的消息是不相同的，那么在执行分区分配的时候，就不是完全的轮询分配，有可能会导致分区分配的不均匀。如果某个消费者没有订阅消费组内的某个 topic，那么在分配分区的时候，此消费者将不会分配到这个 topic 的任何分区。
   </li>
  </ul>
  <p>
   分别举例说明：
  </p>
  <p>
   第一种：比如我们有3个消费者（C0，C1，C2），都订阅了2个主题（T0 和 T1）并且每个主题都有 3 个分区(p0、p1、p2)，那么所订阅的所有分区可以标识为T0p0、T0p1、T0p2、T1p0、T1p1、T1p2。此时使用RoundRobin分配策略后，得到的分区分配结果如下：
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="375"/>
    <col width="375"/>
   </colgroup>
   <tbody>
    <tr>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        消费者线程
       </strong>
      </p>
     </td>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        对应消费的分区序号
       </strong>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C0
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T0p0、T1p0
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       C1
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       T0p1、T1p1
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C2
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T0p2、T1p2
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   可以看到，这时候的分区分配策略是比较平均的。
  </p>
  <p>
   第二种：比如我们依然有3个消费者（C0，C1，C2），他们合在一起订阅了 3 个主题：T0、T1 和 T2（C0订阅的是主题T0，消费者C1订阅的是主题T0和T1，消费者C2订阅的是主题T0、T1和T2），这 3 个主题分别有 1、2、3 个分区(即:T0有1个分区(p0)，T1有2个分区(p0、p1)，T2有3个分区(p0、p1、p2))，即整个消费者所订阅的所有分区可以标识为 T0p0、T1p0、T1p1、T2p0、T2p1、T2p2。此时如果使用RoundRobin分配策略，得到的分区分配结果如下：
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="375"/>
    <col width="375"/>
   </colgroup>
   <tbody>
    <tr>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        消费者线程
       </strong>
      </p>
     </td>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        对应消费的分区序号
       </strong>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C0
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T0p0
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       C1
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       T1p0
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C2
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T1p1、T2p0、T2p1、T2p2
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   这时候显然分配是不均匀的，因此在使用RoundRobin分配策略时，为了保证得均匀的分区分配结果，需要满足两个条件：
  </p>
  <ul list="u469b3c90">
   <li fid="u60c0d0ed">
    同一个消费者组里的每个消费者订阅的主题必须相同；
   </li>
   <li fid="u60c0d0ed">
    同一个消费者组里面的所有消费者的num.streams必须相等。
   </li>
  </ul>
  <p>
   如果无法满足，那最好不要使用RoundRobin分配策略。
  </p>
  <p>
   <strong>
    4、Sticky分配策略
   </strong>
  </p>
  <p>
   <strong>
    Sticky分配策略，这种分配策略是在Kafka的0.11.X版本才开始引入的，是目前最复杂也是最优秀的分配策略。
   </strong>
  </p>
  <p>
   Sticky分配策略的原理比较复杂，它的设计主要实现了两个目的：
  </p>
  <ul list="ue52191e8">
   <li fid="u0e132d76">
    分区的分配要尽可能的均匀；
   </li>
   <li fid="u0e132d76">
    分区的分配尽可能的与上次分配的保持相同。
   </li>
  </ul>
  <p>
   如果这两个目的发生了冲突，优先实现第一个目的。
  </p>
  <p>
   我们举例进行分析：比如我们有3个消费者（C0，C1，C2），都订阅了2个主题（T0 和 T1）并且每个主题都有 3 个分区(p0、p1、p2)，那么所订阅的所有分区可以标识为T0p0、T0p1、T0p2、T1p0、T1p1、T1p2。此时使用Sticky分配策略后，得到的分区分配结果如下：
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="375"/>
    <col width="375"/>
   </colgroup>
   <tbody>
    <tr>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        消费者线程
       </strong>
      </p>
     </td>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        对应消费的分区序号
       </strong>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C0
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T0p0、T1p0
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       C1
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       T0p1、T1p1
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C2
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T0p2、T1p2
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   这里我们可以发现，情况怎么和前面RoundRobin分配策略一样，其实底层实现并不一样。这里假设C2故障退出了消费者组，然后需要对分区进行再平衡操作，如果使用的是RoundRobin分配策略，它会按照消费者C0和C1进行重新轮询分配，再平衡后的结果如下：
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="375"/>
    <col width="375"/>
   </colgroup>
   <tbody>
    <tr>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        消费者线程
       </strong>
      </p>
     </td>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        对应消费的分区序号
       </strong>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C0
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T0p0、T0p2、T1p1
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       C1
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       T0p1、T1p0、T1p2
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   但是如果使用的是Sticky分配策略，再平衡后的结果会是这样：
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="375"/>
    <col width="375"/>
   </colgroup>
   <tbody>
    <tr>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        消费者线程
       </strong>
      </p>
     </td>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        对应消费的分区序号
       </strong>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C0
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T0p0、T1p0、T0p2
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       C1
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       T0p1、T1p1、T1p2
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   Stiky分配策略保留了再平衡之前的消费分配结果，并将原来消费者C2的分配结果分配给了剩余的两个消费者C0和C1，最终C0和C1的分配还保持了均衡。这时候再体会一下sticky（翻译为：粘粘的）这个词汇的意思，是不是豁然开朗了。
  </p>
  <p>
   <strong>
    为什么要这么处理呢？
   </strong>
  </p>
  <p>
   <strong>
    这是因为发生分区重分配后，对于同一个分区而言有可能之前的消费者和新指派的消费者不是同一个，对于之前消费者进行到一半的处理还要在新指派的消费者中再次处理一遍，这时就会浪费系统资源。而使用Sticky策略就可以让分配策略具备一定的“粘性”，尽可能地让前后两次分配相同，进而可以减少系统资源的损耗以及其它异常情况的发生。
   </strong>
  </p>
  <p>
   接下来，再来看一下上一节RoundRobin存在缺陷的地方，这种情况下sticky是怎么分配的？
  </p>
  <p>
   比如我们依然有3个消费者（C0，C1，C2），他们合在一起订阅了 3 个主题：T0、T1 和 T2（C0订阅的是主题T0，消费者C1订阅的是主题T0和T1，消费者C2订阅的是主题T0、T1和T2），这 3 个主题分别有 1、2、3 个分区(即:T0有1个分区(p0)，T1有2个分区(p0、p1)，T2有3个分区(p0、p1、p2))，即整个消费者所订阅的所有分区可以标识为 T0p0、T1p0、T1p1、T2p0、T2p1、T2p2。此时如果使用sticky分配策略，得到的分区分配结果如下：
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="375"/>
    <col width="375"/>
   </colgroup>
   <tbody>
    <tr>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        消费者线程
       </strong>
      </p>
     </td>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        对应消费的分区序号
       </strong>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C0
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T0p0
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       C1
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       T1p0、T1p1
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C2
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T2p0、T2p1、T2p2
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   由于C0消费者没有订阅T1和T2主题，因此如上这样的分配策略已经是这个问题的最优解了！
  </p>
  <p>
   这时候，再补充一个例子，加入C0挂了，发生再平衡后的分配结果，RoundRobin和Sticky又有什么区别呢？
  </p>
  <p>
   RoundRobin再平衡后的分配情况：
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="375"/>
    <col width="375"/>
   </colgroup>
   <tbody>
    <tr>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        消费者线程
       </strong>
      </p>
     </td>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        对应消费的分区序号
       </strong>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C1
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T0p0、T1p1
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       C2
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       T1p0、T2p0、T2p1、T2p2
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   而如果使用Sticky策略，再平衡后分分配情况：
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="375"/>
    <col width="375"/>
   </colgroup>
   <tbody>
    <tr>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        消费者线程
       </strong>
      </p>
     </td>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        对应消费的分区序号
       </strong>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       C1
      </p>
     </td>
     <td>
      <p style="text-align: left">
       T1p0、T1p1、T0p0
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       C2
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       T2p0、T2p1、T2p2
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   这里我们惊奇的发现sticky只是把之前C0消耗的T0p0分配给了C1，我们结合资源消耗来看，这相比RoundRobin能节省更多的资源。
  </p>
  <p>
   综上所述，建议使用sticky分区分配策略。
  </p>
  <h3>
   Kafka生产者分区策略？消费者分区策略？
  </h3>
  <p>
   可回答：1）Kafka怎么知道数据应该存到哪一个分区的？2）如果同一个用户数据想存到同一个分区应该怎么做？3）除了指定分区外还有什么方法吗？
  </p>
  <p>
   问过的一些公司：小米(2022.09)，蔚来提前批(2021.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、生产者分区策略
   </strong>
  </p>
  <p>
   生产者在将消息发送到某个Topic，需要经过拦截器、序列化器和分区器（Partitioner）的一系列作用之后才能发送到对应的Broker，在发往Broker之前是需要确定它所发往的分区。
  </p>
  <p>
   生产端将消息发送给Broker之前，会将producer发送的数据封装成一个 ProducerRecord 对象。是否依赖分区器看partition字段有无指定。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136279721-342aba49-5428-4f49-aec4-215b473d88c7.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_14%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   是否依赖分区器看partition字段有无指定
  </p>
  <ul list="u1eec0b74">
   <li fid="u4ae94943">
    如果消息 ProducerRecord 指定了 partition 字段，那么就不需要分区器。
   </li>
   <li fid="u4ae94943">
    如果消息 ProducerRecord 没有指定 partition 字段，那么就需要依赖分区器，根据key这个字段来计算partition的值。分区器的作用就是为消息分配分区。
   </li>
  </ul>
  <p>
   Kafka中提供的默认分区器是
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    DefaultPartitioner
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，它实现了Partitioner接口（用户可以实现这个接口来自定义分区器）。
   </span>
  </p>
  <p>
   用户可以通过实现
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    kafka.producer.Partitioner
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    接口实现自己的分区类（重载并实现partition方法），在生产端添加配置
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    partitioner.class
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    即可使用。
   </span>
  </p>
  <ul list="u9818a8d0">
   <li fid="u502204c0">
    指明 partition 的情况下，直接将指明的值直接作为 partiton 值。
   </li>
   <li fid="u502204c0">
    没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值。
   </li>
   <li fid="u502204c0">
    既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。
   </li>
   <li fid="u502204c0">
    既没有 partition 值又指定了自定义的分区类，则按自定义分区类来得到 partition 值
   </li>
  </ul>
  <p>
   <strong>
    2、消费者分配策略
   </strong>
  </p>
  <p>
   参考上一题
  </p>
  <h3>
   Kafka 3.0
  </h3>
  <p>
   问过的一些公司：星环科技(2022.10)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   在 Kafka 3.0 中包含了许多重要的新功能，其中比较显著的变化如下所示：
  </p>
  <ul list="uaf483a33">
   <li fid="ub02f0041">
    弃用对 Java 8 和 Scala 2.12 的支持，将在 4.0 版本中彻底移除；
   </li>
   <li fid="ub02f0041">
    Kafka Raft 支持元数据主题的快照以及自动管理仲裁中的其他改进；
   </li>
   <li fid="ub02f0041">
    默认情况下为 Kafka 生产者提供更加强大的交付保证；
   </li>
   <li fid="ub02f0041">
    弃用消息格式 v0 和 v1；
   </li>
   <li fid="ub02f0041">
    OffsetFetch 和 FindCoordinator 请求中的优化；
   </li>
   <li fid="ub02f0041">
    更灵活的 Mirror Maker 2 配置和 Mirror Maker 1 的弃用；
   </li>
   <li fid="ub02f0041">
    能够在 Kafka Connect 中的单个调用中重新其中连接器的任务；
   </li>
   <li fid="ub02f0041">
    现在默认启用连接器日志上下文和连接器客户单覆盖；
   </li>
   <li fid="ub02f0041">
    Kafka Streams 中时间戳同步的增强语义；
   </li>
   <li fid="ub02f0041">
    改进了 Stream 和 TaskId 的公共 API；
   </li>
   <li fid="ub02f0041">
    Kafka 中的默认 serde 变为 None。
   </li>
  </ul>
  <h3>
   Raft
  </h3>
  <p>
   问过的一些公司：星环科技(2022.10)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka 3集群在没有Zookeeper集群的依赖下，也可以正常运行，但是对于在Zookeeper当中保存的各种重要元数据信息，在Kafka 3当中如何实现保存的呢？
  </p>
  <p>
   Kafka 一直都是使用Zookeeper来管理集群以及所有的topic的元数据，并且使用了Zookeeper的强一致性来选举集群的Controller，Controller对整个集群的管理至关重要，包括分区的新增，ISR列表的维护，等等很多功能都需要靠Controller来实现，然后使用Zookeeper来维护Kafka的元数据也存在很多的问题以及存在性能瓶颈。
  </p>
  <p>
   在KIP-500中，Kafka控制器会将其元数据存储在Kafka分区中，而不是存储在ZooKeeper中。但是，由于控制器依赖于该分区，因此分区本身不能依赖控制器来进行领导者选举之类的事情。而是，管理该分区的节点必须实现自我管理的Raft仲裁。
  </p>
  <p>
   <strong>
    在Kafka 3的新的版本当中，使用了新的KRaft协议，可以不用再依赖于Zookeeper来保存Kafka当中的元数据，转而使用Kafka Raft来实现元数据的一致性，并且将元数据保存在Kafka自己的服务器当中，大大提高了Kafka的元数据管理的性能
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。这个协议类似于Zookeeper当中的ZAB协议以及类似于Raft协议，但是KRaft协议使用的是基于事件驱动的模式，与ZAB协议和Raft协议还有点不一样。
   </span>
  </p>
  <p>
   在Kafka 3.0之前的的版本当中，主要是借助于Controller来进行leader partition的选举，而在3.0协议当中，使用了KRaft来实现自己选择leader，并最终令所有节点达成共识，这样简化了Controller的选举过程，效果更加高效。
  </p>
  <h3>
   Controller
  </h3>
  <p>
   问过的一些公司：星环科技(2022.10)，蔚来(2021.04)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   不管是Kafka 2还是Kafka 3当中，Controller控制器都是必不可少的，通过Controller控制器来维护kafka集群的正常运行，例如ISR列表的变更，broker的上线或者下线，topic的创建，分区的指定等等各种操作都需要依赖于Controller，在Kafka 2当中，Controller的选举需要通过Zookeeper来实现，我们没法控制哪些机器选举成为Controller，而在Kafka 3当中,我们可以通过配置文件来自己指定哪些机器成为Controller，这样做的好处就是我们可以指定一些配置比较高的机器作为Controller节点，从而保证Controller节点的稳健性。
  </p>
  <p>
   被选中的Controller节点参与元数据集群的选举，每个Controller节点要么是Active状态，或者就是standBy状态。
  </p>
  <h3>
   Kafka中如何保证数据一致性？
  </h3>
  <p>
   可回答：1）Kafka的一致性；2）Kafka的消费端的数据一致性
  </p>
  <p>
   问过的一些公司：竞技世界(2022.09)，小米(2022.09)，字节(2022.06)(2020.08)，中信银行信用卡中心(2020.11)，美团(2020.08)，乐言科技(2019.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   不论是旧的Leader还是新选举产生的Leader，Consumer都能读到一样的数据，Kafka是通过引入HW（High Water Mark）机制来保证数据一致性。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136281094-1646c5fa-0642-4025-ae16-0e2ecbeabd6a.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_21%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   假设分区的副本为3，其中副本0是Leader，副本1和副本2是follower，并且在ISR列表里面，虽然副本0已经写入了Message4，但是Consumer只能卖取到Message2。因为所有的ISR都同步了Message2，只有High Water Mark以上的消息才支持Consumer读取，而High Water Mark取决于ISR列表里面偏移量最小的分区，对应于上图的副本2，这个很类似于木桶原理。
  </p>
  <p>
   这样做的原因是还没有被足够多副本复制的消息被认为是“不安全”的，如果 Leader 发生崩溃，另一个副本成为新Leader，那么这些消息很可能丢失了。如果我们允许消费者读取这些消息，可能就会破坏一致性。试想，一个消费者从当前Leader（副本0）读取并处理了Message4，这个时候Leader挂掉了，选举了副本1为新的Leader，这时候另一个消费者再去从新的Leader读取消息，发现这个消息其实并不存在，这就导致了数据不一致性的问题。
  </p>
  <p>
   当然，引入了High Water Mark 机制，会导数Broker间的消息复制因为某些原因变慢，那么消息到达消费者的时间也会随之变长（因为我们会先等待消息复制完毕），延迟时间可以通过参数replica.lag.time.max.ms参数配置，它指定了副本在复制消息时可被允许的最大延迟时间。
  </p>
  <h3>
   Kafka消费者如何消费多分区
  </h3>
  <p>
   问过的一些公司：携程(2022.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   一个 Kafka 消费者可以消费多个分区，可以通过订阅多个分区实现。在创建 Kafka 消费者时，可以通过
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    subscribe
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    方法订阅一个主题的多个分区，也可以通过
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    assign
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    方法直接分配多个分区给消费者。
   </span>
  </p>
  <p>
   一个通过订阅主题多个分区的示例代码：
  </p>
  <pre><code class="language-plain" lang="plain">Properties props = new Properties();
 props.put("bootstrap.servers", "localhost:9092");
 props.put("group.id", "test-group");
 
 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);
 
 // 订阅主题中的多个分区
 consumer.subscribe(Arrays.asList("topic1", "topic2"));</code></pre>
  <p>
   通过
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    subscribe
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    方法订阅主题时，Kafka 会自动为消费者分配分区，并且在分区发生变化时自动重新分配。如果需要手动分配分区，则可以使用
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    assign
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    方法：
   </span>
  </p>
  <pre><code class="language-plain" lang="plain">Properties props = new Properties();
 props.put("bootstrap.servers", "localhost:9092");
 props.put("group.id", "test-group");
 
 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);
 
 // 手动分配多个分区
 TopicPartition partition1 = new TopicPartition("topic1", 0);
 TopicPartition partition2 = new TopicPartition("topic2", 1);
 consumer.assign(Arrays.asList(partition1, partition2));</code></pre>
  <p>
   使用
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    assign
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    方法时需要注意的是，需要手动管理消费者与分区之间的关系，包括重新分配、重平衡等。
   </span>
  </p>
  <h3>
   Kafka消费原理
  </h3>
  <p>
   如下图所示，每个consumer会一对一对应一个blockingQueue，这样在consumer分配完要消费的partition后，其实这个阻塞队列和partition的关系就确定了。在这个地方，阻塞队列的大小就和配置参数有关。每个broker会有一个fetchThread，负责把消息放到阻塞队列里边。在放的时候，会根据partition和队列的对应关系，把消息放到不同的队列里边。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136281175-b19beac3-faf8-4990-8b49-aad5ebb41221.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_24%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   max.poll.records参数指定的是从所有消费的partition拉取消息的最大数量。如果是多个partiton的话，一次拉取的消息中每个partition的占比是不确定的，有可能10个都是partition0的消息，也有可能5个是partition0,5个是partition1，也有可能是1:9，人为无法控制。这也是为什么一个consumer对应多个partition时无法保证消费顺序，而只能保证单个partition中消息的顺序性。
  </p>
  <h3>
   Kafka如何保证消息的可靠性
  </h3>
  <p>
   可回答：1）Kafka怎么保证有序性、不丢失、不重复的，生产者怎么做消费者怎么做都讲；2）怎么解决Kafka的数据丢失；3）Kafka如何保证生产者不丢失数据，消费端不丢失数据
  </p>
  <p>
   问过的一些公司：Lazada(2022.11)，字节(2022.04)(2020.04)，荣耀(2021.08)，唯品会社招(2021.08)，快手(2021.06)，招银网络(2021.06)，字节(2021.04)，美团优选(2021.04)，顺丰(2020.11)，小米(2020.10)(2020.08)，农行广州(2020.09)，快手(2020.09)，美团(2020.08)，快手提前批(2020.08)，作业帮提前批(2020.08)，腾讯CSIG(2020.03)，阿里新零售实习(2020.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   如何保证消息的可靠性传输，或者说，如何保证消息不丢失？这对于Kafka来说是最核心问题之一。
  </p>
  <p>
   一条消息从生产到消费，可以划分三个阶段：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136281168-206ad54f-75f7-49e8-880c-fef2edfa4405.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_29%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    生产阶段
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：Producer 创建消息，并通过网络发送给 Broker。
   </span>
  </p>
  <p>
   <strong>
    存储阶段
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：Broker 收到消息并存储，如果是集群，还要同步副本给其他 Broker。
   </span>
  </p>
  <p>
   <strong>
    消费阶段
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：Consumer 向 Broker 请求消息，Broker 通过网络传输给 Consumer。
   </span>
  </p>
  <p>
   这三个阶段都可能丢失数据，所以要保证消息丢失，就需要任意一环都保证可靠。
  </p>
  <p>
   <strong>
    1、生产（Producer）阶段
   </strong>
  </p>
  <p>
   在生产消息阶段，消息队列一般通过请求确认机制，来保证消息的可靠传递，Kafka 也不例外。
  </p>
  <p>
   Kafka 有三种发送方式：同步、异步、异步回调。
  </p>
  <p>
   同步方式能保证消息不丢失，但性能太差；异步方式发送消息，通常会立即返回，但消息可能丢失。
  </p>
  <p>
   解决生产者丢失消息的方案：
  </p>
  <p>
   生产者使用异步回调方式
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    producer.send(msg, callback)
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    发送消息。callback（回调）能准确地告诉你消息是否真的提交成功了。一旦出现消息提交失败的情况，你就可以有针对性地进行处理。
   </span>
  </p>
  <ul list="u115972f2">
   <li fid="u1910a790">
    如果是因为那些瞬时错误，那么仅仅让 Producer 重试就可以了
   </li>
   <li fid="u1910a790">
    如果是消息不合格造成的，那么可以调整消息格式后再次发送
   </li>
  </ul>
  <p>
   但是，
   <strong>
    Kafka 生产者的可靠性主要是基于以下几点来保证
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：
   </span>
  </p>
  <p>
   <strong>
    1）ACK
   </strong>
  </p>
  <p>
   生产者可选的确认模式有三种：
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    ack=0
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    、
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    ack=1
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    、
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    ack=all/-1
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
  </p>
  <ul list="u10e92f9d">
   <li fid="u10c3222e">
    ack=0
    、
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     ack=1
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     都有丢失数据的风险。
    </span>
   </li>
   <li fid="u10c3222e">
    ack=all/-1
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     意味着会等待所有同步副本都收到消息。再结合
    </span>
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     min.insync.replicas
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ，就可以决定在得到确认响应前，至少有多少副本能够收到消息。
    </span>
   </li>
  </ul>
  <p>
   这是最保险的做法，但也会降低吞吐量。根据实际的应用场景，我们设置不同的 acks，以此保证数据的可靠性。
  </p>
  <p>
   注意：能答出这一点，就算是答到点上了。
  </p>
  <p>
   <strong>
    2）重试
   </strong>
  </p>
  <p>
   如果 broker 返回的错误可以通过
   <strong>
    重试
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    来解决，生产者会自动处理这些错误。
   </span>
  </p>
  <ul list="u32b0d02b">
   <li fid="uc1012c75">
    <strong>
     可重试错误
    </strong>
    ，如：
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     LEADER_NOT_AVAILABLE
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ，主副本不可用，可能过一段时间，集群就会选举出新的主副本，重试可以解决问题。
    </span>
   </li>
   <li fid="uc1012c75">
    <strong>
     不可重试错误
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ，如：
    </span>
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     INVALID_CONFIG
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ，即使重试，也无法改变配置选项，重试没有意义。
    </span>
   </li>
  </ul>
  <p>
   需要注意的是：有时可能因为网络问题导致没有收到确认，但实际上消息已经写入成功。生产者会认为出现临时故障，重试发送消息，这样就会出现重复记录。所以，尽可能在业务上保证幂等性。
  </p>
  <p>
   设置
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    retries
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    为一个较大的值。这里的
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    retries
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    同样是 Producer 的参数，对应前面提到的 Producer 自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了 retries &gt; 0 的 Producer 能够自动重试消息发送，避免消息丢失。
   </span>
  </p>
  <p>
   <strong>
    3）错误处理
   </strong>
  </p>
  <p>
   开发者需要自行处理的错误：
  </p>
  <ul list="ub3514256">
   <li fid="u2e1d2d3d">
    不可重试的 broker 错误，如消息大小错误、认证错误等
   </li>
   <li fid="u2e1d2d3d">
    消息发送前发生的错误，如序列化错误
   </li>
   <li fid="u2e1d2d3d">
    生产者达到重试次数上限或消息占用的内存达到上限时发生的错误
   </li>
  </ul>
  <p>
   <strong>
    2、存储（Broker）阶段
   </strong>
  </p>
  <p>
   存储阶段指的是 Kafka Server，也就是 Broker 如何保证消息不丢失。
  </p>
  <p>
   一句话概括，
   <strong>
    Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    （其实就是
   </span>
   <strong>
    分区多副本
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ）。
   </span>
  </p>
  <p>
   上面的话可以解读为：
  </p>
  <ul list="u886c1f79">
   <li fid="uac819310">
    <strong>
     已提交
    </strong>
    ：
    <strong>
     只有当消息被写入分区的若干同步副本时，才被认为是已提交的
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     。为什么是若干个 Broker 呢？这取决于你对“已提交”的定义。你可以选择只要 Leader 成功保存该消息就算是已提交，也可以是令所有 Broker 都成功保存该消息才算是已提交。
    </span>
   </li>
   <li fid="uac819310">
    <strong>
     持久化
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ：Kafka 的数据存储在磁盘上，所以只要写入成功，天然就是持久化的。
    </span>
   </li>
   <li fid="uac819310">
    <strong>
     只要还有一个副本是存活的，那么已提交的消息就不会丢失
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     。
    </span>
   </li>
   <li fid="uac819310">
    <strong>
     消费者只能读取已提交的消息
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     。
    </span>
   </li>
  </ul>
  <p>
   <strong>
    1）副本机制
   </strong>
  </p>
  <p>
   注意：能答出这一点，就算是答到点上了。
  </p>
  <p>
   <strong>
    Kafka 的副本机制是 Kafka 可靠性保证的核心
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
  </p>
  <p>
   Kafka 的主题被分为多个分区，分区是基本的数据块。每个分区可以有多个副本，有一个是 Leader（主副本），其他是 Follower（从副本）。所有数据都直接发送给 Leader，或者直接从 Leader 读取事件。Follower 只需要与 Leader 保持同步，并及时复制最新的数据。当 Leader 宕机时，从 Follower 中选举一个成为新的 Leader。
  </p>
  <p>
   Broker 有 3 个配置参数会影响 Kafka 消息存储的可靠性。
  </p>
  <p>
   <strong>
    （1）副本数
   </strong>
  </p>
  <p>
   replication.factor
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    的作用是设置每个分区的副本数。
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    replication.factor
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    是主题级别配置；
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    default.replication.factor
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    是 broker 级别配置。
   </span>
  </p>
  <p>
   副本数越多，数据可靠性越高；但由于副本数增多，也会增加同步副本的开销，可能会降低集群的可用性。一般，建议设为 3，这也是 Kafka 的默认值。
  </p>
  <p>
   <strong>
    （2）不完全的选主
   </strong>
  </p>
  <p>
   unclean.leader.election.enable
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    用于控制是否支持不同步的副本参与选举 Leader。
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    unclean.leader.election.enable
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    是 broker 级别（实际上是集群范围内）配置，默认值为 True。
   </span>
  </p>
  <ul list="ud16d7e45">
   <li fid="u701787ab">
    如果设为 True，代表着
    <strong>
     允许不同步的副本成为主副本
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     （即不完全的选举），那么将
    </span>
    <strong>
     面临丢失消息的风险
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ；
    </span>
   </li>
   <li fid="u701787ab">
    如果设为 False，就要
    <strong>
     等待原先的主副本重新上线
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ，从而降低了可用性。
    </span>
   </li>
  </ul>
  <p>
   <strong>
    （3）最少同步副本
   </strong>
  </p>
  <p>
   <strong>
    min.insync.replicas
   </strong>
   <strong>
    控制的是消息至少要被写入到多少个副本才算是“已提交”
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    min.insync.replicas
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    是主题级别和 broker 级别配置。
   </span>
  </p>
  <p>
   尽管可以为一个主题配置 3 个副本，但还是可能会出现只有一个同步副本的情况。如果这个同步副本变为不可用，则必须在可用性和数据一致性之间做出选择。Kafka 中，消息只有被写入到所有的同步副本之后才被认为是已提交的。但如果只有一个同步副本，那么在这个副本不可用时，则数据就会丢失。
  </p>
  <p>
   如果要确保已经提交的数据被已写入不止一个副本，就需要把最小同步副本的设置为大一点的值。
  </p>
  <p>
   注意：要确保
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    replication.factor
   </span>
   <span class="lake-fontsize-12" style="color: rgb(119, 119, 119)">
    &gt;
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    min.insync.replicas
   </span>
   <span class="lake-fontsize-12" style="color: rgb(119, 119, 119)">
    。如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。我们不仅要改善消息的持久性，防止数据丢失，还要在不降低可用性的基础上完成。推荐设置成
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    replication.factor = min.insync.replicas + 1
   </span>
   <span class="lake-fontsize-12" style="color: rgb(119, 119, 119)">
    。
   </span>
  </p>
  <p>
   <strong>
    3、消费（Consumer）阶段
   </strong>
  </p>
  <p>
   <strong>
    消费者只能读取已提交的消息
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。这就保证了消费者接收到消息时已经具备了数据一致性。
   </span>
  </p>
  <p>
   消费者唯一要做的是确保哪些消息是已经读取过的，哪些是没有读取过的（通过提交偏移量给 Broker 来确认）。如果消费者提交了偏移量却未能处理完消息，那么就有可能造成消息丢失，这也是消费者丢失消息的主要原因。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136281179-d88113e3-7e8a-4454-bcb5-5093198abbf6.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_15%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   也就是，通过 offset commit 来保证数据的不丢失，kafka 自己记录了每次消费的 offset 数值，下次继续消费的时候，接着上次的 offset 进行消费即可。
  </p>
  <p>
   还有一点要注意，就是要保证数据的幂等性。
  </p>
  <p>
   <strong>
    1）消费者的可靠性配置
   </strong>
  </p>
  <ul list="ue1552032">
   <li fid="u325b43eb">
    group.id
    - 如果希望消费者可以看到主题的所有消息，那么需要为它们设置唯一的
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     group.id
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     。
    </span>
   </li>
   <li fid="u325b43eb">
    auto.offset.reset
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     有两个选项：
    </span>
   </li>
  </ul>
  <ul data-lake-indent="1" list="ue1552032">
   <li fid="ue462860a">
    earliest
    - 消费者会从分区的开始位置读取数据
   </li>
   <li fid="ue462860a">
    latest
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     - 消费者会从分区末尾位置读取数据
    </span>
   </li>
  </ul>
  <ul list="ue1552032" start="3">
   <li fid="u325b43eb">
    enable.auto.commit
    - 消费者自动提交偏移量。如果设为 True，处理流程更简单，但无法保证重复处理消息。
   </li>
   <li fid="u325b43eb">
    auto.commit.interval.ms
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     - 自动提交的频率，默认为每 5 秒提交一次
    </span>
   </li>
  </ul>
  <p>
   <strong>
    2）显示提交偏移量
   </strong>
  </p>
  <p>
   如果
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    enable.auto.commit
   </span>
   <span class="lake-fontsize-12" style="color: rgb(119, 119, 119)">
    设为 True，即自动提交，就无需考虑提交偏移量的问题。
   </span>
  </p>
  <p>
   如果选择显示提交偏移量，需要考虑以下问题：
  </p>
  <ul list="u8c2fc72e">
   <li fid="u9fc34270">
    必须在处理完消息后再发送确认（提交偏移量），不要收到消息立即确认。
   </li>
   <li fid="u9fc34270">
    提交频率是性能和重复消息数之间的权衡
   </li>
   <li fid="u9fc34270">
    分区再均衡
   </li>
   <li fid="u9fc34270">
    消费可能需要重试机制
   </li>
   <li fid="u9fc34270">
    超时处理
   </li>
   <li fid="u9fc34270">
    消费者可能需要维护消费状态，如：处理完消息后，记录在数据库中。
   </li>
   <li fid="u9fc34270">
    幂等性设计
   </li>
  </ul>
  <ul data-lake-indent="1" list="u8c2fc72e">
   <li fid="ude9d9330">
    写数据库：根据主键判断记录是否存在
   </li>
   <li fid="ude9d9330">
    写 Redis：set 操作天然具有幂等性
   </li>
   <li fid="ude9d9330">
    复杂的逻辑处理，则可以在消息中加入全局 ID
   </li>
  </ul>
  <h3>
   Kafka的选举机制
  </h3>
  <p>
   可回答：1）Leader选举
  </p>
  <p>
   问过的一些公司：多益(2022.11)，字节(2022.08)，小米(2020.10)，美团(2020.08)，顺丰(2020.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、控制器（Broker）选举机制
   </strong>
  </p>
  <p>
   控制器是Kafka的核心组件，它的主要作用是在Zookeeper的帮助下管理和协调整个Kafka集群。集群中任意一个Broker都能充当控制器的角色，但在运行过程中，只能有一个Broker成为控制器。
  </p>
  <p>
   控制器选举可以认为是Broker的选举。
  </p>
  <p>
   <strong>
    集群中第一个启动的Broker会通过在Zookeeper中创建临时节点/controller来让自己成为控制器
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，其他Broker启动时也会在Zookeeper中创建临时节点，但是发现节点已经存在，所以它们会收到一个异常，意识到控制器已经存在，那么就会在Zookeeper中创建watch对象，便于它们收到控制器变更的通知。
   </span>
  </p>
  <p>
   那么如果控制器由于网络原因与Zookeeper断开连接或者异常退出，那么其他Broker通过watch收到控制器变更的通知，就会去尝试创建临时节点/controller，如果有一个Broker创建成功，那么其他Broker就会收到创建异常通知，也就意味着集群中已经有了控制器，其他Broker只需创建watch对象即可。
  </p>
  <p>
   如果集群中有一个Broker发生异常退出了，那么控制器就会检查这个Broker是否有分区的副本Leader，如果有，那么这个分区就需要一个新的Leader，此时控制器就会去遍历其他副本，决定哪一个成为新的Leader，同时更新分区的ISR集合。
  </p>
  <p>
   如果有一个Broker加入集群中，那么控制器就会通过Broker ID去判断新加入的Broker中是否含有现有分区的副本，如果有，就会从分区副本中去同步数据。
  </p>
  <p>
   选举控制器的详细流程：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136281804-7eee441b-8a20-4e50-8149-98811421cfa1.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_29%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <ol list="u62d71920">
   <li fid="u6c78ee34">
    第一个在 ZooKeeper 中成功创建
    /controller
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     临时节点的 Broker 会被指定为控制器。
    </span>
   </li>
   <li fid="u6c78ee34">
    其他 Broker 在控制器节点上创建 Zookeeper watch 对象。
   </li>
   <li fid="u6c78ee34">
    如果控制器被关闭或者与 Zookeeper 断开连接，Zookeeper 临时节点就会消失。集群中的其他 Broker 通过 watch 对象得到状态变化的通知，它们会尝试让自己成为新的控制器。
   </li>
   <li fid="u6c78ee34">
    第一个在 Zookeeper 里创建一个临时节点
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     /controller
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     的 Broker 成为新控制器。其他 Broker 在新控制器节点上创建 Zookeeper watch 对象。
    </span>
   </li>
   <li fid="u6c78ee34">
    每个新选出的控制器通过 Zookeeper 的条件递增操作获得一个全新的、数值更大的 controller epoch。其他节点会忽略旧的 epoch 的消息。
   </li>
   <li fid="u6c78ee34">
    当控制器发现一个 Broker 已离开集群，并且这个 Broker 是某些 Partition 的 Leader。此时，控制器会遍历这些 Partition，并用轮询方式确定谁应该成为新 Leader，随后，新 Leader 开始处理生产者和消费者的请求，而 Follower 开始从 Leader 那里复制消息。
   </li>
  </ol>
  <p>
   简而言之，
   <strong>
    Kafka 使用 Zookeeper 的临时节点来选举控制器，并在节点加入集群或退出集群时通知控制器。控制器负责在节点加入或离开集群时进行 Partition Leader 选举。控制器使用 epoch 来避免“脑裂”，“脑裂”是指两个节点同时被认为自己是当前的控制器
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
  </p>
  <p>
   <strong>
    2、分区副本选举机制
   </strong>
  </p>
  <p>
   Kafka 副本有两种角色：
  </p>
  <ul list="u0304f815">
   <li fid="u97a7e08b">
    <strong>
     Leader 副本（主）
    </strong>
    ：每个 Partition 都有且仅有一个 Leader 副本。为了保证数据一致性，
    <strong>
     Leader 处理一切对 Partition （分区）的读写请求
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ；
    </span>
   </li>
   <li fid="u97a7e08b">
    <strong>
     Follower 副本（从）
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ：Leader 副本以外的副本都是 Follower 副本。
    </span>
    <strong>
     Follower 唯一的任务就是从 Leader 那里复制消息，保持与 Leader 一致的状态
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     。
    </span>
   </li>
  </ul>
  <p>
   <strong>
    如果 Leader 宕机，其中一个 Follower（ISR中的） 会被选举为新的 Leader。
   </strong>
  </p>
  <p>
   <strong>
    要了解副本选举机制，我们要先了解ISR。
   </strong>
  </p>
  <p>
   ISR 即 In-sync Replicas，表示同步副本。ISR 是一个动态调整的集合，会不断将同步副本加入集合，将不同步副本移除集合。Leader 副本天然就在 ISR 中。
  </p>
  <p>
   这里我们还需要了解下
   <strong>
    Unclean leader选举
   </strong>
  </p>
  <p>
   因为 Leader 副本天然就在 ISR 中，而ISR是动态变化的，所以ISR列表就有为空的时候，如果 ISR 为空了，就说明 Leader 副本也“挂掉”了，Kafka 需要重新选举一个新的 Leader。但ISR为空，怎么进行leader选举呢？
  </p>
  <p>
   <strong>
    Kafka 把所有不在 ISR 中的存活副本都称为非同步副本
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。通常来说，非同步副本落后 Leader 太多，因此，如果选择这些副本作为新 Leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老 Leader 中的消息。在 Kafka 中，选举这种副本的过程称为 Unclean 领导者选举。
   </span>
   <strong>
    Broker 端参数
   </strong>
   <strong>
    unclean.leader.election.enable
   </strong>
   <strong>
    控制是否允许 Unclean 领导者选举
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
  </p>
  <p>
   <strong>
    开启 Unclean 领导者选举可能会造成数据丢失
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，但好处是：它使得 Partition Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止 Unclean 领导者选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。
   </span>
  </p>
  <p>
   <strong>
    3、消费组选举机制
   </strong>
  </p>
  <p>
   在Kafka的消费端，会有一个消费者协调器以及消费组，组协调器（Group Coordinator）需要为消费组内的消费者选举出一个消费组的Leader。
  </p>
  <p>
   如果消费组内还没有Leader，那么第一个加入消费组的消费者即为消费组的Leader，如果某一个时刻Leader消费者由于某些原因退出了消费组，那么就会重新选举Leader，选举方式如下：
  </p>
  <pre><code class="language-plain" lang="plain">private val members = new mutable.HashMap[String, MemberMetadata]
 leaderId = members.keys.headOption</code></pre>
  <p>
   上面代码是Kafka源码中的部分代码，member是一个HashMap的数据结构，key为消费者的
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    member_id
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，value是元数据信息，那么它会将LeaderId选举为HashMap中的第一个键值对，它和随机基本没啥区别。
   </span>
  </p>
  <h3>
   Kafka是如何读写副本消息的？
  </h3>
  <p>
   可回答：1）Kafka的partition副本写数据是怎么写的？
  </p>
  <p>
   问过的一些公司：多益(2022.11)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   先给出小结
  </p>
  <p>
   Kafka副本状态机类ReplicaManager读写副本的核心方法：
  </p>
  <ul list="u9b7b453c">
   <li fid="ua6bf38ac">
    appendRecords：向副本写入消息，利用Log#append方法和Purgatory机制实现Follower副本向Leader副本获取消息后的数据同步操作
   </li>
   <li fid="ua6bf38ac">
    fetchMessages：从副本读取消息，为普通Consumer和Follower副本所使用。当它们向Broker发送FETCH请求时，Broker上的副本管理器调用该方法从本地日志中获取指定消息
   </li>
  </ul>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136283454-ba898b69-e491-455f-b32d-abf072ace858.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_34%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   无论是读取副本还是写入副本，都是通过底层的Partition对象完成的，而这些分区对象全部保存在allPartitions字段中。
  </p>
  <p>
   <strong>
    1、副本写入：appendRecords
   </strong>
  </p>
  <p>
   向副本底层日志写入消息的逻辑就实现在ReplicaManager#appendRecords。
  </p>
  <p>
   Kafka需副本写入的场景：
  </p>
  <ol list="u04290a38">
   <li fid="u2a6cde77">
    生产者向Leader副本写入消息
   </li>
   <li fid="u2a6cde77">
    Follower副本拉取消息后写入副本
   </li>
  </ol>
  <ol data-lake-indent="1" list="u04290a38">
   <li fid="u7e464ec5">
    仅该场景调用Partition对象的方法，其余3个都是调用appendRecords完成
   </li>
  </ol>
  <ol list="u04290a38" start="3">
   <li fid="u2a6cde77">
    消费者组写入组信息
   </li>
   <li fid="u2a6cde77">
    事务管理器写入事务信息（包括事务标记、事务元数据等）
   </li>
  </ol>
  <p>
   appendRecords方法将给定的一组分区的消息写入对应Leader副本，并根据PRODUCE请求中acks的设置，有选择地等待其他副本写入完成。然后，调用指定回调逻辑。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136283596-d3a558e7-178d-47be-bbb6-545a9570a03f.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_68%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   appendRecords向副本日志写入消息的过程：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136284157-22e361fd-7154-4da7-b862-792e066f20ab.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_51%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   执行流程：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136283400-5979d095-f738-44e0-a7fe-4484038d99bb.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_14%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   可见，appendRecords：
  </p>
  <ul list="ufbcc49c7">
   <li fid="uddece3e8">
    实现消息写入的方法是
    <strong>
     appendToLocalLog
    </strong>
   </li>
   <li fid="uddece3e8">
    判断是否需要等待其他副本写入的方法
    <strong>
     delayedProduceRequestRequired
    </strong>
   </li>
  </ul>
  <p>
   <strong>
    appendToLocalLog写入副本本地日志
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136284097-d656613b-ac4e-48f2-b810-8670958a46f0.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_50%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   利用Partition#appendRecordsToLeader写入消息集合，就是利用appendAsLeader方法写入本地日志的。
  </p>
  <p>
   <strong>
    delayedProduceRequestRequired
   </strong>
  </p>
  <p>
   判断消息集合被写入到日志之后，是否需要等待其它副本也写入成功：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136284670-e154ff6e-162f-48d1-b928-de4e7f1ce6f4.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_24%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   若等待其他副本的写入，须同时满足：
  </p>
  <ol list="u439797bd">
   <li fid="u8479cf06">
    requiredAcks==-1
   </li>
   <li fid="u8479cf06">
    依然有数据尚未写完
   </li>
   <li fid="u8479cf06">
    至少有一个分区的消息，已成功被写入本地日志
   </li>
  </ol>
  <p>
   2和3可结合来看。若所有分区的数据写入都不成功，则可能出现严重错误，此时应不再等待，而是直接返回错误给发送方。
  </p>
  <p>
   而有部分分区成功写入，部分分区写入失败，则可能偶发的瞬时错误导致。此时，不妨将本次写入请求放入Purgatory，给个重试机会。
  </p>
  <p>
   <strong>
    2、副本读取：fetchMessages
   </strong>
  </p>
  <p>
   ReplicaManager#fetchMessages负责读取副本数据。无论：
  </p>
  <ul list="uf204d445">
   <li fid="ua48427b0">
    Java消费者API
   </li>
   <li fid="ua48427b0">
    Follower副本
   </li>
  </ul>
  <p>
   拉取消息的主途径都是向Broker发FETCH请求，Broker端接收到该请求后，调用fetchMessages从底层的Leader副本取出消息。
  </p>
  <p>
   fetchMessages也可能会延时处理FETCH请求，因Broker端必须要累积足够多数据后，才会返回Response给请求发送方。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136285478-3d677495-4168-4550-8bb1-5b69f657653e.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_11%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136285927-c488a985-5d69-4f6f-b2ba-8b81eb982897.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_63%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   整个方法分为：
  </p>
  <p>
   <strong>
    读取本地日志
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136288583-deae101b-1f3e-4333-b3f5-91f79a5e489c.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_52%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   首先判断，读取消息的请求方，就能确定可读取的范围了。
  </p>
  <p>
   fetchIsolation，读取隔离级别:
  </p>
  <ul list="uf1a97fa3">
   <li fid="u866df3c4">
    对Follower副本，它能读取到Leader副本LEO值以下的所有消息
   </li>
   <li fid="u866df3c4">
    普通Consumer，只能“看到”Leader副本高水位值以下的消息
   </li>
  </ul>
  <p>
   确定可读取范围后，调用
   <strong>
    readFromLog
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    读取本地日志上的消息数据，并将结果赋给logReadResults变量。readFromLog调用readFromLocalLog，在待读取分区上依次调用其日志对象的read方法执行实际的消息读取。
   </span>
  </p>
  <p>
   <strong>
    根据读取结果确定Response
   </strong>
  </p>
  <p>
   根据上一步读取结果创建对应Response：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136288749-56a0b4cf-1580-4ef5-86f6-7873454e6968.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_58%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   根据上一步得到的读取结果，统计可读取的总字节数，然后判断此时是否能够立即返回Reponse。
  </p>
  <p>
   副本管理器读写副本的两个方法appendRecords和fetchMessages本质上在底层分别调用Log的append和read方法，以实现本地日志的读写操作。完成读写操作后，这两个方法还定义了延时处理的条件。一旦满足延时处理条件，就交给对应Purgatory处理。
  </p>
  <h3>
   Kafka的ACK机制
  </h3>
  <p>
   可回答：1）ACK作用
  </p>
  <p>
   问过的一些公司：传音(2022.10)，字节(2022.08)，Flow++(2022.04)，京东(2021.04)，美团实习(2021.04)，快手提前批(2020.09)，转转(2020.09)，美团(2020.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka的ACK机制，指的是producer的消息发送确认机制，在不同的场景可以灵活的选择ACK机制来配合业务，不同的ACK设置对Kafka集群的吞吐量和消息可靠性也会有影响，灵活使用，用于保证消息发送的可靠性。
  </p>
  <p>
   ACK有3个可选值，分别是1，0，-1。
  </p>
  <ul list="uf550b8a2">
   <li fid="u799f9b22">
    ACK = 0 时， 发送一次，不论Leader是否接收
   </li>
   <li fid="u799f9b22">
    ACK = 1 时， 等待Leader接收成功即可
   </li>
   <li fid="u799f9b22">
    ACK = -1 时 ，需等待Leader将消息同步给Follower
   </li>
  </ul>
  <p>
   详细情况如下：
  </p>
  <ul list="u9624edac">
   <li fid="u399e80dd">
    ACK = 0，就是Kafka生产端发送消息之后，不管broker的副本有没有成功收到消息，在producer端都会认为是发送成功了，这种情况提供了最小的延迟，和最弱的持久性，如果在发送途中Leader异常，就会造成数据丢失。
   </li>
   <li fid="u399e80dd">
    ACK = 1，是Kafka默认的消息发送确认机制，此机制是在producer发送数据成功，并且Leader接收成功并确认后就算消息发送成功，但是这种情况如果Leader接收成功了，但是Follower未同步时Leader异常，就会造成上位的Follower丢失数据，提供了较好的持久性和较低的延迟性。
   </li>
   <li fid="u399e80dd">
    ACK = -1，也可以设置成all，此机制是producer发送成功数据，并且Leader接收成功，并且Follower也同步成功之后，producer才会发送下一条数据。
   </li>
  </ul>
  <h3>
   说一下Kafka生产者如何生产数据，消费者如何消费数据
  </h3>
  <p>
   可回答：1）Kafka是怎么消费数据的？2）如果我在客户端写一个文件Kafka的整个流程是什么？3）消费的是Leader分区的还是Follwer也可以读？
  </p>
  <p>
   问过的一些公司：百度提前批(2022.09)，蔚来提前批(2021.09)，农行上海研发中心(2020.09)，跟谁学(2020.09)，京东提前批(2020.07)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、Kafka生产者写入（生产）数据流程
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136288374-b4125250-a9ba-40bb-bae0-87994556a426.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_24%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <ol list="u0b141e3a">
   <li fid="uea5416dc">
    生产者先从Zookeeper的 "/brokers/topics/主题名/partitions/分区名/state"节点找到该 partition 的Leader
   </li>
   <li fid="uea5416dc">
    生产者在Zookeeper中找到该ID找到对应的broker
   </li>
   <li fid="uea5416dc">
    broker进程上的Leader将消息写入到本地log中
   </li>
   <li fid="uea5416dc">
    Follower从Leader上拉取消息，写入到本地log，并向Leader发送ACK
   </li>
   <li fid="uea5416dc">
    Leader接收到所有的ISR中的Replica的ACK后，并向生产者返回ACK
   </li>
  </ol>
  <p>
   <strong>
    2、Kafka消费者消费数据流程
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136291088-6fbf0cd8-f172-47f6-9afa-ef6ef966e3ef.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_14%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <ol list="u9efb479f">
   <li fid="u9eb5bd35">
    通过Zookeeper找partition对应的Leader位置以及offset，Leader是负责读的
   </li>
   <li fid="u9eb5bd35">
    找到该分区的Leader，拉取数据
   </li>
   <li fid="u9eb5bd35">
    然后开始从offset往后顺序从本地log中读取数据
   </li>
   <li fid="u9eb5bd35">
    消费者提交offset（自动提交——每隔多少秒提交一次offset、手动提交——放入到事务中提交）
   </li>
  </ol>
  <h3>
   为什么不能读Follwer？
  </h3>
  <p>
   问过的一些公司：跟谁学(2020.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   在Kafka中，实现副本的目的就是冗余备份，且仅仅是冗余备份，所有的读写请求都是由Leader副本进行处理的。Follower副本仅有一个功能，那就是从Leader副本拉取消息，尽量让自己跟Leader副本的内容一致。
  </p>
  <p>
   关于“为什么不能读Follwer”这个问题本质上是对性能和一致性的取舍。试想一下，如果Follower副本也对外提供服务那会怎么样呢？首先，性能是肯定会有所提升的。但同时，会出现一系列问题。类似数据库事务中的幻读，脏读。：
  </p>
  <p>
   比如现在写入一条数据到Kafka主题a，消费者b从主题a消费数据，却发现消费不到，因为消费者b去读取的那个分区副本中，最新消息还没写入。而这个时候，另一个消费者c却可以消费到最新那条数据，因为它消费了Leader副本。
  </p>
  <p>
   如果说为了提高那么些性能而导致出现数据不一致问题，那显然是不值得的。
  </p>
  <h3>
   Kafka监控实现？
  </h3>
  <p>
   问过的一些公司：京东方(2022.07)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   几种监控工具对比：
  </p>
  <p>
   Kafka Manager：雅虎出品，可管理多个Kafka集群，是目前功能最全的管理工具。但是注意，当你的Topic太多，监控数据会占用你大量的带宽，造成你的机器负载增高。其监控功能偏弱，不满足需求。
  </p>
  <p>
   Kafka Offset Monitor：程序一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全。
  </p>
  <p>
   Kafka Web Console：监控功能较为全面，可以预览消息，监控Offset、Lag等信息，不建议在生产环境中使用。
  </p>
  <p>
   Burrow：是LinkedIn开源的一款专门监控consumer lag的框架。支持报警，只提供HTTP接口，没有webui。
  </p>
  <p>
   Availability Monitor for Kafka：微软开源的Kafka可用性、延迟性的监控框架,提供JMX接口，用的很少。
  </p>
  <h3>
   Kafka Follower如何与Leader同步数据
  </h3>
  <p>
   问过的一些公司：兴金数金(2022.07)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136291171-b98e2728-c636-443f-b0db-d4ba3cb6e544.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_15%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。完全同步复制要求All Alive Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下，如果Leader挂掉，会丢失数据。
   <strong>
    Kafka使用ISR的方式很好的均衡了确保数据不丢失以及吞吐率
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。Follower可以批量的从Leader复制数据，而且Leader充分利用磁盘顺序读以及send file(zero copy)机制，这样极大的提高复制性能，内部批量写磁盘，大幅减少了Follower与Leader的消息量差。
   </span>
  </p>
  <p>
   当ISR中的Follower完成数据的同步之后，Leader就会给Follower发送ack。如果Follower长时间未向Leader同步数据，则该Follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的 Leader。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136294881-968672a2-441f-4e1b-9663-31c6a8a1c7bf.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_13%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <h3>
   说下Kafka的ISR机制
  </h3>
  <p>
   可回答：1）从ISR踢出去之后呢；2）一般Leader怎么判断Follower挂掉？
  </p>
  <p>
   问过的一些公司：字节(2022.06)，虎牙(2021.08)，腾讯(2021.04)，小米(2020.10)，电信云(2020.10)，快手提前批(2020.09)，跟谁学(2020.09)，美团(2020.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    ISR
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    （In-Sync Replicas）：副本同步队列
   </span>
  </p>
  <p>
   ISR是Leader维护的一个动态副本同步队列，是和Leader保持同步的Follower集合。Kafka通过多副本来保证消息不丢失。
  </p>
  <p>
   每一个Leader partition都有一个ISR，Leader动态维护，要保证Kafka不丢失message，就要保证ISR这组集合存活（至少有一个存活），并且消息commit成功。Partition Leader保持同步的Partition Follower集合，当ISR中Partition Follower完成数据的同步之后，就会给Leader发送ack。如果Partition Follower长时间（replica.lag.time.max.ms，默认10s）未向Leader同步数据，则该Partition Follower将被踢出ISR，存入OSR（Outof-Sync Replicas）列表（AR = ISR + OSR，AR（Assigned Repllicas）：一个partition的所有副本（就是replica，不区分Leader或Follower）），新加入的Follower也会先存放在OSR中。Partition Leader发生故障之后，就会从ISR中选举新的Partition Leader。
  </p>
  <h3>
   AR、ISR、OSR
  </h3>
  <p>
   问过的一些公司：快手提前批(2020.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   分区中的所有副本统称为AR（Assigned Replicas）。
  </p>
  <p>
   所有与leader副本保持一定程度同步的副本（包括Leader副本在内）组成ISR（In-Sync Replicas），ISR集合是AR集合中的一个子集。
  </p>
  <p>
   与Leader副本同步滞后过多的副本（不包括Leader副本）组成OSR（Out-of-Sync Replicas）
  </p>
  <p>
   消息会先发送到Leader副本，然后Follower副本才能从Leader副本中拉取消息进行同步，同步期间内Follower副本相对于Leader副本而言会有一定程度的滞后。前面所说的“一定程度的同步”是指可忍受的滞后范围，这个范围可以通过参数进行配置。由此可见，AR=ISR+OSR。在正常情况下，所有的Follower副本都应该与Leader副本保持一定程度的同步，即 AR=ISR，OSR集合为空.
  </p>
  <p>
   <strong>
    ISR与OSR转换
   </strong>
  </p>
  <p>
   Leader副本负责维护和跟踪ISR集合中所有Follower副本的滞后状态，当Follower副本落后太多或失效时，Leader副本会把它从ISR集合中剔除。如果OSR集合中有Follower副本“追上”了Leader副本，那么Leader副本会把它从OSR集合转移至ISR集合。
  </p>
  <h3>
   Kafka设置ack=-1时一定会保证消息不丢失吗
  </h3>
  <p>
   问过的一些公司：字节(2022.04)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   与ack=1或0相比，当其他环境相同的情况下，ack设置为-1/all可以达到最强的可靠性，但这并不意味这消息就一定可靠，因为ISR中可能只有Leader副本，这就成了ack=1的情况，所有此时是不安全。
  </p>
  <p>
   Kafka的Broker端提供了一个参数
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    min.insync.replicas
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，该参数控制的是消息至少被写入到多少个副本才算是"真正写入"，该值默认值为1，生产环境设定为一个大于1的值可以提升消息的持久性，因为如果同步副本的数量低于该配置值，则生产者会收到错误响应，从而确保消息不丢失。
   </span>
  </p>
  <p>
   所以想要获得更高的可靠性，需要配合
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    min.insync.replicas
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    等参数的联动，才能尽量保证消息不丢失。
   </span>
  </p>
  <h3>
   Kafka的消费者组作用
  </h3>
  <p>
   问过的一些公司：Flow++(2022.04)，京东寻猎计划(2020.09)，电信云(2020.09)，快手(2019.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、什么是消费者组
   </strong>
  </p>
  <p>
   消费者组就是让若干个消费者实例（Consumer Instance）形成一个消费者组，共同消费共同消费同一个或者多个 Topic 的不同 Partition 中的消息，注意，每一个 Partition 只能由消费者组中的一个 Consumer 来消费。
  </p>
  <p>
   多个消费者组之间是相互独立的，它们可以订阅同样的主题进行消费。
  </p>
  <p>
   <strong>
    2、消费者组是怎么工作的
   </strong>
  </p>
  <p>
   当一个消费者组订阅了一个或者多个 Topic 的时候，这些 Topic 中所有的 Partition 会被「分配」给消费者组中的消费者实例，这些消费者实例各自处理自己分配到的分区的消息。
  </p>
  <p>
   举一个具体的例子。假设一个消费者组订阅了 T1、T2、T3 三个 Topic，他们中分别有3个、5个、10个分区，加起来一共18个分区。此时：
  </p>
  <ul list="u416a2462">
   <li fid="u0ef5d229">
    如果消费者组中正好有18个实例，那么，每个实例负责消费一个分区的消息。
   </li>
   <li fid="u0ef5d229">
    如果消费者组中的实例数超过了总的分区数，比如有20个实例，那么将会有2个实例是空闲的，什么都不做，除非有其他实例挂掉触发 Rebalance。
   </li>
   <li fid="u0ef5d229">
    如果消费者组中有 6 个实例，那么每个实例会被分配3个分区。
   </li>
  </ul>
  <p>
   因此，推荐消费者组中的实例数与订阅的 Topic 的总分区数相同。
  </p>
  <p>
   <strong>
    3、这里再介绍下Rebalance
   </strong>
  </p>
  <p>
   Rebalance 机制是 Kafka 中将消息分区分配给消费者组中的消费者实例的机制。
  </p>
  <p>
   上面的例子中，将 18 个分区分配给 6 个消费者实例的过程，就是 Rebalance。
  </p>
  <p>
   除了在消费者组订阅 Topic 后需要进行这样的分配，还有一些情况会触发 Relalance：
  </p>
  <ul list="u98f6db3c">
   <li fid="u24426ed3">
    消费者组中的实例发生变化的时候，包括但不限于个别消费者实例宕机、有新的实例加入、有实例被踢出等等情况。此时，因为有些分区对应的消费者实例不在组中了，或者有新的实例可以「分担」其他实例的工作，那么，此时会触发 Rebalance。
   </li>
   <li fid="u24426ed3">
    订阅的主题发生变更的时候。因为消费者组可以通过匹配表达式来订阅主题，比如，订阅所有以
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     abc
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     开头的主题。当有新的主题被创建，如果这个主题的名称是以
    </span>
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     abc
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     开头的，那么，消费者组会自动消费这个主题，此时，就需要重新分配分区和消费者实例的关系。
    </span>
   </li>
   <li fid="u24426ed3">
    主题的分区发生变化的时候，无论分区数增多还是减少，都需要重新分配消费者实例，以达到均衡。
   </li>
  </ul>
  <p>
   当以上因素出现的时候，Rebalance 机制能都实现自动并且尽可能均衡的分配，但是，目前它还有一些问题。
  </p>
  <ul list="u14198a44">
   <li fid="udba530f8">
    当执行 Relalance 的时候，消费者组中所有消费者实例都会停止对消息的消费，知道 Rebalance 工作完成。这类似于垃圾回收器中经常会提到了
    stop-the-world
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ，因此，要尽可能避免频繁地 Rebalance。
    </span>
   </li>
   <li fid="udba530f8">
    目前的 Rebalance 操作十分「不智能」，每次 Rebalance 操作都会将所有的分区进行重新分配，而不是采取尽量减少变动的方式，这一问题导致在分区数和消费者实例数比较多的情况下， Rebalance 操作是一个耗时的操作，因此，再次建议，要尽可能避免频繁地 Rebalance。
   </li>
  </ul>
  <p>
   <strong>
    4、消费者组的作用（优势）
   </strong>
  </p>
  <p>
   <strong>
    1）提升消费效率
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136294968-85e7a862-4312-4539-9010-464bdc02060c.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_12%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   假设一个主题有10个分区，如果没有消费者组，只有一个消费者对这10个分区消费，他的压力肯定大。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136295018-f2e6a0e8-4fce-44aa-a9f2-edad1a4c1320.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_14%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   如果有了消费者组，组内的成员就可以分担这10个分区的压力，提高消费性能。
  </p>
  <p>
   <strong>
    2）消费模式更灵活
   </strong>
  </p>
  <p>
   假设有4个消费者订阅一个主题，不同的组合方式就可以形成不同的消费模式。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136294976-e1cd4c69-2b5c-4e27-9b83-af7c841600e0.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_14%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   使用4个消费者组，每组里放一个消费者，利用分区在消费者组间共享的特性，就实现了
   <strong>
    广播（发布订阅）模式
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136294997-3f7bf83a-8ebe-430f-995e-1a5d7ec88f0c.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_12%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   只使用一个消费者组，把4个消费者都放在一起，利用分区在组内成员间互斥的特性，就实现了
   <strong>
    单播（队列）模式
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
  </p>
  <p>
   <strong>
    3）便于故障容灾
   </strong>
  </p>
  <p>
   如果只有一个消费者，出现故障后就比较麻烦了，但有了消费者组之后就方便多了。
  </p>
  <p>
   消费组会对其成员进行管理，在有消费者加入或者退出后，消费者成员列表发生变化，消费组就会执行再平衡的操作。
  </p>
  <p>
   例如一个消费者宕机后，之前分配给他的分区会重新分配给其他的消费者，实现消费者的故障容错。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136296371-ac88b038-9588-4a87-a5ed-792b1de996f6.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_12%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <h3>
   Kafka分区的减少和扩大，为什么
  </h3>
  <p>
   问过的一些公司：网易(2022.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   目前Kafka只支持增加分区数而不支持减少分区数。我们可以使用
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    bin/kafka-topics.sh
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    命令对增加Kafka的分区数据，但是Kafka不支持减少分区数。如果当前分区数为3，修改为1，就会报出InvalidPartitionException的异常。
   </span>
  </p>
  <p>
   Kafka分区数据不支持减少是由很多原因造成的，比如减少的分区其数据放到哪里去？是删除，还是保留？删除的话，那么这些没消费的消息不就丢了。如果保留这些消息如何放到其他分区里面？追加到其他分区后面的话那么就破坏了Kafka单个分区的有序性。如果要保证删除分区数据插入到其他分区保证有序性，那么实现起来逻辑就会非常复杂。
  </p>
  <h3>
   Kafka的数据积压和数据倾斜问题
  </h3>
  <p>
   可回答：1）Kafka中出现数据堆积如何处理？
  </p>
  <p>
   问过的一些公司：佳都科技(2022.10)，OPPO(2022.03)，茄子科技(2021.09)，北京元安物联社招(2020.10)，有赞(2020.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    Kafka消息积压的典型场景：
   </strong>
  </p>
  <p>
   <strong>
    1、实时/消费任务挂掉
   </strong>
  </p>
  <p>
   比如，我们写的实时应用因为某种原因挂掉了，并且这个任务没有被监控程序监控发现通知相关负责人，负责人又没有写自动拉起任务的脚本进行重启。
  </p>
  <p>
   那么在我们重新启动这个实时应用进行消费之前，这段时间的消息就会被滞后处理，如果数据量很大，可就不是简单重启应用直接消费就能解决的。
  </p>
  <p>
   <strong>
    2、Kafka分区数设置的不合理（太少）和消费者"消费能力"不足
   </strong>
  </p>
  <p>
   Kafka单分区生产消息的速度qps通常很高，如果消费者因为某些原因（比如受业务逻辑复杂度影响，消费时间会有所不同），就会出现消费滞后的情况。
  </p>
  <p>
   此外，Kafka分区数是Kafka并行度调优的最小单元，如果Kafka分区数设置的太少，会影响Kafka consumer消费的吞吐量。
  </p>
  <p>
   <strong>
    3、Kafka消息的key不均匀，导致分区间数据不均衡
   </strong>
  </p>
  <p>
   在使用Kafka producer消息时，可以为消息指定key，但是要求key要均匀，否则会出现Kafka分区间数据不均衡。
  </p>
  <p>
   <strong>
    一般情况下，针对性的解决办法有以下几种：
   </strong>
  </p>
  <p>
   <strong>
    1、实时/消费任务挂掉导致的消费滞后
   </strong>
  </p>
  <p>
   在积压数据不多和影响较小的情况下，重新启动消费任务，排查宕机原因。
  </p>
  <p>
   如果消费任务宕机时间过长导致积压数据量很大，除了重新启动消费任务、排查问题原因，还需要解决消息积压问题。
  </p>
  <p>
   此时解决消息积压可以采用下面方法：
  </p>
  <ul list="u659085aa">
   <li fid="ucbc74688">
    任务重新启动后直接消费最新的消息，对于"滞后"的历史数据采用离线程序进行"补漏"。
   </li>
   <li fid="ucbc74688">
    任务启动从上次提交offset处开始消费处理。如果积压的数据量很大，需要增加任务的处理能力，比如增加资源，让任务能尽可能的快速消费处理，并赶上消费最新的消息
   </li>
  </ul>
  <p>
   <strong>
    2、Kafka分区数设置的不合理或消费者"消费能力"不足的优化
   </strong>
  </p>
  <p>
   Kafka分区数是Kafka并行度调优的最小单元，如果Kafka分区数设置的太少，会影响Kafka Consumer消费的吞吐量。
  </p>
  <p>
   如果数据量很大，Kafka消费能力不足，则可以考虑增加Topic的Partition的个数，同时提升消费者组的消费者数量。
  </p>
  <p>
   <strong>
    3、由于Kafka消息key设置的不合理，导致分区数据不均衡
   </strong>
  </p>
  <p>
   使用Kafka Producer消息时，可以为消息指定key，但是要求key要均匀，否则会出现Kafka分区间数据不均衡。所以根据业务，合理修改Producer处的key设置规则。
  </p>
  <h3>
   说一下Kafka比其他中间件的优势
  </h3>
  <p>
   可回答：1）Kafka相比于其它消息组件有什么好处？
  </p>
  <p>
   问过的一些公司：虎牙(2021.08)，美团(2019.11)，美团新到店(2018.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、高吞吐量、低延迟
   </strong>
  </p>
  <p>
   Kafka最大的优点就是吞吐量高。一般配合大数据类系统来进行实时计算，日志采集等场景来使用。即使在非常廉价的机器上，Kafka也能做到每秒处理几十万条消息，而它的延迟最低只有几毫秒
  </p>
  <p>
   <strong>
    2、高可用性
   </strong>
  </p>
  <p>
   可用性非常高，Kafka是分布式的，一个数据多个副本，少数机器宕机不会数据丢失，不会导致不可用。
  </p>
  <p>
   <strong>
    3、高可靠性
   </strong>
  </p>
  <p>
   经过参数优化配置，可以达到0丢失。
  </p>
  <p>
   <strong>
    4、支持集群热扩展
   </strong>
  </p>
  <p>
   Kafka集群启动运行后，用户可以直接向集群添加实例。
  </p>
  <h3>
   Kafka生产者与消费者
  </h3>
  <p>
   问过的一些公司：深信服(2022.09)，吉利(2022.09)，荣耀(2021.09)，大华(2021.07)，电信云(2020.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    生产者：Producer
   </strong>
  </p>
  <p>
   即消息的发布者，生产者将数据发布到他们选择的主题。
  </p>
  <p>
   生产者负责选择将哪个记录分配给主题中的哪个分区。即：生产者生产的一条消息，会被写入到某一个Partition。
  </p>
  <p>
   <strong>
    消费者：Consumer
   </strong>
  </p>
  <p>
   可以从Broker中读取消息。
  </p>
  <p>
   Kafka的消费者，一个消费者可以消费多个Topic的消息；一个消费者可以消费同一个Topic中的多个Partition中的消息；一个Partiton允许多个Consumer同时消费。
  </p>
  <h3>
   Kafka分区容错性
  </h3>
  <p>
   问过的一些公司：大华(2021.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka集群中，单个节点或多个节点出问题（主要是网络问题）后，正常服务的服务器依然能正常提供服务，并且满足设计好的一致性和可用性。
  </p>
  <p>
   重点在于：部分节点因网络问题，业务依然能够继续运行。
  </p>
  <p>
   如果是整个网络都出现故障，那就没辙了。
  </p>
  <h3>
   Kafka的数据存在内存还是磁盘
  </h3>
  <p>
   问过的一些公司：唯品会社招(2021.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka的数据是存储在磁盘上。
  </p>
  <p>
   Kafka速度快的原因如下：
  </p>
  <ul list="u5db3d4ab">
   <li fid="u9392042f">
    顺序写入。磁盘是机械结构，每次读写都会寻址-&gt;写入，其中寻址是一个“机械动作”，它是耗时的。所以硬盘“讨厌”随机I/O，喜欢顺序I/O。为了提高读写硬盘的速度，Kafka就是使用顺序I/O。如果一个topic建立多个分区那么每个parathion都是一个文件，收到消息后Kafka会把数据插入到文件末尾。
   </li>
   <li fid="u9392042f">
    Memory Mapped Files（内存映射文件）。64位操作系统中一般可以表示20G的数据文件，它的工作原理是直接利用操作系统的Page来实现文件到物理内存的直接映射。完成映射之后你对物理内存的操作会被同步到硬盘上。
   </li>
   <li fid="u9392042f">
    Kafka高效文件存储设计特点。Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。通过索引信息可以快速定位message和确定response的大小。通过index元数据全部映射到memory（内存映射文件），可以避免segment file的IO磁盘操作。通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。
   </li>
  </ul>
  <h3>
   怎么查看Kafka的offset
  </h3>
  <p>
   问过的一些公司：唯品会社招(2021.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   查看 Kafka 的 offset，可以使用 Kafka 提供的命令行工具
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    kafka-consumer-groups.sh
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    或者通过程序 API 进行查询。
   </span>
  </p>
  <p>
   使用命令行工具
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    kafka-consumer-groups.sh
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    查询时，需要指定消费者组、Kafka 集群地址、topic 名称等参数，具体命令为：
   </span>
  </p>
  <p>
   ./kafka-consumer-groups.sh --bootstrap-server &lt;kafka地址&gt; --group &lt;消费者组&gt; --describe --topic &lt;topic名称&gt;
  </p>
  <p>
   执行该命令后，会返回当前消费者组对应的消费者列表，以及每个消费者对应的 partition 以及对应 partition 的 offset 信息。
  </p>
  <p>
   0.9 版本以上，可以用最新的 Consumer client 客户端，有 consumer.seekToEnd() / consumer.position() 可以用于得到当前最新的 offset。
  </p>
  <h3>
   Kafka的Message包括哪些信息
  </h3>
  <p>
   问过的一些公司：唯品会社招(2021.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   一个 Kafka 的 Message 由一个固定长度的 header 和一个变长的消息体 body 组成，header 部分由一个字节的 magic（文件格式）和四个字节的 crc32（用于判断 body 消息体是否正常）构成。当 magic 的值为 1 的时候，会在 magic 和 crc32 之间多一个字节的数据：attributes（保存一些相关属性，比如是否压缩、压缩格式等等）；如果 magic 的值为 0，那么不存在 attributes 属性。
  </p>
  <p>
   body 是由 N 个字节构成的一个消息体，包含了具体的 key/value 消息
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662719725655-32a1b93b-310e-4bb7-a51d-5de1c04f6a5b.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_27%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   具体字段解释如下：
  </p>
  <ul list="u4e52c754">
   <li fid="ue4e6e110">
    CRC32：4个字节，消息的校验码。
   </li>
   <li fid="ue4e6e110">
    magic：1字节，魔数标识，与消息格式有关，取值为0或1。当magic为0时，消息的offset使用绝对offset且消息格式中没有timestamp部分；当magic为1时，消息的offset使用相对offset且消息格式中存在timestamp部分。所以，magic值不同，消息的长度是不同的。
   </li>
   <li fid="ue4e6e110">
    attributes： 1字节，消息的属性。其中第0~ 2位的组合表示消息使用的压缩类型，0表示无压缩，1表示gzip压缩，2表示snappy压缩，3表示lz4压缩。第3位表示时间戳类型，0表示创建时间，1表示追加时间。
   </li>
   <li fid="ue4e6e110">
    timestamp： 时间戳，其含义由attributes的第3位确定。
   </li>
   <li fid="ue4e6e110">
    key length：消息key的长度。
   </li>
   <li fid="ue4e6e110">
    key：消息的key。
   </li>
   <li fid="ue4e6e110">
    value length：消息的value长度。
   </li>
   <li fid="ue4e6e110">
    value：消息的内容
   </li>
  </ul>
  <h3>
   Kafka高可用体现在哪里
  </h3>
  <p>
   可回答：1）kafka的高可用是怎么保证的？
  </p>
  <p>
   问过的一些公司：58同城(2021.08)，字节社招(2021.05)，富途(2021.03)，字节(2020.09)，蘑菇街实习(2020.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   高可用性（High Availability），指系统无间断地执其功能的能，代表系统的可性程度。
  </p>
  <p>
   Kafka 0.8以前，是没有HA机制的，就是任何一个broker宕机了，那个broker上的partition就废了，没法写也没法读，没有什么高可用性可言。
  </p>
  <p>
   Kafka 0.8以后，提供了HA机制，就是replica副本机制。每个partition的数据都会同步到其他机器上，形成自己的多个replica副本。然后所有replica会选举一个Leader出来，那么生产和消费都跟这个Leader打交道，然后其他replica就是Follower。写的时候，Leader会负责把数据同步到所有Follower上去，读的时候就直接读Leader上数据即可。只能读写Leader？很简单，要是你可以随意读写每个Follower，那么就要考虑数据一致性的问题，系统复杂度太高，很容易出问题。Kafka会均匀的将一个partition的所有replica分布在不同的机器上，这样才可以提高容错性。
  </p>
  <p>
   Kafka的这种机制，就是所谓的高可用性了，因为如果某个broker宕机了，也没事儿，因为那个broker上面的partition在其他机器上都有副本的，那么此时会重新选举一个新的Leader出来，大家继续读写那个新的Leader即可。这就有所谓的高可用性了。
  </p>
  <p>
   主要是提现在消息备份、ISR、ACK、LEO、HW、故障恢复机制方面。
  </p>
  <h3>
   Kafka支持什么语义，怎么实现Exactly Once？
  </h3>
  <p>
   可回答：1）幂等性如何保证exactly-once；2）Kafka怎么保证只发送一次
  </p>
  <p>
   问过的一些公司：贝壳(2022.09)，万兴科技(2022.09)，字节社招(2021.05)，蔚来(2021.04)，美团实习(2021.04)，快手(2020.03)x2，360社招(2020.01)，乐言科技(2019.03)，快手(2019.09)，美团新到店(2018.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka支持的
   <strong>
    消费语义
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    有以下几种：
   </span>
  </p>
  <ul list="ud2a1ae6f">
   <li fid="u8972b94f">
    at most once：最多消费一次，消息可能会丢失-------log日志
   </li>
   <li fid="u8972b94f">
    at least once：至少消费一次，但是会重复消费 例如手动异步提交offset
   </li>
   <li fid="u8972b94f">
    exactly once：正好一次，不丢失，不重复
   </li>
  </ul>
  <p>
   <strong>
    1）至少一次
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：at-least-once 表示的是关闭offset自动提交功能，消费端在消费数据的时候很可能在commitAync之前，已经保存在数据库，但是这个时候服务器宕机了，从而导致offset不能提交成功。这个时候再次启动消费者的时候，还是会再次写入数据库，也就是至少一次会重复消费，至少不会丢数据。
   </span>
  </p>
  <p>
   <strong>
    2）至多一次
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：at-most-once 表示有可能是0次或者1次，可以选择开启自动提交offset的功能，然后把自动提交offset的时间设置一下，有可能消费者在消费的时间段内就到了自动提交的时间，从而导致了offset已经提交了，但是数据库保存还没进行。下一次再消费的时候就会认为offset已经成功了，直接丢弃消息。就会造成丢数据。
   </span>
  </p>
  <p>
   <strong>
    3）仅一次
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：exactly-once 表示数据仅被消费一次，还是开启自动开启offset提交的功能，可以开启consumer.seek()方法，相当于自己处理分区和offset，可以在此基础上开启事务，保持原子性，只有数据库保存成功再提交offset，保证两者同时成功。
   </span>
  </p>
  <p>
   <strong>
    Exactly Once语义
   </strong>
  </p>
  <p>
   将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once语义。
  </p>
  <p>
   At Least Once可以保证数据不丢失，但是不能保证数据不重复；相对的，At Least Once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义。在0.11版本以前的Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。
  </p>
  <p>
   0.11版本的Kafka，引入了一项重大特性：
   <strong>
    幂等性
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。所谓的幂等性就是
   </span>
   <strong>
    指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
   <strong>
    幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。即：
   </span>
  </p>
  <p>
   At Least Once + 幂等性 = Exactly Once
  </p>
  <p>
   要启用幂等性，只需要将Producer的参数中enable.idompotence设置为True即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。
  </p>
  <p>
   但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。
  </p>
  <h3>
   Kafka数据丢失怎么处理？
  </h3>
  <p>
   可回答：1）Kafka的生产者写数据丢数据怎么办；2）Kafka传输过程中断电了，怎么保证可靠性？
  </p>
  <p>
   问过的一些公司：字节社招(2021.05)，网易(2021.05)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    Producer 数据不丢失：
   </strong>
  </p>
  <ul list="udde3aebc">
   <li fid="u90bfbba3">
    同步模式：配置ack=1 （只有Leader收到，-1 所有副本成功，0 不等待）Leader Partition挂了，数据就会丢失
    <strong>
     解决方案
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ：设置 ack=-1 保证produce 写入所有副本算成功 producer.type = sync request.required.acks=-1
    </span>
   </li>
   <li fid="u90bfbba3">
    异步模式，当缓冲区满了，如果配置为0（没有收到确认，一满就丢弃），数据立刻丢弃
    <strong>
     解决方案
    </strong>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ：不限制阻塞超时时间。就是一满生产者就阻塞
    </span>
   </li>
  </ul>
  <pre><code class="language-plain" lang="plain">producer.type = async
request.required.acks=1
queue.buffering.max.ms=5000
queue.buffering.max.messages=10000
queue.enqueue.timeout.ms = -1
batch.num.messages=200</code></pre>
  <p>
   <strong>
    Customer 不丢失数据
   </strong>
  </p>
  <p>
   在获取Kafka的消息后正准备入库（未入库），但是消费者挂了，那么如果让Kafka自动去维护offset ，它就会认为这条数据已经被消费了，那么会造成数据丢失。
  </p>
  <p>
   <strong>
    解决方案
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：使用Kafka低级API，自己手动维护偏移量，当数据入库之后进行偏移量的更新（适用于基本数据源）
   </span>
  </p>
  <p>
   流处理中的几种可靠性语义：
  </p>
  <ul list="uc373325f">
   <li fid="u837055cf">
    at most once 每条数据最多被处理一次（0次或1次），会出现数据丢失的问题
   </li>
   <li fid="u837055cf">
    at least once 每条数据最少被处理一次（1次或更多），这个不会出现数据丢失，但是会出现数据重复
   </li>
   <li fid="u837055cf">
    exactly once 每种数据只会被处理一次，没有数据丢失，没有数据重复，这种语义是大家最想实现的，也是最难实现的
   </li>
  </ul>
  <p>
   但是开启WAL后，依旧存在数据丢失问题，原因是任务中断时receiver也被强行终止了，将会造成数据丢失。在Streaming程序的最后添加代码，只有在确认所有receiver都关闭的情况下才终止程序。
  </p>
  <h3>
   Kafka和传统消息队列的区别
  </h3>
  <p>
   问过的一些公司：网易(2021.05)，快手(2020.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka 持久化日志，这些日志可以被重复读取和无限期保留
  </p>
  <p>
   Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据提升容错能力和高可用性
  </p>
  <p>
   Kafka 支持实时的流式处理
  </p>
  <p>
   <strong>
    四大主流MQ（kafka、ActiveMQ、RabbitMQ、RocketMQ）各自的优缺点
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136296430-a1262983-600b-4ce4-8131-ef1979f05b13.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_23%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <h3>
   Kafka为什么同一个消费者组的消费者不能消费相同的分区
  </h3>
  <p>
   问过的一些公司：网易(2021.05)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   在设计Kafka的时候就是要保证分区下消息的顺序，也就是说消息在一个分区中的顺序是怎样的，那么消费者在消费的时候就是什么样的顺序，要做到这一点首先要保证消息是由消费者主动拉取的（pull），其次要保证一个分区只能由一个消费者负责。
  </p>
  <p>
   如果两个消费者负责同一个分区，那么就意味着两个消费者同时读取分区的消息，由于消费者自己可以控制读取消息的offset，就有可能C2读到2，而C1才读到1，C1还没处理完，C2已经读到3了，则会造成浪费，这就相当于多线程读取同一个消息，
   <strong>
    会造成消息处理（消费）的重复，且不能保证消息的顺序
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，这就跟主动推送（push）无异，而且可能还会重复消费消息，没什么意义。
   </span>
  </p>
  <h3>
   Kafka新旧API区别
  </h3>
  <p>
   问过的一些公司：字节日常实习(2021.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、高级API
   </strong>
  </p>
  <p>
   优点：
  </p>
  <ul list="u8413f17c">
   <li fid="ua29e28d2">
    高级API写起来简单
   </li>
   <li fid="ua29e28d2">
    不需要去自行去管理offset，系统通过zookeeper自行管理
   </li>
   <li fid="ua29e28d2">
    不需要管理分区，副本等情况，系统自动管理
   </li>
   <li fid="ua29e28d2">
    消费者断线会自动根据上一次记录在 zookeeper中的offset去接着获取数据
   </li>
   <li fid="ua29e28d2">
    可以使用group来区分对访问同一个topic的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响）
   </li>
  </ul>
  <p>
   缺点：
  </p>
  <ul list="u7a56fe36">
   <li fid="u31243386">
    不能自行控制 offset（对于某些特殊需求来说）
   </li>
   <li fid="u31243386">
    不能细化控制如分区、副本、zk 等
   </li>
  </ul>
  <p>
   <strong>
    2、低级API
   </strong>
  </p>
  <p>
   优点：
  </p>
  <ul list="u6a23dec7">
   <li fid="u783f982f">
    能够开发者自己控制offset，想从哪里读取就从哪里读取
   </li>
   <li fid="u783f982f">
    自行控制连接分区，对分区自定义进行负载均衡
   </li>
   <li fid="u783f982f">
    对 zookeeper 的依赖性降低（如：offset 不一定非要靠 zk 存储，自行存储offset 即可，比如存在文件或者内存中）
   </li>
  </ul>
  <p>
   缺点：
  </p>
  <ul list="u0090ac25">
   <li fid="u254d19c0">
    太过复杂，需要自行控制offset，连接哪个分区，找到分区leader等
   </li>
  </ul>
  <h3>
   说下Kafka中的Partition？
  </h3>
  <p>
   可回答：1）Kafka为什么使用会有partition的数据结构，这样做的好处？（中间的一部分可回答）；2）Kafka中的partition如何保证有序？3）Kafka的分区
  </p>
  <p>
   问过的一些公司：字节日常实习(2020.11)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   为了后面更好的理解，前面先介绍下几个概念
  </p>
  <p>
   <strong>
    1、Events，Streams，Topics
   </strong>
  </p>
  <p>
   <strong>
    Event
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    （事件）代表过去发生的一个事实。简单理解就是一条消息、一条记录。
   </span>
  </p>
  <p>
   Event 是不可变的，但是很活跃，经常从一个地方流向另一个地方。
  </p>
  <p>
   <strong>
    Stream
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    事件流表示运动中的相关事件。
   </span>
  </p>
  <p>
   当一个事件流进入 Kafka 之后，它就成为了一个
   <strong>
    Topic
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    主题。
   </span>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136296431-3c077548-29b6-4088-b211-5f260de704cc.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_29%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   所以，Topic 就是具体的事件流，也可以理解为一个 Topic 就是一个静止的 Stream。
  </p>
  <p>
   Topic 把相关的 Event 组织在一起，并且保存。一个 Topic 就像数据库中的一张表。
  </p>
  <p>
   <strong>
    2、Partition分区
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136298819-525fa4b1-a1f6-49e5-be04-f07b74a39eb6.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_25%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   Kafka 中 Topic 被分成多个 Partition 分区。
  </p>
  <p>
   Topic 是一个
   <strong>
    逻辑概念
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，Partition 是最小的
   </span>
   <strong>
    存储单元
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，掌握着一个 Topic 的部分数据。
   </span>
  </p>
  <p>
   每个 Partition 都是一个单独的 log 文件，每条记录都以追加的形式写入。
  </p>
  <p>
   Record（记录） 和 Message（消息）是一个概念。
  </p>
  <p>
   <strong>
    3、Offsets（偏移量）和消息的顺序（
   </strong>
   <strong>
    可回答：Kafka中的partition如何保证有序
   </strong>
   <strong>
    ）
   </strong>
  </p>
  <p>
   Partition 中的每条记录都会被分配一个唯一的序号，称为
   <strong>
    Offset
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    （偏移量）。
   </span>
  </p>
  <p>
   Offset 是一个递增的、不可变的数字，由 Kafka 自动维护。
  </p>
  <p>
   当一条记录写入 Partition 的时候，它就被追加到 log 文件的末尾，并被分配一个序号，作为 Offset。
  </p>
  <p>
   如上图，这个 Topic 有 3 个 Partition 分区，向 Topic 发送消息的时候，实际上是被写入某一个 Partition，并赋予 Offset。
  </p>
  <p>
   消息的顺序性需要注意，一个 Topic 如果有多个 Partition 的话，那么从 Topic 这个层面来看，消息是无序的。
  </p>
  <p>
   但单独看 Partition 的话，Partition 内部消息是有序的。所以，一个 Partition 内部消息有序，一个 Topic 跨 Partition 是无序的。如果强制要求 Topic 整体有序，就只能让 Topic 只有一个 Partition。
  </p>
  <p>
   <strong>
    4、Partition 为 Kafka 提供了扩展能力（
   </strong>
   <strong>
    可回答partition的作用
   </strong>
   <strong>
    ）
   </strong>
  </p>
  <p>
   一个 Kafka 集群由多个 Broker（就是 Server） 构成，每个 Broker 中含有集群的部分数据。
  </p>
  <p>
   Kafka 把 Topic 的多个 Partition 分布在多个 Broker 中。
  </p>
  <p>
   这样会有多种好处：
  </p>
  <ul list="u2b981e34">
   <li fid="ua46e04cf">
    如果把 Topic 的所有 Partition 都放在一个 Broker 上，那么这个 Topic 的可扩展性就大大降低了，会受限于这个 Broker 的 IO 能力。把 Partition 分散开之后，Topic 就可以水平扩展 。
   </li>
   <li fid="ua46e04cf">
    一个 Topic 可以被多个 Consumer 并行消费。如果 Topic 的所有 Partition 都在一个 Broker，那么支持的 Consumer 数量就有限，而分散之后，可以支持更多的 Consumer。
   </li>
   <li fid="ua46e04cf">
    一个 Consumer 可以有多个实例，Partition 分布在多个 Broker 的话，Consumer 的多个实例就可以连接不同的 Broker，大大提升了消息处理能力。可以让一个 Consumer 实例负责一个 Partition，这样消息处理既清晰又高效。
   </li>
  </ul>
  <p>
   <strong>
    5、Partition 为 Kafka 提供了数据冗余（
   </strong>
   <strong>
    可回答partition的作用
   </strong>
   <strong>
    ）
   </strong>
  </p>
  <p>
   Kafka 为一个 Partition 生成多个副本，并且把它们分散在不同的 Broker。
  </p>
  <p>
   如果一个 Broker 故障了，Consumer 可以在其他 Broker 上找到 Partition 的副本，继续获取消息。
  </p>
  <p>
   <strong>
    6、写入Partition
   </strong>
  </p>
  <p>
   一个Topic 有多个 Partition，那么，向一个 Topic 中发送消息的时候，具体是写入哪个 Partition 呢？有3种写入方式。
  </p>
  <p>
   <strong>
    1）使用 Partition Key 写入特定 Partition
   </strong>
  </p>
  <p>
   Producer 发送消息的时候，可以指定一个 Partition Key，这样就可以写入特定 Partition 了。
  </p>
  <p>
   Partition Key 可以使用任意值，例如设备ID、User ID。Partition Key 会传递给一个 Hash 函数，由计算结果决定写入哪个 Partition。所以，有相同 Partition Key 的消息，会被放到相同的 Partition。
  </p>
  <p>
   例如使用 User ID 作为 Partition Key，那么此 ID 的消息就都在同一个 Partition，这样可以保证此类消息的有序性。
  </p>
  <p>
   这种方式需要注意 Partition 热点问题。
  </p>
  <p>
   例如使用 User ID 作为 Partition Key，如果某一个 User 产生的消息特别多，是一个头部活跃用户，那么此用户的消息都进入同一个 Partition 就会产生热点问题，导致某个 Partition 极其繁忙。
  </p>
  <p>
   <strong>
    2）由Kafka决定
   </strong>
  </p>
  <p>
   如果没有使用 Partition Key，Kafka 就会使用轮询的方式来决定写入哪个 Partition。这样，消息会均衡的写入各个 Partition。但这样无法确保消息的有序性。
  </p>
  <p>
   <strong>
    3）自定义规则
   </strong>
  </p>
  <p>
   Kafka 支持自定义规则，一个 Producer 可以使用自己的分区指定规则。
  </p>
  <p>
   <strong>
    7、读取Partition
   </strong>
  </p>
  <p>
   Kafka 不像普通消息队列具有发布/订阅功能，Kafka 不会向 Consumer 推送消息。Consumer 必须自己从 Topic 的 Partition 拉取消息。一个 Consumer 连接到一个 Broker 的 Partition，从中依次读取消息。
  </p>
  <p>
   消息的 Offset 就是 Consumer 的游标，根据 Offset 来记录消息的消费情况。读完一条消息之后，Consumer 会推进到 Partition 中的下一个 Offset，继续读取消息。Offset 的推进和记录都是 Consumer 的责任，Kafka 是不管的。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136299026-421b34ef-e952-4cb3-a314-fb5de0fe496a.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_14%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   Kafka 中有一个 Consumer Group（消费组）的概念，多个 Consumer 组团去消费一个 Topic。同组的 Consumer 有相同的 Group ID。Consumer Group 机制会保障一条消息只被组内唯一一个 Consumer 消费，不会重复消费。消费组这种方式可以让多个 Partition 并行消费，大大提高了消息的消费能力，最大并行度为 Topic 的 Partition 数量。
  </p>
  <p>
   例如一个 Topic 有 3 个 Partition，你有 4 个 Consumer 负责这个 Topic，也只会有 Consumer 工作，另一个作为后补队员，当某个 Consumer 故障了，它再补上去，是一种很好的容错机制。
  </p>
  <h3>
   Kafka集群为什么挂掉一个broker后还能工作？
  </h3>
  <p>
   问过的一些公司：字节日常实习(2020.11)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka使用了
   <strong>
    副本机制
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，每个分区的数据会被复制到多个broker上，即使某个broker挂掉，其他broker上的副本也能继续服务，确保了整个集群的可用性。
   </span>
  </p>
  <h3>
   Kafka一个生产者可以把消息发到多个分区吗？
  </h3>
  <p>
   可回答：1）副本是怎么同步消息的？
  </p>
  <p>
   问过的一些公司：字节日常实习(2020.11)，京东寻猎计划(2020.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   ISR（in-sync replica）是Kafka为某个分区维护的一组同步集合，每个分区都有自己的一个ISR集合，处于ISR集合中的副本，意味着Follower副本与Leader副本保持同步状态。
  </p>
  <p>
   当生存者发送消息时，Leader副本写入消息，Follower主动向Leader副本请求同步Kafka日志，保证主从副本数据保持一致。
  </p>
  <h3>
   Kafka消息在磁盘上的组织方式
  </h3>
  <p>
   问过的一些公司：字节日常实习(2020.11)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka为了保证消息的可靠性，服务端会将接收的消息进行序列化并保存到磁盘上（Kafka的多副本存储机制），这里涉及到消息的存储格式，即消息编码后落到磁盘文件上的二进制的数据格式。下图是根据Kafka 3.0官方文档整理的消息格式：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136300595-26decef1-d787-4118-9e8f-436d0312c54e.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_30%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   包含三个部分：BatchRecords、Record，以及Header的编码格式。
  </p>
  <p>
   BatchRecords是Kafka数据的存储单元，一个BatchRecords中包含多个Record（即我们通常说的一条消息）。BatchRecords中各个字段的含义如下：
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="375"/>
    <col width="375"/>
   </colgroup>
   <tbody>
    <tr>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        字段名
       </strong>
      </p>
     </td>
     <td style="background-color: rgb(242, 242, 242)">
      <p style="text-align: left">
       <strong>
        含义
       </strong>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       baseOffset
      </p>
     </td>
     <td>
      <p style="text-align: left">
       这批消息的起始Offset
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       partitionLeaderEpoch
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       用于Partition的Recover时保护数据的一致性，具体场景可以见KIP101
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       batchLength
      </p>
     </td>
     <td>
      <p style="text-align: left">
       BatchRecords的长度
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       magic
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       魔数字段，可以用于拓展存储一些信息，当前3.0版本的magic是2
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       crc
      </p>
     </td>
     <td>
      <p style="text-align: left">
       crc校验码，包含从attributes开始到BatchRecords结束的数据的校验码
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       attributes
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       int16，其中bit0
       2中包含了使用的压缩算法，bit3是timestampType，bit4表示是否失误，bit5表示是否是控制指令，bit6
       <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
        15暂未使用
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       lastOffsetDelta
      </p>
     </td>
     <td>
      <p style="text-align: left">
       BatchRecords中最后一个Offset，是相对baseOffset的值
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       firstTimestamp
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       BatchRecords中最小的timestamp
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       maxTimestamp
      </p>
     </td>
     <td>
      <p style="text-align: left">
       BatchRecords中最大的timestamp
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       producerId
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       发送端的唯一ID，用于做消息的幂等处理
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       producerEpoch
      </p>
     </td>
     <td>
      <p style="text-align: left">
       发送端的Epoch，用于做消息的幂等处理
      </p>
     </td>
    </tr>
    <tr>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       baseSequence
      </p>
     </td>
     <td style="background-color: rgb(250, 250, 250)">
      <p style="text-align: left">
       BatchRecords的序列号，用于做消息的幂等处理
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       records
      </p>
     </td>
     <td>
      <p style="text-align: left">
       具体的消息内容
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   一个BatchRecords中可以包含多条消息，即上图中的Record，而每条消息又可以包含多个Header信息，Header是Key-Value形式的。
  </p>
  <h3>
   Zookeeper在Kafka的作用
  </h3>
  <p>
   可回答：Kafka在什么地方需要用到Zookeeper
  </p>
  <p>
   问过的一些公司：小米(2020.10)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    简洁版：
   </strong>
  </p>
  <p>
   Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。
  </p>
  <p>
   Controller的管理工作都是依赖于Zookeeper的。也就是说Zookeeper是辅助Controller的管理工作的。
  </p>
  <p>
   以下为partition的leader选举过程：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136301737-cf902952-e35c-4016-a582-952d51eeef21.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_20%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    详细版：
   </strong>
  </p>
  <p>
   <strong>
    1、Broker注册
   </strong>
  </p>
  <p>
   <strong>
    Broker是分布式部署并且相互之间相互独立，但是需要有一个注册系统能够将整个集群中的Broker管理起来
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，此时就使用到了Zookeeper。在Zookeeper上会有一个专门
   </span>
   <strong>
    用来进行Broker服务器列表记录
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    的节点：
   </span>
  </p>
  <p>
   /brokers/ids
  </p>
  <p>
   每个Broker在启动时，都会到Zookeeper上进行注册，即到/brokers/ids下创建属于自己的节点，如/brokers/ids/[0...N]。
  </p>
  <p>
   Kafka使用了全局唯一的数字来指代每个Broker服务器，不同的Broker必须使用不同的Broker ID进行注册，创建完节点后，
   <strong>
    每个Broker就会将自己的IP地址和端口信息记录
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    到该节点中去。其中，Broker创建的节点类型是临时节点，一旦Broker宕机，则对应的临时节点也会被自动删除。
   </span>
  </p>
  <p>
   <strong>
    2、Topic注册
   </strong>
  </p>
  <p>
   在 Kafka 中，所有 Topic 与 Broker 的对应关系都由 ZooKeeper 来维护，在 ZooKeeper 中，通过建立专属的节点来存储这些信息，其路径为 ：/brokers/topics/{topic_name}
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136302182-43deb1bf-adfa-4750-89ae-e60e217151c9.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_22%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   为了保障数据的一致性，ZooKeeper 机制得以引入。基于 ZooKeeper，Kafka 为每一个 Partition 找一个节点作为 Leader，其余备份作为 Follower；接续上图的例子，就 TopicA 的 Partition1 而言，如果位于 Broker2（Kafka 节点）上的 Partition1 为 Leader，那么位于 Broker1 和 Broker4 上面的 Partition1 就充当 Follower，则有下图：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136303213-f1164444-d8cd-4e28-acde-89ff7b15ad79.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_22%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   基于上图的架构，当 Producer Push 的消息写入 Partition（分区）时，作为 Leader 的 Broker（Kafka 节点）会将消息写入自己的分区，同时还会将此消息复制到各个 Follower，实现同步。如果某个 Follower 挂掉，Leader 会再找一个替代并同步消息；如果 Leader 挂了，Follower 们会选举出一个新的 Leader 替代，继续业务，这些都是由 ZooKeeper 完成的。
  </p>
  <p>
   <strong>
    3、生产者负载均衡
   </strong>
  </p>
  <p>
   由于同一个Topic消息会被分区并将其分布在多个Broker上，因此，
   <strong>
    生产者需要将消息合理地发送到这些分布式的Broker上
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，那么如何实现生产者的负载均衡，Kafka支持传统的四层负载均衡，也支持Zookeeper方式实现负载均衡。
   </span>
  </p>
  <p>
   1）四层负载均衡，根据生产者的IP地址和端口来为其确定一个相关联的Broker。通常，一个生产者只会对应单个Broker，然后该生产者产生的消息都发往该Broker。这种方式逻辑简单，每个生产者不需要同其他系统建立额外的TCP连接，只需要和Broker维护单个TCP连接即可。但是，其无法做到真正的负载均衡，因为实际系统中的每个生产者产生的消息量及每个Broker的消息存储量都是不一样的，如果有些生产者产生的消息远多于其他生产者的话，那么会导致不同的Broker接收到的消息总数差异巨大，同时，生产者也无法实时感知到Broker的新增和删除。
  </p>
  <p>
   2)）使用Zookeeper进行负载均衡，由于每个Broker启动时，都会完成Broker注册过程，生产者会通过该节点的变化来动态地感知到Broker服务器列表的变更，这样就可以实现动态的负载均衡机制。
  </p>
  <p>
   <strong>
    4、消费者负载均衡
   </strong>
  </p>
  <p>
   与生产者类似，Kafka中的消费者同样需要进行负载均衡来实现多个消费者合理地从对应的Broker服务器上接收消息，每个消费者分组包含若干消费者，
   <strong>
    每条消息都只会发送给分组中的一个消费者
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，不同的消费者分组消费自己特定的Topic下面的消息，互不干扰。
   </span>
  </p>
  <p>
   <strong>
    5、分区与消费者的关系
   </strong>
  </p>
  <p>
   <strong>
    消费组 (Consumer Group)：
   </strong>
  </p>
  <p>
   consumer group下有多个Consumer（消费者）。对于每个消费者组 (Consumer Group)，Kafka都会为其分配一个全局唯一的Group ID，Group内部的所有消费者共享该ID。订阅的topic下的每个分区只能分配给某个group下的一个consumer(当然该分区还可以被分配给其他group)。
  </p>
  <p>
   同时，Kafka为每个消费者分配一个Consumer ID，通常采用"Hostname:UUID"形式表示。
  </p>
  <p>
   在Kafka中，规定了
   <strong>
    每个消息分区 只能被同组的一个消费者进行消费
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，因此，需要在Zookeeper上记录消息分区与Consumer之间的关系，每个消费者一旦确定了对一个消息分区的消费权力，需要将其Consumer ID写入到Zookeeper对应消息分区的临时节点上，例如：
   </span>
  </p>
  <p>
   /consumers/[group_id]/owners/[topic]/[broker_id-partition_id]
  </p>
  <p>
   其中，[broker_id-partition_id]就是一个消息分区的标识，节点内容就是该消息分区上消费者的Consumer ID。
  </p>
  <p>
   <strong>
    6、消费者注册
   </strong>
  </p>
  <p>
   消费者服务器在初始化启动时加入消费者分组的步骤如下：
  </p>
  <p>
   注册到消费者分组。每个消费者服务器启动时，都会到Zookeeper的指定节点下创建一个属于自己的消费者节点，例如/consumers/[group_id]/ids/[consumer_id]，完成节点创建后，消费者就会将自己订阅的Topic信息写入该临时节点。
  </p>
  <p>
   <strong>
    7、记录Partition与Consumer的关系
   </strong>
  </p>
  <p>
   Consumer Group 在 ZooKeeper 上的注册节点为 /consumers/[group_id]，而 Consumer Group 中的 Consumer 在 ZooKeeper 上的注册节点为 /consumers/[group_id] 下的子节点 owners，它们共享一个 Group ID。为了 Consumer 负载均衡，同一个 Group 订阅的 Topic 下的任一 Partition 都只能分配给一个 Consumer。Partition 与 Consumer 的对应关系也需要在 ZooKeeper 中记录，路径为：
  </p>
  <p>
   /consumers/[group_id]/owners/[topic]/[broker_id-partition_id]
  </p>
  <p>
   这个路径也是一个临时节点，进行 Rebalance 时会被删除，而后依据新的对应关系重建。此外，[broker_id-partition_id] 是一个消息分区的标识，其内容就是该消息分区消费者的 Consumer ID，通常采用 hostname:UUID 形式表示。
  </p>
  <h3>
   Kafka消费者组内的消费者会不会重复消费数据
  </h3>
  <p>
   可回答：1）Kafka如何防止重消费
  </p>
  <p>
   问过的一些公司：电信云(2020.10)(2020.09)，网易严选(2020.04)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   导致重复消费的一些原因：
  </p>
  <ul list="ud9852723">
   <li fid="uaff89176">
    消费者宕机、重启或者被强行kill进程，导致消费者消费的offset没有提交。
   </li>
   <li fid="uaff89176">
    设置
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     enable.auto.commit
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     为True，如果在关闭消费者进程之前，取消了消费者的订阅，则有可能部分offset没提交，下次重启会重复消费。
    </span>
   </li>
   <li fid="uaff89176">
    消费后的数据，当offset还没有提交时，Partition就断开连接。比如，通常会遇到消费的数据，处理很耗时，导致超过了Kafka的
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     session timeout.ms
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     时间，那么就会触发reblance重平衡，此时可能存在消费者offset没提交，会导致重平衡后重复消费。
    </span>
   </li>
  </ul>
  <p>
   重复消费的解决方法：
  </p>
  <ul list="u13a41601">
   <li fid="u300828d5">
    提高消费者的处理速度。例如：对消息处理中比较耗时的步骤可通过异步的方式进行处理、利用多线程处理等。在缩短单条消息消费的同时，根据实际场景可将
    max.poll.interval.ms
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     值设置大一点，避免不必要的Rebalance。可根据实际消息速率适当调小
    </span>
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     max.poll.records
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     的值。
    </span>
   </li>
   <li fid="u300828d5">
    引入消息去重机制。例如：生成消息时，在消息中加入唯一标识符如消息id等。在消费端，可以保存最近的
    <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
     max.poll.records
    </span>
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     条消息id到Redis或MySQL表中，这样在消费消息时先通过查询去重后，再进行消息的处理。
    </span>
   </li>
   <li fid="u300828d5">
    保证消费者逻辑幂等。
   </li>
  </ul>
  <h3>
   Kafka怎么实现数据的分类处理
  </h3>
  <p>
   问过的一些公司：大华(2020.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   1、使用Kafka Connect工具
  </p>
  <p>
   Kafka Connect 是一种用于在 Apache Kafka 和其他系统之间可扩展且可靠地流式传输数据的工具。它使快速定义将大量数据移入和移出 Kafka 的连接器变得简单。Kafka Connect 可以摄取整个数据库或从所有应用程序服务器收集指标到 Kafka 主题中，使数据可用于低延迟的流处理。导出作业可以将数据从 Kafka 主题传送到二级存储和查询系统或批处理系统进行离线分析。
  </p>
  <p>
   Kafka Connect专注于Kafka之间的数据流，可以更简单地编写高质量、可靠和高性能的连接器插件。Kafka Connect还使框架能够保证使用其他框架很难做到的事情。当与Kafka和流处理框架结合时，Kafka Connect是ETL管道的一个不可或缺的组件。
  </p>
  <p>
   2、使用Kafka Streams框架
  </p>
  <p>
   Kafka Streams是一个流处理框架，它可以对Kafka中的数据进行实时处理和转换。通过Kafka Streams，可以将Kafka中的数据按照某种规则进行分类处理，例如按照某个字段的值进行分类或者按照时间窗口进行分类等。
  </p>
  <h3>
   Kafka的消息有序吗？
  </h3>
  <p>
   问过的一些公司：字节(2020.09)，京东(2020.07)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   单分区内是有序的；多分区，分区与分区间无序。
  </p>
  <h3>
   Kafka怎么回溯数据？
  </h3>
  <p>
   问过的一些公司：字节(2020.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka支持两种方式回溯。一种是基于消息偏移量回溯，一种是基于时间点的消息回溯。
  </p>
  <p>
   <strong>
    1、基于消息偏移量回溯
   </strong>
  </p>
  <p>
   在kafka的每个分区中，每条消息都有一个唯一的offset值 ，即消息偏移量，用来表示消息在partition分区中的位置。消费者每次消费了消息，都会把消费的此条消息的offset提交到broker(消息节点），用于记录消费到分区中的位置，下条消息从这个位置之后开始消费。所以基于消息偏移量回溯很简单，只需要重置offset，然后消费者会从该offset之后开始消费。
  </p>
  <p>
   <strong>
    2、基于时间点的消息回溯
   </strong>
  </p>
  <p>
   要想讲清楚kafka基于时间点的消息回溯的原理，得先从kafka存储消息的文件格式开始讲。
  </p>
  <p>
   kafka存储消息是以日志的形式存储的，每一个partition都对应一个日志，但是日志不是一个文件，是多个文件组成的。日志文件都存储在一个文件夹里面的，文件格式为： topic-0 。
  </p>
  <p>
   其中topic是kafka对应的主题名称、0是partition所在的分区号。文件夹里面存储的有日志分段文件、偏移量索引文件、时间戳索引文件。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136304785-3103e650-b44b-4a56-8f66-5ca997c442e5.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_20%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    日志分段文件
   </strong>
  </p>
  <p>
   kafka消息存储在一个.log的日志文件中，但是随着日志文件越来越大不利于消息的维护与清理，也不利于集群扩容时消息的复制。所以kafka需要对日志进行分段。
  </p>
  <p>
   日志分段文件名称的定义：
  </p>
  <p>
   日志分段名称是由日志从这日志片段开始的基准偏移量（ baseOffset ）命名的，名称固定为 20 位数字。因为baseOffset是long型的，long型最大值19位，所以文件名20位即可满足所以的偏移量要求
  </p>
  <p>
   例如：00000000000000000054.log
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136304755-72337949-371c-488b-b54f-c3a44f11e72c.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_9%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   当kafka判断一个日志文件很大了时，就会重新开辟一个.log的日志文件进行消息写入，对老的.log文件设置为只读，不能写入。kafka判断消息需要分片的策略有4中，满足其一即可。
  </p>
  <ul list="u2d15fb10">
   <li fid="uc572ab76">
    当前日志分段文件的大小超过了broker端参数 log.segmentbytes配置的值。log.segmentbytes参数的默认值为 1073741824，即lGB
   </li>
   <li fid="uc572ab76">
    当前日志分段中消息的最大时间戳与当前系统的时间戳的差值大于log.roll.hours。默认情况下，配置了 log.roll.hours参数，其值为168天。
   </li>
   <li fid="uc572ab76">
    偏移量索引文件或时间戳索引文件的大小达到broker端参数log.index.size.max.bytes配置的值。log.index.size.max.bytes 的默认值为10485760，即10MB
   </li>
   <li fid="uc572ab76">
    追加的消息的偏移量与当前日志分段的偏移量之间的差值大于 Integer.MAX_VALUE
   </li>
  </ul>
  <p>
   <strong>
    偏移量索引文件
   </strong>
  </p>
  <p>
   文件存储的是消息对应在物理磁盘的地址。是以key、value形式存在的。索引文件采用稀疏索引（ sparse index ）的方式构造消息的索引。它并不保证每个消息在索引文件中都有对应的索引 每当写入一定量（由 broker 端参数 log.index.interval.bytes 指定，默认值为 4096 ，即 4KB ）的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项。
  </p>
  <p>
   文件格式：00000000000054.index 。其中54依然是索引文件中存在的第一个消息的offset（偏移量）
  </p>
  <p>
   索引格式如下：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136305745-625333f9-d13d-47ac-876f-4a6ddc852113.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_13%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   每个索引占用 8 个字节，分为两个部分。
  </p>
  <ul list="u7b0e134d">
   <li fid="uf10482f1">
    relativeOffset ：日志索引的相对偏移量。相对于文件名的基准偏移量来说的，比如索引的第一个消息，那么relativeOffset是0
   </li>
   <li fid="uf10482f1">
    position ：消息存储在磁盘的物理位置
   </li>
  </ul>
  <p>
   那现在应该知道，通过消息的偏移量怎么找到对应的日志分段文件，然后之后的所有消息都知道了。
  </p>
  <p>
   其中0000000000000000034.index偏移量索引文件内容如下：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136306136-f45f241e-31a9-4e62-98ce-d56de91ae68d.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_13%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    要找到偏移量为56的消息怎么找呢？
   </strong>
  </p>
  <ol list="u68f75d7c">
   <li fid="u43b6cecf">
    首先定位到0000000000000000034.log 的日志文件。那么它是怎么找到这个日志文件的呢，kafka中用跳跃表存储了日志文件baseOffset对应的日志文件名。通过跳跃表很容易查到不大于56的最大的baseOffset，然后定位到日志文件。可以计算出56相对于34的相对偏移量为22。
   </li>
   <li fid="u43b6cecf">
    既然定位到日志文件是0000000000000000034.log。那么索引文件也肯定是0000000000000000034.index。在步骤1中，已经算出了相对偏移量为22，即对应在index索引文件是relativeOffset。那么我们需要在0000000000000000034.index中找到不大于22的最大的relativeOffset，是按照第一索引消息顺序找到的，即定位到22的索引。拿到对应的position
   </li>
   <li fid="u43b6cecf">
    拿到了对应的物理磁盘position，既能直接找到消息，然后顺序往后查找，既能拿到所有的消息进行消息回溯
   </li>
  </ol>
  <p>
   <strong>
    时间索引文件
   </strong>
  </p>
  <p>
   文件格式：文件格式：0001586662165087.timeindex 。其中1586662165087对应的是这个时间点生成的时间索引文件
  </p>
  <p>
   存储的时间索引内容格式：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136307251-a0a2bf00-6fab-43ab-817f-2c34fc92f479.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_13%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   每个索项占用 12 个字节，分为两个部分。
  </p>
  <ul list="uf1aaf643">
   <li fid="ud6c94257">
    timestamp ：时间戳。
   </li>
   <li fid="ud6c94257">
    relativeOffset ：时间戳所对应的消息的相对偏移量。
   </li>
  </ul>
  <p>
   那么是怎么通过时间戳找到定义的消息的呢？
  </p>
  <p>
   先通过时间索引文件找到时间对应的offset偏移量，在通过偏移量索引文件找到消息位置。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136308513-10294171-1f40-4819-9230-e0f8d2350f72.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_25%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   例如要回溯2020-4-12 09:00:00的之后消息：
  </p>
  <ol list="uf1c62f47">
   <li fid="uf85b04d7">
    首先换算成时间戳为1586653200000
   </li>
   <li fid="uf85b04d7">
    根据时间戳1586653200000在时间索引中找到不大于1586653200000的最大的偏移量
   </li>
   <li fid="uf85b04d7">
    找了偏移量，按照偏移量索引讲解的步骤，逐一去查找，即可找到对应的消息position
   </li>
   <li fid="uf85b04d7">
    通过position定位了消息，获取消息的生成时间，比1586653200000进行比对，然后按顺序逐渐和后面的消息一一进行时间戳比对，如果前一个消息的时间戳&lt;1586653200000 &amp; 后一个消息的时间戳 &gt; 1586653200000 。 那么这个位置是就是消息回溯点，拿到消息的offset，对消费者消费记录的offset进行重置，那么整个回溯就完成了
   </li>
  </ol>
  <h3>
   Kafka分区多副本机制？
  </h3>
  <p>
   可回答：1）Kafka副本机制；2）Kafka副本同步策略；3）Kafka副本间怎么同步的；4）Kafka如何主副同步
  </p>
  <p>
   问过的一些公司：soul(2021.09)，京东寻猎计划(2020.09)，字节(2020.08)，美团(2020.08)，昆仑万维提前批(2020.08)，恒生电子实习(2020.06)，陌陌实习(2020.06)，阿里(2018.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka为分区引入了多副本（Replica）机制，通过增加副本数量可以提升容灾能力。同一分区的不同副本中保存的是相同消息（在同一时刻，副本之前并非完全一样），副本之间是“一主多从”的关系，其中leader副本负责处理读写请求，follower副本只负责与leader副本的消息同步。副本处于不同的broker中，当leader副本出现故障时，从follower副本中重新选举新的leader副本对外提供服务。Kafka通过多副本机制实现了故障的自动转义，当Kafka集群中某个broker失效时扔然能够保证服务可用。
  </p>
  <p>
   如下图，Kafka集群中有4个broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个leader副本和2个follower副本。生产者和消费者只与leader副本进行交互，而follower副本只负责消息的同步，很多时候follower副本中的消息相对leader副本而言会有一定的滞后。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136308784-1557aa73-0cda-43ee-b25b-bf01a3bb6fae.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_24%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   Kafka消费端也具备一定的容灾能力。Consumer使用拉（Pull）模式从服务端拉去消息，并且保存消费的具体位置，当消费者宕机后恢复上线时可以根据之前保存的消费位置重新拉取需要的消息进行消费，这样就不会造成消息丢失。
  </p>
  <p>
   分区中的所有副本统称为 AR ( Assigned Replicas ）。所有与 leader 副本保持一定程度同步的副本（包括 leader 副本在内组成 ISR On-Sync Replicas ) ，ISR集合是 AR 集合中的一个子集 。 消息会先发送到 leadr 副本，然后follower副本才能从 leader 副本中拉取消息进行同步，同步期间内 follower 副本相对于 leader 副本而言会有一定程度的滞后。前面所说的“一定程度的同步”是指可忍受的滞后范围，这个范围可以通过参数进行配置。与 leader 副本同步滞后过多的副本（不包括 leader 副本）组成 OSR ( Out-of-Sync Replicas ），由此可见，AR=ISR+OSR。在正常情况下， 所有的 follower 副本都应该与 leader 副本保持一定程度 的同步，即 AR=ISR，OSR 集合为空。
  </p>
  <p>
   leader 副本负责维护和跟踪 ISR 集合中所有 follower 副本的滞后状态， 当 follower 副本落后太多或失效时， leader 副本会把它从 ISR 集合中剔除 。 如果 OSR 集合中有 follower 副本 “追上’’了 leader 副本，那么 leader 副本会把它从 OSR 集合转移至 ISR 集合 。 默认情况下， 当 leader 副本发生故障时，只有在 ISR 集合中的副本才有资格被选举为新的 leader， 而在 OSR 集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变）。
  </p>
  <p>
   ISR 与 HW 和 LEO 也有紧密的关系 。 HW 是 High Watermark 的缩写，俗称高水位，它标识了一个特定 的消息偏移量（ offset），消费者只能拉取到这个offset之前的消息 。如图所示，它代表一个日志文件，这个日志文件中有 9 条消息，第一条消息的 offset( LogStartOffset ）为 0，最后一条消息的 offset 为 8, offset 为 9 的消息用虚线框表示，代表下一条待写入的消息。日志文件的 HW 为 6，表示消费者只能拉取到 offset 在 0 至 5 之间的消息，而offset为 6 的消息对消费者而言是不可见的 。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136309613-893cf903-5427-4e06-8daf-68743732b66a.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_19%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   LEO是Log End Offset 的缩写，它标识当前日志文件中下一条待写入消息 的 offset，图中offset 为 9 的位置即为当前日志文件的LEO, LEO的大小相当于当前日志分区中最后一条消息的offset 值加 1。分区ISR 集合中的每个副本都会维护自身的LEO ，而 ISR集合中最小的 LEO即为分区的LEO ，对消费者而言只能消费HW之前的消息。
  </p>
  <p>
   为了让读者更好地理解 ISR 集合， 以及 HW 和 LEO 之间的关系， 下面通过一个简单的示例来进行相关的说明 。如下图所示，假设某个分区的 ISR 集合中有 3 个副本，即一个 leader副本和 2 个 follower 副本，此时分区的 LEO 和 HW 都为 3 。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136310146-a0a55ef8-b375-4aea-8bc8-d1f2817f0d7e.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_15%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   消息3和消息4从生产者发出之后会被先存入leader副本，如下图所示
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136312148-4f8f5f4b-b123-4d87-bdf1-7ca17c54a6c4.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_13%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   在消息写入 leader 副本之后， follower 副本会发送拉取请求来拉取消息 3 和消息 4 以进行消息同步。在同步过程中，不同的 follow副本的同步效率也不尽相同。如下图所示， 在某一时刻follower1完全跟上了 leader 副本而 follower2 只同步了消息 3 ，如此 leader 副本的 LEO 为 5,follower1 的 LEO 为 5 , follower2 的 LEO 为 4 ， 那么当前分区的 HW 取最小值 4 ，此时消费者可以消 费到 offset 为 0 至 3 之间的消息。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136311396-2266de7c-b745-4af3-95ed-529973e79ef3.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_13%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   写入消息情形如下图所示，所有的副本都成功写入了消息 3 和消息 4，整个分区的HW 和 LEO 都变为 5，因此消费者可以消费到 offset 为 4 的消息了 。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136312163-c175bdad-088f-4ac9-9750-4826a2f8f51e.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_13%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   由此可见， Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的 follower 副本都复制完，这条消息才会被确认为已成功提交，这种复制方式极大地影响了性能。而在异步复制方式下， follower 副本异步地从 leader 副本中 复制数据，数据只要被 leader 副本写入就被认为已经成功提交。在这种情况下，如果 follower 副本都还没有复制完而落后于 leader 副本，突然 leader 副本着机，则会造成数据丢失。 Kafka 使用的这种 ISR 的方式则有效地权衡了数据可靠性和性能之间的关系。
  </p>
  <h3>
   Kafka中都有哪里会有选举过程，使用什么工具支持选举
  </h3>
  <p>
   问过的一些公司：字节(2020.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Controller选举、分区Leader选举、消费组Leader的选举。
  </p>
  <p>
   使用Zookeeper支持选举。
  </p>
  <h3>
   Kafka读取消息是推还是拉的模式？有什么好处？
  </h3>
  <p>
   可回答：1）Kafka为什么使用拉取消息的机制？
  </p>
  <p>
   问过的一些公司：触宝(2020.09)，京东(2020.08)，腾讯互娱(2020.03)，快手(2019.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   producer使用push（推）模式将消息发布到broker。consumer采用pull（拉）模式从broker中读取数据。
  </p>
  <p>
   push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。
  </p>
  <p>
   优点：
  </p>
  <ul list="u335ced55">
   <li fid="ub59cea9c">
    生产者主动推送给消费者，及时性很高
   </li>
  </ul>
  <p>
   缺点：
  </p>
  <ul list="u604d88f9">
   <li fid="ua7313d26">
    当消费者消费能力远低于生产者生产能力，那么一旦生产者推送大量消息到消费者时，就会导致消费者消息堆积，处理缓慢，甚至服务崩溃
   </li>
   <li fid="ua7313d26">
    生产者需要维护和每个消费者之间的会话
   </li>
  </ul>
  <p>
   pull（拉）模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。
  </p>
  <p>
   优点：
  </p>
  <ul list="u05fbd4cb">
   <li fid="u5f5d3685">
    消费者可以依据自己的消费能力进行消费
   </li>
   <li fid="u5f5d3685">
    生产者不需要维护和消费者之间的会话
   </li>
  </ul>
  <p>
   缺点：
  </p>
  <ul list="uc4e3067b">
   <li fid="uc31260c8">
    拉取消息的间隔不太好设置。间隔太短，对服务器请求压力过大。间隔时间过长，那么必然会造成一部分数据的延迟
   </li>
   <li fid="uc31260c8">
    实时性相对较低
   </li>
  </ul>
  <h3>
   Kafka解决两个客户端消费数据的问题
  </h3>
  <p>
   可回答：1）零拷贝经过堆了吗？既然没经过堆，Java是怎么实现零拷贝的？
  </p>
  <p>
   问过的一些公司：大疆(2022.09)，一点资讯(2020.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   在Kafka中，同一个消费者组内的消费者可以共同消费一个或多个分区的数据。这种方式被称为分区分配。Kafka通过负载均衡来实现分区分配，确保一个分区只由一个消费者组内的消费者来消费。
  </p>
  <p>
   如果在同一个消费者组内有多个消费者，每个消费者可以独立地读取分区数据，并且每个分区只会被分配给一个消费者。这样就能够实现分布式消费数据的效果，同时也保证了消费者读取到的数据是有序的。如果一个消费者挂掉了，那么它所消费的分区将会重新分配给其他消费者，确保数据的高可靠性和可用性。
  </p>
  <h3>
   Kafka零拷贝技术
  </h3>
  <p>
   问过的一些公司：汇量科技(2022.09)，字节(2022.08)，腾讯PCG实习(2021.04)，快手提前批(2020.08)，Bigo(2020.08)，腾讯云(2020.03)，网易严选(2019.08)，美团新到店(2018.09)，阿里(2018.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、传统数据文件拷贝过程
   </strong>
  </p>
  <p>
   传统的数据文件拷贝过程如下图所示，大概可以分成四个过程：
  </p>
  <ul list="u35462e38">
   <li fid="u73c409c6">
    操作系统将数据从磁盘中加载到内核空间的Read Buffer（页缓存区）中。
   </li>
   <li fid="u73c409c6">
    应用程序将Read Buffer中的数据拷贝到应用空间的应用缓冲区中。
   </li>
   <li fid="u73c409c6">
    应用程序将应用缓冲区的数据拷贝到内核的Socket Buffer中。
   </li>
   <li fid="u73c409c6">
    操作系统将数据从Socket Buffer中发送到网卡，通过网卡发送给数据接收方。
   </li>
  </ul>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136312523-05310bf3-e899-4d7a-a3f1-75355ab6cfe1.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_16%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   通过上图可以发现，传统的数据文件传输需要多次在用户态和核心态之间进行切换，并且需要把数据在用户太和和核心态之间拷贝多次，最终才打到网卡，传输给接收方。
  </p>
  <p>
   <strong>
    2、Kafka零拷贝过程
   </strong>
  </p>
  <p>
   所谓的零拷贝是指将数据在内核空间直接从磁盘文件复制到网卡中，而不需要经由用户态的应用程序之手。这样既可以提高数据读取的性能，也能减少核心态和用户态之间的上下文切换，提高数据传输效率。
  </p>
  <p>
   先来看一下DMA（Direct Memory Access）技术。DMA又称之为直接内存访问，是零拷贝技术的基石。DMA传输将数据从一个地址空间复制到另外一个地址空间。当CPU初始化这个传输动作，传输动作本身是由DMA控制器来实行和完成。因此通过DMA，硬件则可以绕过CPU，自己去直接访问系统主内存。很多硬件都支持DMA，其中就包括网卡、声卡、磁盘驱动控制器等
  </p>
  <p>
   有了DMA技术的支持之后，网卡就可以直接区访问内核空间的内存，这样就可以实现内核空间和应用空间之间的零拷贝了，极大地提升传输性能。
  </p>
  <p>
   下图展示了Kafka零拷贝的数据传输过程。数据传输的的过程就简化成了：
  </p>
  <ul list="u24c281f6">
   <li fid="u8b119576">
    操作系统将数据从磁盘中加载到内核空间的Read Buffer（页缓存区）中。
   </li>
   <li fid="u8b119576">
    操作系统之间将数据从内核空间的Read Buffer（页缓存区）传输到网卡中，并通过网卡将数据发送给接收方。
   </li>
   <li fid="u8b119576">
    操作系统将数据的描述符拷贝到Socket Buffer中。Socket 缓存中仅仅会拷贝一个描述符过去，不会拷贝数据到 Socket 缓存。
   </li>
  </ul>
  <p>
   Kafka数据零拷贝的过程如下图所示：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136314283-f98ec3c5-bed1-4132-b864-47455743c03c.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_16%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   通过零拷贝技术，就不需要把内核空间页缓存里的数据拷贝到应用层缓存，再从应用层缓存拷贝到 Socket 缓存了，两次拷贝都省略了，所以叫做零拷贝。这个过程大大的提升了数据消费时读取文件数据的性能。Kafka 从磁盘读数据的时候，会先看看内核空间的页缓存中是否有，如果有的话，直接通过网关发送出去。
  </p>
  <p>
   <strong>
    3、Java中零拷贝的实现
   </strong>
  </p>
  <p>
   Java中的零拷贝是依靠
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    java.nio.channels.FileChannel
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    中的
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    transferTo(long position, long count, WritableByteChannel target)
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    方法来实现的。
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    transferTo
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    方法的底层实现是基于操作系统的sendfile这个system call来实现的，无需将数据拷贝到用户态，sendfile负责把数据从某个fd（file descriptor）传输到另一个fd。这样就完成了零拷贝的过程。
   </span>
  </p>
  <p>
   零拷贝的示例代码如下所示：
  </p>
  <pre><code class="language-plain" lang="plain">import java.io.File;
import java.io.RandomAccessFile;
import java.net.InetSocketAddress;
import java.nio.channels.FileChannel;
import java.nio.channels.SocketChannel;

public class ZeroCopy {
    public static void main(String[] args) throws Exception {
        File file  = new File("xxxxxx.log");

        RandomAccessFile raf = new RandomAccessFile(file, "rw");

        FileChannel channel = raf.getChannel();

        //Opens a socket channel and connects it to a remote address.
        SocketChannel socketChannel = SocketChannel.open(
                new InetSocketAddress("192.168.2.222", 9091)
        );

        //Transfers bytes from this channel's file to the given writable byte channel.
        channel.transferTo(0,channel.size(), socketChannel);
    }
}</code></pre>
  <p>
   综上，Kafka用到的零拷贝技术，主要是减少了核心态和用户态之间的两次数据拷贝过程，使得数据可以不用经过用户态直接通过网卡发送到接收方，同时通过DMA技术，可以使CPU得到解放，这样实现了数据的高性能传输。
  </p>
  <p>
   这里再说一点：
   <strong>
    用户态和内核态的区别
   </strong>
  </p>
  <p>
   用户态和内核态的区别是，内核态运行操作系统程序，操作硬件，用户态运行用户程序；当程序运行在3级特权级上时，可以称之为运行在用户态，当程序运行在0级特权级上时，称之为运行在内核态。
  </p>
  <h3>
   Kafka在哪个阶段用的零拷贝？
  </h3>
  <p>
   问过的一些公司：腾讯云(2020.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka在生产者和消费者两个阶段都使用了零拷贝技术。
  </p>
  <p>
   在生产者端，Kafka的生产者会将消息缓存在一个待发送的缓冲区中，这个缓冲区通常是ByteBuffer。当生产者发送消息时，它会将ByteBuffer中的数据直接发送到网络套接字中，避免了将数据从一个缓冲区拷贝到另一个缓冲区的开销。
  </p>
  <p>
   在消费者端，Kafka的消费者从Broker拉取消息时，消息首先被读入到操作系统的页缓存中。然后，Kafka的消费者会使用mmap将页缓存中的数据映射到自己的内存空间中。这个过程中避免了数据从内核空间拷贝到用户空间的开销，也就是零拷贝。
  </p>
  <h3>
   Kafka的消费者和消费者组有什么区别？为什么需要消费者组？
  </h3>
  <p>
   可回答：1）说下Kafka的消费者和消费者组，以及它们的作用是什么？
  </p>
  <p>
   问过的一些公司：字节提前批(2020.08)，京东(2020.08)，Bigo(2020.08)，拼多多学霸批(2019.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、什么是消费者
   </strong>
  </p>
  <p>
   顾名思义，消费者就是从kafka集群消费数据的客户端，如下图，展示了一个消费者从一个topic中消费数据的模型。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136314422-25c00da2-12e3-4420-9488-bb049cf5267a.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_14%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    2、为什么需要消费者组
   </strong>
  </p>
  <p>
   如果这个时候 kafka 上游生产的数据很快，超过了这个
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    消费者1
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    的消费速度，那么就会导致数据堆积，产生一些大家都知道的蛋疼事情了，那么我们只能加强
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    消费者
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    的消费能力，所以也就有了
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    消费者组
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
  </p>
  <p>
   <strong>
    3、什么是消费者组
   </strong>
  </p>
  <p>
   所谓
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    消费者组
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，其实就是一组
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    消费者
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    的集合，当我们看到下面这张图是不是就特别舒服了，我们采用了一个
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    消费组
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    来消费这个
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    topic
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，众人拾柴火焰高，其消费能力那是按倍数递增的，所以这里我们一般来说都是采用
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    消费者组
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    来消费数据，而不会是
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    单消费者
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    来消费数据的。
   </span>
  </p>
  <p>
   注意：
  </p>
  <p>
   一个topic可以被多个消费者组消费，但是每个消费者组消费的数据是互不干扰的，也就是说，每个消费组消费的都是完整的数据 。
  </p>
  <p>
   一个分区只能被同一个消费组内的一个消费者消费，而不能拆给多个消费者消费，也就是说如果你某个消费者组内的消费者数比该 Topic 的分区数还多，那么多余的消费者是不起作用的。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136314590-e3e83c09-4730-491e-86d4-f7286ee58fa3.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_14%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    扩展一下：
   </strong>
  </p>
  <p>
   <strong>
    1）是不是一个消费组的消费者越多其消费能力就越强呢？
   </strong>
  </p>
  <p>
   从下图我们就可以很好的可以回答这个问题了，我们可以看到消费者4是完全没有消费任何的数据的，所以如果你想要加强消费者组的能力，除了添加消费者，分区的数量也是需要跟着增加的，只有这样他们的并行度才能上的去，消费能力才会强。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136316633-62286aa1-d142-4c0a-b57b-0515cd0bfc8e.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_14%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    2）为了提高消费组的消费能力，我是不是可以随便添加分区和消费者呢？
   </strong>
  </p>
  <p>
   答案当然是否定的。。。我们看到下图，一般来说我们建议消费者数量和分区数量是一致的，当我们的消费能力不够时，就必须通过调整分区的数量来提高并行度，但是，我们应该尽量来避免这种情况发生。
  </p>
  <p>
   比如：现在我们需要在下图的基础上增加一个分区4，那么这个分区4该由谁来消费呢？这个时候kafka会进行分区再均衡，来为这个分区分配消费者，分区再均衡期间该Topic是不可用的，并且作为一个被消费者，分区数的改动将影响到每一个消费者组 ，所以在创建 topic 的时候，我们就应该考虑好分区数，来尽量避免这种情况发生。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136316674-a0996297-d7fc-4a0a-91f3-6051d2a12ec3.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_14%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <h3>
   Kafka的单播和多播
  </h3>
  <p>
   问过的一些公司：猿辅导(2020.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、单播
   </strong>
  </p>
  <p>
   一条消息只能被某一个消费者消费的模式称为单播。要实现消息单播，只要让这些消费者属于同一个消费者组即可。当生产者发送一条消息时，两个消费者中只有一个能收到消息。
  </p>
  <p>
   <strong>
    2、多播
   </strong>
  </p>
  <p>
   一条消息能够被多个消费者消费的模式称为多播。之所以不称之为广播，是因为一条消息只能被Kafka同一个分组下某一个消费者消费，而不是所有消费者都能消费，所以从严格意义上来讲并不能算是广播模式，当然如果希望实现广播模式只要保证每个消费者均属于不同的消费者组。针对Kafka同一条消息只能被同一个消费者组下的某一个消费者消费的特性，要实现多播只要保证这些消费者属于不同的消费者组即可。然后通过生产者发送几条消息，可以看到不同消费者组的消费者同时能消费到消息，然而同一个消费者组下的消费者却只能有一个消费者能消费到消息。
  </p>
  <h3>
   Kafka连接Spark Streaming的几种方式
  </h3>
  <p>
   问过的一些公司：作业帮提前批(2020.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Spark Streaming获取kafka数据的两种方式：
   <strong>
    Receiver
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    与
   </span>
   <strong>
    Direct
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    的方式，可以从代码中简单理解成
   </span>
   <strong>
    Receiver方式是通过zookeeper来连接kafka队列
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，
   </span>
   <strong>
    Direct方式是直接连接到kafka的节点上获取数据
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
  </p>
  <p>
   <strong>
    1、基于Receiver的方式
   </strong>
  </p>
  <p>
   这种方式使用Receiver来获取数据。Receiver是使用Kafka的高层次Consumer API来实现的。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Streaming启动的job会去处理那些数据。
  </p>
  <p>
   然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的预写日志机制（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。
  </p>
  <p>
   注意：
  </p>
  <ul list="u49575f4c">
   <li fid="u24c26506">
    Kafka中的topic的partition，与Spark中的RDD的partition是没有关系的。所以，在KafkaUtils.createStream()中，提高partition的数量，只会增加一个Receiver中，读取partition的线程的数量。不会增加Spark处理数据的并行度。
   </li>
   <li fid="u24c26506">
    可以创建多个Kafka输入DStream，使用不同的consumer group和topic，来通过多个receiver并行接收数据。
   </li>
   <li fid="u24c26506">
    如果基于容错的文件系统，比如HDFS，启用了预写日志机制，接收到的数据都会被复制一份到预写日志中。因此，在KafkaUtils.createStream()中，设置的持久化级别是StorageLevel.MEMORY_AND_DISK_SER。
   </li>
  </ul>
  <p>
   <strong>
    2、基于Direct的方式
   </strong>
  </p>
  <p>
   这种新的不基于Receiver的直接方式，是在Spark 1.3中引入的，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。
  </p>
  <p>
   这种方式有如下优点：
  </p>
  <p>
   1）简化并行读取
  </p>
  <p>
   如果要读取多个partition，不需要创建多个输入DStream然后对它们进行union操作。Spark会创建跟Kafka partition一样多的RDD partition，并且会并行从Kafka中读取数据。所以在Kafka partition和RDD partition之间，有一个一对一的映射关系。
  </p>
  <p>
   2）高性能
  </p>
  <p>
   如果要保证零数据丢失，在基于receiver的方式中，需要开启WAL机制。这种方式其实效率低下，因为数据实际上被复制了两份，Kafka自己本身就有高可靠的机制，会对数据复制一份，而这里又会复制一份到WAL中。而基于direct的方式，不依赖Receiver，不需要开启WAL机制，只要Kafka中作了数据的复制，那么就可以通过Kafka的副本进行恢复。
  </p>
  <p>
   3）一次且仅一次的事务机制
  </p>
  <p>
   基于receiver的方式，是使用Kafka的高阶API来在ZooKeeper中保存消费过的offset的。这是消费Kafka数据的传统方式。这种方式配合着WAL机制可以保证数据零丢失的高可靠性，但是却无法保证数据被处理一次且仅一次，可能会处理两次。因为Spark和ZooKeeper之间可能是不同步的。
  </p>
  <p>
   4）降低资源
  </p>
  <p>
   Direct不需要Receivers，其申请的Executors全部参与到计算任务中；而Receiver-based则需要专门的Receivers来读取Kafka数据且不参与计算。因此相同的资源申请，Direct 能够支持更大的业务。
  </p>
  <p>
   5）降低内存
  </p>
  <p>
   Receiver-based的Receiver与其他Exectuor是异步的，并持续不断接收数据，对于小业务量的场景还好，如果遇到大业务量时，需要提高Receiver的内存，但是参与计算的Executor并无需那么多的内存。而Direct 因为没有Receiver，而是在计算时读取数据，然后直接计算，所以对内存的要求很低。实际应用中我们可以把原先的10G降至现在的2-4G左右。
  </p>
  <p>
   6）鲁棒性更好
  </p>
  <p>
   Receiver-based方法需要Receivers来异步持续不断的读取数据，因此遇到网络、存储负载等因素，导致实时任务出现堆积，但Receivers却还在持续读取数据，此种情况很容易导致计算崩溃。Direct 则没有这种顾虑，其Driver在触发batch 计算任务时，才会读取数据并计算。队列出现堆积并不会引起程序的失败。
  </p>
  <p>
   基于direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据是消费一次且仅消费一次。
  </p>
  <h3>
   Kafka生成者客户端有几个线程
  </h3>
  <p>
   可回答：1）Kafka生产者客户端工作原理
  </p>
  <p>
   问过的一些公司：昆仑万维提前批(2020.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   消息在真正发往 Kafka 之前，有可能需要经历拦截器、序列化器和分区器等一系列的作用。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136316901-7e3ed863-b317-4243-a167-f3de84d9fe93.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_21%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   整个生产者客户端由
   <strong>
    两个线程
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    协调运行，这两个线程分别为
   </span>
   <strong>
    主线程
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    和
   </span>
   <strong>
    发送线程
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息收集器（RecordAccumulator，也称为消息累加器）中。发送线程负责从消息收集器中获取消息并将其发送到 Kafka 中。
   </span>
  </p>
  <p>
   主要用来缓存消息以便发送线程可以批量发送，进而减少网络传输的资源消耗以提升性能。消息收集器缓存的大小可以通过生产者客户端参数 buffer.memory 配置，默认值为 33554432B，即32MB。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候 KafkaProducer 的 send() 方法调用要么被阻塞，要么抛出异常，这个取决于参数 max.block.ms 的配置，此参数的默认值为60000，即60秒。
  </p>
  <p>
   主线程中发送过来的消息都会被追加到消息收集器的某个双端队列（Deque）中，在其的内部为每个分区都维护了一个双端队列，队列中的内容就是ProducerBatch，即 Deque。消息写入缓存时，追加到双端队列的尾部；Sender 读取消息时，从双端队列的头部读取。注意 ProducerBatch 不是 ProducerRecord，ProducerBatch 中可以包含一至多个 ProducerRecord。
  </p>
  <p>
   通俗地说，ProducerRecord 是生产者中创建的消息，而 ProducerBatch 是指一个消息批次，ProducerRecord 会被包含在 ProducerBatch 中，这样可以使字节的使用更加紧凑。与此同时，将较小的 ProducerRecord 拼凑成一个较大的 ProducerBatch，也可以减少网络请求的次数以提升整体的吞吐量。
  </p>
  <p>
   如果生产者客户端需要向很多分区发送消息，则可以将 buffer.memory 参数适当调大以增加整体的吞吐量。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136318099-39b2bd9d-d7ef-4868-9077-9bfb431c6f23.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_22%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   消息在网络上都是以字节的形式传输的，在发送之前需要创建一块内存区域来保存对应的消息。在 Kafka 生产者客户端中，通过 java.io.ByteBuffer实现消息内存的创建和释放。不过频繁的创建和释放是比较耗费资源的，在 消息收集器的内部还有一个 BufferPool，它主要用来实现 ByteBuffer 的复用，以实现缓存的高效利用。不过 BufferPool 只针对特定大小的 ByteBuffer 进行管理，而其他大小的 ByteBuffer 不会缓存进 BufferPool 中，这个特定的大小由 batch.size 参数来指定，默认值为16384B，即16KB。我们可以适当地调大 batch.size 参数以便多缓存一些消息。
  </p>
  <p>
   ProducerBatch 的大小和 batch.size 参数也有着密切的关系。当一条消息流入消息收集器时，会先寻找与消息分区所对应的双端队列（如果没有则新建），再从这个双端队列的尾部获取一个 ProducerBatch（如果没有则新建），查看 ProducerBatch 中是否还可以写入这个 ProducerRecord，如果可以则写入，如果不可以则需要创建一个新的 ProducerBatch。
  </p>
  <p>
   在新建 ProducerBatch 时评估这条消息的大小是否超过 batch.size 参数的大小，如果不超过，那么就以 batch.size 参数的大小来创建 ProducerBatch，这样在使用完这段内存区域之后，可以通过 BufferPool 的管理来进行复用；如果超过，那么就以评估的大小来创建 ProducerBatch，这段内存区域不会被复用。
  </p>
  <p>
   Sender 从 RecordAccumulator 中获取缓存的消息之后，会进一步将原本&lt;分区, Deque&lt; ProducerBatch&gt;&gt; 的保存形式转变成 &lt;Node, List&lt; ProducerBatch&gt; 的形式，其中 Node 表示 Kafka 集群的 broker 节点。
  </p>
  <p>
   对于网络连接来说，生产者客户端是与具体的 broker 节点建立的连接，也就是向具体的 broker 节点发送消息，而并不关心消息属于哪一个分区；而对于 KafkaProducer 的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以在这里需要做一个应用逻辑层面到网络I/O层面的转换。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136318512-060f0082-8173-41c7-8a4e-ee7c7fe7f7cb.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_23%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   在转换成 &lt;Node, List&gt; 的形式之后，Sender 还会进一步封装成 &lt;Node, Request&gt; 的形式，这样就可以将 Request 请求发往各个 Node 了，这里的 Request 是指 Kafka 的各种协议请求，对于消息发送而言就是指具体的 ProduceReques
  </p>
  <p>
   请求在从 Sender 线程发往 Kafka 之前还会保存到 InFlightRequests 中，保存对象的具体形式为 Map&lt;NodeId, Deque&gt;，它的主要作用是缓存了已经发出去但还没有收到响应的请求（NodeId 是一个 String 类型，表示节点的 id 编号）。
  </p>
  <p>
   与此同时，InFlightRequests 还提供了许多管理类的方法，并且通过配置参数还可以限制每个连接（也就是客户端与 Node 之间的连接）最多缓存的请求数。这个配置参数为 max.in.flight.requests.per.connection，默认值为5，即每个连接最多只能缓存5个未响应的请求，超过该数值之后就不能再向这个连接发送更多的请求了，除非有缓存的请求收到了响应（Response）。通过比较 Deque 的 size 与这个参数的大小来判断对应的 Node 中是否已经堆积了很多未响应的消息，如果真是如此，那么说明这个 Node 节点负载较大或网络连接有问题，再继续向其发送请求会增大请求超时的可能。
  </p>
  <h3>
   Kafka中Producer，Broker，Cousumer的关系
  </h3>
  <p>
   问过的一些公司：字节提前批(2020.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   一条消息从生产到消费，可以划分三个阶段：
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136319838-77433809-e7b0-48d4-b38f-04455a5a1c72.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_29%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   Producer：生产者。创建消息，并通过网络发送给 Broker。
  </p>
  <p>
   Broker：缓存代理。存储收到的消息，如果是集群，还要同步副本给其他 Broker。
  </p>
  <p>
   Consumer：消费者。它向 Broker 请求消息，Broker 通过网络传输给 Consumer。
  </p>
  <h3>
   Kafka为什么有分区
  </h3>
  <p>
   问过的一些公司：陌陌实习(2020.06)，阿里菜鸟(2019.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   主要是为了减轻单台服务器存储数据的压力。
  </p>
  <p>
   当数据量很大的时候，达到PB级时，单台服务器的硬盘存储容量肯能存不下，就算存下了，很大可能快要存满了，不能再继续存储数据了，所以这种情况是需要将这种大数据量进行切片分割，分成几部分然后存储在几台不同的服务器中。
  </p>
  <h3>
   Kafka中的partition数据的查找
  </h3>
  <p>
   问过的一些公司：陌陌实习(2020.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   每个partition下面有多个segment，先定位segment再找具体offset的数据
  </p>
  <h3>
   Kafka消费者怎么从Kafka取数据的
  </h3>
  <p>
   问过的一些公司：快手(2020.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Consumer 通过拉（pull）的模式从 broker 中读取数据。
  </p>
  <p>
   pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。
  </p>
  <p>
   pull 模式的不足之处是，如果Kafka 没有数据，消费者可能会陷入循环中，一直返回空数据。针对此，Kafka 的消费者在消息数据时会传入一个时长参数，如果当前没有数据可供消费，Consumer 会等待一段时间之后再返回，这段时长即为 timeout。
  </p>
  <h3>
   如果有一条offset对应的数据，消费完成之后，手动提交失败，如何处理？
  </h3>
  <p>
   问过的一些公司：安恒信息(2020.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   回滚，利用Kafka的事务解决
  </p>
  <h3>
   介绍下Kafka的事务
  </h3>
  <p>
   问过的一些公司：安恒信息(2020.03)，360社招(2020.01)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、什么是Kafka的事务机制
   </strong>
  </p>
  <p>
   Kafka的事务机制，是Kafka实现端到端有且仅有一次语义（exactly once)的基础；
  </p>
  <p>
   Kafka的事务机制，涉及到 transactional producer 和 transactional consumer, 两者配合使用，才能实现端到端有且仅有一次的语义（end-to-end EOS)；
  </p>
  <p>
   Kafka的 producer 和 consumer 是解耦的，也可以使用非 transactional consumer 来消费 transactional producer 生产的消息，但此时就丢失了事务 ACID 的支持；
  </p>
  <p>
   <strong>
    通过事务机制，Kafka实现了对多个 topic 的多个 partition 的原子性的写入
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，即处于同一个事务内的所有消息，不管最终需要落地到哪个 topic 的哪个 partition, 最终结果都是要么全部写成功，要么全部写失败（Atomic multi-partition writes）；
   </span>
  </p>
  <p>
   Kafka的事务机制，在底层依赖于幂等生产者，幂等生产者是Kafka事务的必要不充分条件；
  </p>
  <p>
   事实上，开启Kafka事务时，Kafka会自动开启幂等生产者。
  </p>
  <p>
   <strong>
    2、Kafka内部是如何支持事务的
   </strong>
  </p>
  <p>
   1）为支持事务机制，Kafka引入了两个新的组件：Transaction Coordinator 和 Transaction Log。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136320378-f23ce517-0480-43dd-81fb-014e62cb614f.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_11%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <ul list="u51cdb8dd">
   <li fid="u54a5aabe">
    transaction coordinator 是运行在每个 kafka broker 上的一个模块，是 kafka broker 进程承载的新功能之一（不是一个独立的新的进程）；
   </li>
   <li fid="u54a5aabe">
    transaction log 是 kakafa 的一个内部 topic（类似大家熟悉的 __consumer_offsets ，是一个内部 topic）；
   </li>
   <li fid="u54a5aabe">
    transaction log 有多个分区，每个分区都有一个 leader，该 leade对应哪个 kafka broker，则那个 broker上的 transaction coordinator 就负责对这些分区的写操作；
   </li>
   <li fid="u54a5aabe">
    由于 transaction coordinator 是 kafka broker内部的一个模块，而 transaction log 是 kakfa 的一个内部 topic, 所以 KAFKA 可以通过内部的复制协议和选举机制（replication protocol and leader election processes)，来确保 transaction coordinator 的可用性和 transaction state 的持久性；
   </li>
   <li fid="u54a5aabe">
    transaction log topic 内部存储的只是事务的最新状态和其相关元数据信息，kafka producer 生产的原始消息，仍然是只存储在kafka producer指定的 topic 中。事务的状态有：“Ongoing,” “Prepare commit,” 和 “Completed.” 。
   </li>
   <li fid="u54a5aabe">
    实际上，每个 transactional.id 通过 hash 都对应到 了 transaction log 的一个分区，也就是说，每个 transactional.id 都有且仅有一个 transaction coordinator 负责。
   </li>
  </ul>
  <p>
   2）为支持事务机制，Kafka将日志文件格式进行了扩展，添加了控制消息 control batch
  </p>
  <p>
   为支持事务机制，Kafka将底层日志文件的格式进行了扩展：
  </p>
  <ul list="ue6c964ff">
   <li fid="ud95cc606">
    日志中除了普通的消息，还有一种消息专门用来标志事务的状态，它就是控制消息 control batch；
   </li>
   <li fid="ud95cc606">
    控制消息跟其他正常的消息一样，都被存储在日志中，但控制消息不会被返回给 consumer 客户端；
   </li>
   <li fid="ud95cc606">
    控制消息共有两种类型：commit 和 abort，分别用来表征事务已经成功提交或已经被成功终止；
   </li>
   <li fid="ud95cc606">
    RecordBatch 中 attributes 字段的第5位用来标志当前消息是否处于事务中，1代表消息处于事务中，0则反之；（A record batch is a container for records. ）
   </li>
   <li fid="ud95cc606">
    RecordBatch 中 attributes 字段的第6位用来标识当前消息是否是控制消息，1代表是控制消息，0则反之；
   </li>
   <li fid="ud95cc606">
    由于控制消息总是处于事务中，所以控制消息对应的RecordBatch 的 attributes 字段的第5位和第6位都被置为1。
   </li>
  </ul>
  <h3>
   offset你们公司如何维护的？为什么不放在MySQL？
  </h3>
  <p>
   问过的一些公司：小米(2022.09)，360社招(2020.01)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   一般来说，Kafka中的消费者位移（offset）会被存储在ZooKeeper或者Kafka内部的__consumer_offsets主题中。具体的实现方式可以使用Kafka提供的Consumer API，也可以使用第三方的Consumer SDK。另外，还可以使用一些开源的工具，如Kafka Connect、Kafka Manager等来管理消费者位移。
  </p>
  <p>
   相对于将消费者位移存储在MySQL等外部数据库中，将其存储在Kafka或ZooKeeper中有以下几个优点：
  </p>
  <ul list="u54556f1c">
   <li fid="ue9e278f7">
    实现简单：Kafka内置了__consumer_offsets主题，可直接使用，而不需要再另外创建数据库表。
   </li>
   <li fid="ue9e278f7">
    高可用性：Kafka和ZooKeeper都是分布式的、高可用的系统，能够提供良好的消费者位移存储服务。
   </li>
   <li fid="ue9e278f7">
    与Kafka集群的耦合度低：将消费者位移存储在Kafka或ZooKeeper中，可以避免将位移与业务数据存储在同一个外部存储系统中，减少了系统之间的耦合度。
   </li>
   <li fid="ue9e278f7">
    与Kafka的分区保持一致：如果将消费者位移存储在Kafka中，那么它们就可以与Kafka的分区一起管理，进而实现更加灵活和高效的消费者位移管理。
   </li>
  </ul>
  <h3>
   Kafka用的哪个版本，offset存在哪
  </h3>
  <p>
   问过的一些公司：美团(2019.11)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为 __consumer_offsets
  </p>
  <p>
   Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中。
  </p>
  <p>
   __consumer_offsets主题里面采用key和value的方式存储数据。key是group.id+topic+分区号，value就是当前offset的值。每隔一段时间，kafka内部会对这个topic进行compact，也就是每个group.id+topic+分区号就保留最新数据 。
  </p>
  <h3>
   Kafka如何实现幂等性？
  </h3>
  <p>
   问过的一些公司：深信服(2019.10)x2(2019.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   在之前的旧版本中，Kafka只能支持两种语义：At most once和At least once。At most once保证消息不会朝服，但是可能会丢失。在实践中，很有有业务会选择这种方式。At least once保证消息不会丢失，但是可能会重复，业务在处理消息需要进行去重。
  </p>
  <p>
   Kafka在0.11.0.0版本支持增加了对幂等的支持。幂等是针对生产者角度的特性。
   <strong>
    幂等可以保证上生产者发送的消息，不会丢失，而且不会重复
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
  </p>
  <p>
   <strong>
    1、如何实现幂等
   </strong>
  </p>
  <p>
   HTTP/1.1中对幂等性的定义是：一次和多次请求某一个资源对于资源本身应该具有同样的结果（网络超时等问题除外）。也就是说，其任意多次执行对资源本身所产生的影响均与一次执行的影响相同
  </p>
  <p>
   实现幂等的关键点就是服务端可以区分请求是否重复，过滤掉重复的请求。要区分请求是否重复的有两点：
  </p>
  <ul list="u7fa20862">
   <li fid="ub39afe6c">
    唯一标识：要想区分请求是否重复，请求中就得有唯一标识。例如支付请求中，订单号就是唯一标识。
   </li>
   <li fid="ub39afe6c">
    记录下已处理过的请求标识：光有唯一标识还不够，还需要记录下那些请求是已经处理过的，这样当收到新的请求时，用新请求中的标识和处理记录进行比较，如果处理记录中有相同的标识，说明是重复交易，拒绝掉。
   </li>
  </ul>
  <p>
   <strong>
    2、Kafka幂等性实现原理
   </strong>
  </p>
  <p>
   为了实现Producer的幂等性，Kafka引入了Producer ID（即PID）和Sequence Number。
  </p>
  <ul list="u8fc10170">
   <li fid="u64b1a4cf">
    PID。每个新的Producer在初始化的时候会被分配一个唯一的PID，这个PID对用户是不可见的。
   </li>
   <li fid="u64b1a4cf">
    Sequence Numbler。（对于每个PID，该Producer发送数据的每个&lt;Topic, Partition&gt;都对应一个从0开始单调递增的Sequence Number
   </li>
  </ul>
  <p>
   Kafka可能存在多个生产者，会同时产生消息，但对Kafka来说，只需要保证每个生产者内部的消息幂等就可以了，所有引入了PID来标识不同的生产者。
  </p>
  <p>
   对于Kafka来说，要解决的是生产者发送消息的幂等问题。也即需要区分每条消息是否重复。Kafka通过为每条消息增加一个Sequence Numbler，通过Sequence Numbler来区分每条消息。每条消息对应一个分区，不同的分区产生的消息不可能重复。所有Sequence Numbler对应每个分区
  </p>
  <p>
   Broker端在缓存中保存了这seq number，对于接收的每条消息，如果其序号比Broker缓存中序号大于1则接受它，否则将其丢弃。这样就可以实现了消息重复提交了。但是，只能保证单个Producer对于同一个&lt;Topic, Partition&gt;的Exactly Once语义。不能保证同一个Producer一个topic不同的partion幂等。
  </p>
  <p>
   <strong>
    3、实现幂等前后对比
   </strong>
  </p>
  <p>
   标准实现
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136321945-318b723d-8c82-492f-b149-1972e2322aee.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_22%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   发生重试时
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136322409-6997102b-faa7-43f4-938b-a6e0cf1cf7a2.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_21%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   实现幂等之后
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136322800-a3d87299-ddad-4bc6-a6de-03056d7f0de4.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_23%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   发生重试时
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136324092-281c436c-cbd4-48a5-8315-acf74737db79.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_23%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    4、幂等性示例
   </strong>
  </p>
  <p>
   生产者要使用幂等性很简单，只需要增加以下配置即可：
  </p>
  <p>
   enable.idempotence=True
  </p>
  <pre><code class="language-plain" lang="plain">Properties props = new Properties();
props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "True");
props.put("acks", "all"); // 当 enable.idempotence 为 True，这里默认为 all
props.put("bootstrap.servers", "localhost:9092");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaProducer producer = new KafkaProducer(props);

producer.send(new ProducerRecord(topic, "test");</code></pre>
  <p>
   Prodcuer 幂等性对外保留的接口非常简单，其底层的实现对上层应用做了很好的封装，应用层并不需要去关心具体的实现细节，对用户非常友好
  </p>
  <p>
   <strong>
    5、幂等性实现生产者流程
   </strong>
  </p>
  <p>
   此流程只展示了涉及生产者幂等性相关的重要操作
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136324438-dd4fc833-802a-458a-8d76-e07a46fa9951.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_22%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   这里重点关注幂等性相关的内容，首先，KafkaProducer启动时，会初始化一个
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    TransactionManager 实例，它的作用有以下几个部分：
   </span>
  </p>
  <ul list="u797beb9b">
   <li fid="ufd7f4412">
    记录本地的事务状态（事务性时必须）
   </li>
   <li fid="ufd7f4412">
    记录一些状态信息以保证幂等性，比如：每个 topic-partition 对应的下一个 sequence numbers 和 last acked batch（最近一个已经确认的 batch）的最大的 sequence number 等；
   </li>
   <li fid="ufd7f4412">
    记录 ProducerIdAndEpoch 信息（PID 信息）。
   </li>
  </ul>
  <p>
   幂等性时，Producer 的发送流程如下：
  </p>
  <p>
   1）调用kafkaProducer的send方法将数据添加到 RecordAccumulator 中，添加时会判断是否需要新建一个 ProducerBatch，这时这个 ProducerBatch 还是没有 PID 和 sequence number 信息的；
  </p>
  <p>
   2）Producer 后台发送线程 Sender，在 run() 方法中，会先根据 TransactionManager 的 shouldResetProducerStateAfterResolvingSequences() 方法判断当前的 PID 是否需要重置，重置的原因是因为：如果有topic-partition的batch已经超时还没处理完，此时可能会造成sequence number 不连续。因为sequence number 有部分已经分配出去了，而Kafka服务端没有收到这部分sequence number 的序号，Kafka服务端为了保证幂等性，只会接受同一个pid的sequence number 等于服务端缓存sequence number +1的消息，所有这时候需要重置Pid来保证幂等性。
  </p>
  <pre><code class="language-plain" lang="plain">synchronized boolean shouldResetProducerStateAfterResolvingSequences() {
         /**
          * 是否是事务即配置了Tid
          * 如果是事务则不需重置Pid
          */
         if (isTransactional())
             // We should not reset producer state if we are transactional. We will transition to a fatal error instead.
             return False;
         for (Iterator&lt;TopicPartition&gt; iter = partitionsWithUnresolvedSequences.iterator(); iter.hasNext(); ) {
             TopicPartition topicPartition = iter.next();
             if (!hasInflightBatches(topicPartition)) {//没有该分区的消息在发送中
                 // The partition has been fully drained. At this point, the last ack'd sequence should be once less than
                 // next sequence destined for the partition. If so, the partition is fully resolved. If not, we should
                 // reset the sequence number if necessary.
                 /**
                  * 判断SequenceNo是否连续
                  * 如果连续的，就不需要重置Pid
                  */
                 if (isNextSequence(topicPartition, sequenceNumber(topicPartition))) {
                     // This would happen when a batch was expired, but subsequent batches succeeded.
                     iter.remove();
                 } else {
                     // We would enter this branch if all in flight batches were ultimately expired in the producer.
                     log.info("No inflight batches remaining for {}, last ack'd sequence for partition is {}, next sequence is {}. " +
                             "Going to reset producer state.", topicPartition, lastAckedSequence(topicPartition), sequenceNumber(topicPartition));
                     return True;
                 }
             }
         }
         return False;
     }</code></pre>
  <p>
   3）Sender线程调用maybeWaitForProducerId()方法判断是否要申请Pid，如果需要，会阻塞直到成功申请到Pid
  </p>
  <pre><code class="language-plain" lang="plain">ProducerIdAndEpoch producerIdAndEpoch = None;   
 boolean isTransactional = False;
 if (transactionManager != None) {//有事务或者启用幂等
     //事务是否允许向此分区发送消息        
     if (!transactionManager.isSendToPartitionAllowed(tp))   
         break;  
     producerIdAndEpoch = transactionManager.producerIdAndEpoch(); 
     if (!producerIdAndEpoch.isValid())  
         // we cannot send the batch until we have refreshed the producer id 
         break;        
     //是否支持事务                 
     isTransactional = transactionManager.isTransactional(); 
     /**                     
      * 如果该分区的前面还有没发送完成的Batch，则需要跳过该分区的Batch，等待之前batch发送完成
      */
     if (!first.hasSequence() &amp;&amp; transactionManager.hasUnresolvedSequence(first.topicPartition))                          
         break;                                                                             
     /**   
      * 该分区存在发送中的Batch，该Batch有Sequence，和first的不相等。则跳过、
      *          
      * 也即first是个重试的Batch（因为它有Sequence），需要等待该分区发送中的Batch完成 
      */    
     int firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);                          
     if (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence()
             &amp;&amp; first.baseSequence() != firstInFlightSequence)            
         break; 
 } 
 ProducerBatch batch = deque.pollFirst();
 /** 
  * 校验当前batch是否已经设置了Sequence          
  * 如果没有，则需要设置batch的Sequence，增加对应分区的Next Sequence，将batch加入到inflightBatchesBySequence中                                     
  */         
 if (producerIdAndEpoch != None &amp;&amp; !batch.hasSequence()) { 
         //设置Batch的sequenceNumber 和isTransactional 
     batch.setProducerState(producerIdAndEpoch, transactionManager.sequenceNumber(batch.topicPartition), isTransactional);
     //增加该分区的sequenceNumber，增加值为Batch中消息的个数 
     transactionManager.incrementSequenceNumber(batch.topicPartition, batch.recordCount); 
     log.debug("Assigned producerId {} and producerEpoch {} to batch with base sequence " +
                     "{} being sent to partition {}", producerIdAndEpoch.producerId,
             producerIdAndEpoch.epoch, batch.baseSequence(), tp); 
     //加入到发送队列中 
     transactionManager.addInFlightBatch(batch);
 }                          
 batch.close();//关闭此batch，不可追加消息  
 size += batch.records().sizeInBytes();//累计size  
 ready.add(batch);//加到集合中，最后一起返回出去 
 batch.drained(now);//更新drainedMs时间戳</code></pre>
  <p>
   5）最后调用sendProduceRequest方法将消息发送出去
  </p>
  <h3>
   Kafka蓄水池机制
  </h3>
  <p>
   问过的一些公司：深信服(2019.10)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136327949-48be9dd3-4dd3-4e1b-9814-72daa5651648.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_27%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136328240-293f96a2-cc73-4b51-8ba5-47ccd55f6689.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_20%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   从上面的架构图可以看出，生产的流程主要就是一个producer线程和一个sender线程，它们之间通过BatchQueue来获取数据，它们的关系是一一对应的，所以kafka的生产过程都是异步过程，它的同步和异步指的是接收响应结果的模式是同步阻塞还是异步回调。同步和异步的生产者调用示例如下：
  </p>
  <p>
   异步生产模式：
  </p>
  <p>
   producer.send(new ProducerRecord&lt;&gt;(topic,messageNo,messageStr), new DemoCallBack(startTime, messageNo, messageStr));
  </p>
  <p>
   同步生产模式：
  </p>
  <p>
   producer.send(new ProducerRecord&lt;&gt;(topic,messageNo,messageStr)).get();
  </p>
  <p>
   同步接收是依据send之后返回Future，再调用Future的get方法进行阻塞等待。下面我们就从producer和sender两个类所对应的流程来进行分析，他们分别是消息收集过程和消息发送过程。从上面的架构图我们可以看到这个过程的数据最终是放在BatchQueue，像是将水流入了一个蓄水池的场景，这就是称其为”蓄水池”的含义了。
  </p>
  <h3>
   Kafka怎么防止脑裂
  </h3>
  <p>
   问过的一些公司：网易有道(2019.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、先说下什么是kafka controller
   </strong>
  </p>
  <p>
   控制器（controller）其实就是一个broker ，只不过它除了具有一般broker的功能之外，还负责分区首领的选举，相当于整个kafka集群的
   <strong>
    master
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，负责topic的创建、删除、以及partition的状态机转换，broker的上线、下线等。集群里第一个启动的broker通过在Zookeeper里创建一个临时节点/controller 让自己成为控制器。其它broker在启动时也会尝试创建这个节点，不过它们会收到一个“节点已存在”的异常，然后“意识”到控制器节点已存在，也就是说集群里已经有一个控制器了。其他broker在控制器节点上创建Zookeeper watch对象，这样它们就可以收到这个节点的变更通知。这种方式可以确保集群次只有一个控制器存在。
   </span>
  </p>
  <p>
   <strong>
    2、什么是脑裂
   </strong>
  </p>
  <p>
   kafka中只有一个控制器controller 负责分区的leader选举，同步broker的新增或删除消息，但有时由于网络问题，可能同时有两个broker认为自己是controller，这时候其他的broker就会发生脑裂，不知道该听从谁的。
  </p>
  <p>
   <strong>
    3、如何解决脑裂
   </strong>
  </p>
  <p>
   通过controller epoch来解决。
  </p>
  <p>
   每当新的controller产生时就会通过Zookeeper生成一个全新的、数值更大的controller epoch标识。其他broker在知道当前controller epoch后，如果收到由控制器发出的包含较旧epoch的消息，就会忽略它们。
  </p>
  <h3>
   Kafka同步消费和异步消费知道吗？
  </h3>
  <p>
   问过的一些公司：字节(2022.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka消费者支持同步和异步两种方式消费消息。
  </p>
  <p>
   同步消费：消费者同步地从Kafka集群中读取消息，并在收到响应之前一直等待。如果消息没有可用，消费者会阻塞，直到有消息可用。这种方式主要用于对实时性要求高的场景。
  </p>
  <p>
   异步消费：消费者通过回调函数异步地接收消息。消费者不阻塞，回调函数会在消息到达时被触发。这种方式主要用于处理消息的吞吐量比较高的场景。
  </p>
  <p>
   选择哪种方式消费，取决于应用场景和对实时性和吞吐量的要求。
  </p>
  <h3>
   项目里要重新设置offset到某个时间点怎么做
  </h3>
  <p>
   问过的一些公司：远景智能(2021.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   要将Kafka的偏移量（offset）重新设置为特定时间点，可以按照以下步骤进行操作：
  </p>
  <p>
   1）找到Kafka集群的配置文件（通常名为
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    server.properties
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ），并打开该文件。
   </span>
  </p>
  <p>
   2）在该配置文件中找到并编辑以下两个配置参数：
  </p>
  <ul list="u0e56c335">
   <li fid="u6e5e5163">
    offsets.topic.retention.minutes
    ：该参数定义了Kafka保存offsets的时间（以分钟为单位）。将其设置为较大的值，确保在重新设置offset之后，消费者有足够的时间读取数据。
   </li>
   <li fid="u6e5e5163">
    offsets.retention.check.interval.ms
    <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
     ：该参数定义了Kafka检查offsets的间隔时间（以毫秒为单位）。将其设置为较短的时间，以确保offsets尽快被更新。
    </span>
   </li>
  </ul>
  <p>
   3）确定要将offsets设置为的时间点，并使用以下命令将其设置为新的offsets：
  </p>
  <p>
   kafka-consumer-groups --bootstrap-server &lt;kafka_broker&gt; --group &lt;consumer_group&gt; --reset-offsets --to-date &lt;timestamp&gt; --topic &lt;topic_name&gt; --execute
  </p>
  <p>
   其中，
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    &lt;kafka_broker&gt;
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    是Kafka集群的地址，
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    &lt;consumer_group&gt;
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    是要重置offsets的消费者组名称，
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    &lt;timestamp&gt;
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    是要设置的时间点（格式为
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    yyyy-MM-ddTHH:mm:ss
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ），
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    &lt;topic_name&gt;
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    是要重置offsets的主题名称。
   </span>
  </p>
  <p>
   4）重新启动消费者，以便它们可以从新的offsets开始读取消息。
  </p>
  <p>
   注意，重新设置offsets可能会导致数据丢失或重复消费。因此，在执行此操作之前，要确保已经备份了重要的数据。
  </p>
  <h3>
   offset提交的几种方式
  </h3>
  <p>
   问过的一些公司：远景智能(2021.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka对于offset的处理有两种提交方式：
  </p>
  <ul list="udf6aaec7">
   <li fid="ub19c5904">
    自动提交（默认的提交方式）
   </li>
   <li fid="ub19c5904">
    手动提交（可以灵活地控制offset）
   </li>
  </ul>
  <p>
   1、自动提交偏移量
  </p>
  <p>
   Kafka中偏移量的自动提交是由参数enable_auto_commit和auto_commit_interval_ms控制的，当enable_auto_commit=True时，Kafka在消费的过程中会以频率为auto_commit_interval_ms向Kafka自带的topic(__consumer_offsets)进行偏移量提交，具体提交到哪个Partation是以算法：partation=hash(group_id)%50来计算的。
  </p>
  <p>
   如：
  </p>
  <p>
   group_id=test_group_1，则partation=hash("test_group_1")%50=28
  </p>
  <p>
   2、手动提交偏移量
  </p>
  <p>
   鉴于Kafka自动提交offset的不灵活性和不精确性(只能是按指定频率的提交)，Kafka提供了手动提交offset策略。手动提交能对偏移量更加灵活精准地控制，以保证消息不被重复消费以及消息不被丢失。
  </p>
  <p>
   对于手动提交offset主要有3种方式：1.同步提交 2.异步提交 3.异步+同步 组合的方式提交
  </p>
  <p>
   1）同步手动提交偏移量
  </p>
  <p>
   同步模式下提交失败的时候一直尝试提交，直到遇到无法重试的情况下才会结束，同时同步方式下消费者线程在拉取消息会被阻塞，在broker对提交的请求做出响应之前，会一直阻塞直到偏移量提交操作成功或者在提交过程中发生异常，限制了消息的吞吐量。
  </p>
  <p>
   2）异步手动提交偏移量+回调函数
  </p>
  <p>
   异步手动提交offset时，消费者线程不会阻塞，提交失败的时候也不会进行重试，并且可以配合回调函数在broker做出响应的时候记录错误信息。
  </p>
  <p>
   3）异步+同步组合的方式提交偏移量
  </p>
  <p>
   针对异步提交偏移量丢失的问题，通过对消费者进行异步批次提交并且在关闭时同步提交的方式，这样即使上一次的异步提交失败，通过同步提交还能够进行补救，同步会一直重试，直到提交成功。
  </p>
  <h3>
   Kafka是怎么知道每个topic的分区是该存在哪个地方的？
  </h3>
  <p>
   问过的一些公司：蔚来提前批(2021.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka中元数据（Metadata）含有集群中各个主题、分区、Broker等相关信息的数据。
  </p>
  <p>
   Kafka的元数据存储在ZooKeeper集群中，通过ZooKeeper提供的协调服务，Kafka可以实现元数据的管理和维护。
  </p>
  <p>
   比如：
  </p>
  <ul list="u27fdbc6c">
   <li fid="u62cba42a">
    Broker信息：包括每个Broker的ID、主机名、端口号等基本信息。
   </li>
   <li fid="u62cba42a">
    Topic信息：包括每个主题的名称、分区数、分区副本数等信息。
   </li>
   <li fid="u62cba42a">
    分区信息：包括每个分区的ID、所属主题、分区副本所在的Broker、备选副本列表等信息。
   </li>
   <li fid="u62cba42a">
    分区副本信息：包括每个分区副本的ID、所在Broker的ID、是否是Leader副本等信息。
   </li>
  </ul>
  <p>
   通过ZooKeeper中的元数据，Kafka消费者可以获取与特定主题相关的分区、分区副本以及每个分区副本所在的Broker信息。
  </p>
  <h3>
   Kafka分区副本之间是怎么交流的？
  </h3>
  <p>
   问过的一些公司：蔚来提前批(2021.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   在Kafka中，分区副本之间的通信主要是通过Zookeeper和Broker之间的协作来实现的，包括Leader副本的选举、元数据的更新、数据同步等操作。
  </p>
  <p>
   当一个分区副本发生故障或下线时，其他副本会通过Zookeeper集群中的协调服务进行通信，确定新的Leader副本。Zookeeper会将新的Leader副本的信息广播给所有副本，让它们更新自己的元数据信息，从而确保整个集群的一致性。
  </p>
  <p>
   除了Leader副本的选举外，分区副本之间还会通过Broker之间的数据同步来保证数据的可靠性和一致性。具体来说，Leader副本会将所有的消息写入分区日志，并将消息的偏移量（offset）发送给所有副本。其他副本会定期从Leader副本同步数据，将缺失的消息从Leader副本复制过来，并将复制的消息追加到自己的分区日志中。
  </p>
  <p>
   在数据同步过程中，Kafka还使用了一种高效的机制——批量复制（Batch Replication）。批量复制可以将多个消息一次性发送给副本，减少了网络传输的开销，提高了数据同步的效率。
  </p>
  <h3>
   如果你正在消费一条数据，此时Kafka挂掉了，那么重启以后，消费的offset是哪一个
  </h3>
  <p>
   问过的一些公司：招银网络(2021.05)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   当Kafka服务器挂掉时，消费者正在消费的offset会保存在消费者所在的客户端内存中，并且会定期定时地将消费的offset提交到Kafka集群中。如果此时Kafka服务器挂掉，那么消费者无法将消费的offset提交到Kafka集群中，此时消费者的offset并没有被记录到Kafka集群中，也就是说，
   <strong>
    在Kafka服务器重启后，消费者的offset会被重置到上一次提交的offset位置，而不是当前正在消费的offset位置
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。
   </span>
  </p>
  <p>
   Kafka提供了两种offset管理方式：自动管理和手动管理。在自动管理方式下，消费者会定期地将消费的offset提交到Kafka集群中，Kafka会自动记录消费的offset位置。在手动管理方式下，消费者需要手动调用
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    commitSync()
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    或
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    commitAsync()
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    方法将消费的offset提交到Kafka集群中，手动管理方式可以更加精确地控制offset的提交位置，但也需要更多的编程工作。
   </span>
  </p>
  <p>
   无论采用哪种offset管理方式，当Kafka服务器挂掉时，如果消费者无法将消费的offset提交到Kafka集群中，那么在Kafka服务器重启后，消费者会从上一次提交的offset位置重新开始消费。如果希望避免这种情况，可以使用Kafka的高级消费者API中提供的
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    auto.offset.reset
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    属性来配置消费者在没有提交offset的情况下从哪里开始消费：如果设置为
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    latest
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，则从最新的消息开始消费；如果设置为
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    earliest
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，则从最早的消息开始消费。
   </span>
  </p>
  <h3>
   Kafka的Topic
  </h3>
  <p>
   问过的一些公司：跟谁学(2020.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   在Kafka中，Topic（主题）是一个存储消息的逻辑概念，可以认为是一个消息集合。每个Topic都由一个或多个分区（Partition）组成，每个分区都有自己的一组有序的消息序列。消息发送者可以向指定Topic的一个或多个分区发送消息，而消息消费者可以订阅一个或多个Topic，消费指定分区的消息。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2023/png/28141494/1682136328259-1b3423c7-07aa-4e3d-a691-5b09b9324040.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_22%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <h3>
   Kafka的Consumer挂掉了但Producer还在发数据，这种情况会怎样
  </h3>
  <p>
   问过的一些公司：京东(2020.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   如果Kafka的Consumer挂掉了，但Producer还在发送数据，Kafka集群仍然会继续接收和存储Producer发送的数据，只是这些数据不会被该Consumer消费。这种情况下，数据仍然会保存在Kafka的Topic中，后续其他Consumer启动后继续消费。
  </p>
  <p>
   Kafka中有一个参数
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    auto.offset.reset
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，用于控制当消费者启动时，如何处理之前未被消费的消息。该参数有两个可选值：
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    earliest
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    和
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    latest
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。如果将
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    auto.offset.reset
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    设置为
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    earliest
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，则新启动的consumer会从该Topic最早的未被消费的消息开始消费；如果设置为
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    latest
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，则新启动的consumer会从该Topic最新的消息开始消费。因此，如果挂掉的consumer重新启动，并且该参数设置为
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    earliest
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，它将从该Topic最早的未被消费的消息开始消费。如果设置为
   </span>
   <span class="lake-fontsize-12" style="color: rgb(233, 105, 0); background-color: rgb(248, 248, 248)">
    latest
   </span>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ，则它将从最新的消息开始消费。
   </span>
  </p>
  <h3>
   Kafka怎么保证数据不丢失，不重复？
  </h3>
  <p>
   可回答：Kafka如何保证生产者不丢失数据，消费者不丢失数据？
  </p>
  <p>
   问过的一些公司：腾讯云(2020.03)，安恒信息(2020.03)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    存在数据丢失的几种情况
   </strong>
  </p>
  <ul list="u726b0ab8">
   <li fid="uecc84cb6">
    使用同步模式的时候，有3种状态保证消息被安全生产，在配置为1（只保证写入leader成功）的话，如果刚好leader partition挂了，数据就会丢失。
   </li>
   <li fid="uecc84cb6">
    还有一种情况可能会丢失消息，就是使用异步模式的时候，当缓冲区满了，如果配置为0（还没有收到确认的情况下，缓冲池一满，就清空缓冲池里的消息），数据就会被立即丢弃掉。
   </li>
  </ul>
  <p>
   <strong>
    避免方法的一些概述
   </strong>
  </p>
  <p>
   <strong>
    1、在数据生产时避免数据丢失的方法
   </strong>
  </p>
  <p>
   只要能避免上述两种情况，那么就可以保证消息不会被丢失。
  </p>
  <p>
   1）在同步模式的时候，确认机制设置为-1，也就是让消息写入leader和所有的副本。
  </p>
  <p>
   2）在异步模式下，如果消息发出去了，但还没有收到确认的时候，缓冲池满了，在配置文件中设置成不限制阻塞超时的时间，也就说让生产端一直阻塞，这样也能保证数据不会丢失。
  </p>
  <p>
   在数据消费时，避免数据丢失的方法：确认数据被完成处理之后，再更新offset值。低级API中需要手动控制offset值。
  </p>
  <p>
   消息队列的问题都要从源头找问题，就是生产者是否有问题。
  </p>
  <p>
   讨论一种情况，如果数据发送成功，但是接受response的时候丢失了，机器重启之后就会重发。
  </p>
  <p>
   重发很好解决，消费端增加去重表就能解决，但是如果生产者丢失了数据，问题就很麻烦了。
  </p>
  <p>
   <strong>
    2、数据重复消费的情况，处理情况如下
   </strong>
  </p>
  <p>
   1）去重：将消息的唯一标识保存到外部介质中，每次消费处理时判断是否处理过；
  </p>
  <p>
   2）不管：大数据场景中，报表系统或者日志信息丢失几条都无所谓，不会影响最终的统计分析结。
  </p>
  <p>
   Kafka到底会不会丢数据(data loss)? 通常不会，但有些情况下的确有可能会发生。下面的参数配置及Best practice列表可以较好地保证数据的持久性(当然是trade-off，牺牲了吞吐量)。
  </p>
  <p>
   如果想要高吞吐量就要能容忍偶尔的失败（重发漏发无顺序保证）。
  </p>
  <pre><code class="language-plain" lang="plain">block.on.buffer.full = True
 acks = all
 retries = MAX_VALUE
 max.in.flight.requests.per.connection = 1
 使用KafkaProducer.send(record, callback)
 callback逻辑中显式关闭producer：close(0) 
 unclean.leader.election.enable=False
 replication.factor = 3 
 min.insync.replicas = 2
 replication.factor &gt; min.insync.replicas
 enable.auto.commit=False</code></pre>
  <p>
   消息处理完成之后再提交位移
  </p>
  <p>
   给出列表之后，我们从两个方面来探讨一下数据为什么会丢失：
  </p>
  <p>
   <strong>
    Producer端
   </strong>
  </p>
  <p>
   新版本的Kafka替换了Scala版本的old producer，使用了由Java重写的producer。新版本的producer采用异步发送机制。KafkaProducer.send(ProducerRecord)方法仅仅是把这条消息放入一个缓存中(即RecordAccumulator，本质上使用了队列来缓存记录)，同时后台的IO线程会不断扫描该缓存区，将满足条件的消息封装到某个batch中然后发送出去。显然，
   <strong>
    这个过程中就有一个数据丢失的窗口
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    ：若IO线程发送之前client端挂掉了，累积在accumulator中的数据的确有可能会丢失。
   </span>
  </p>
  <p>
   Producer的另一个问题是
   <strong>
    消息的乱序问题
   </strong>
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    。假设客户端代码依次执行下面的语句将两条消息发到相同的分区
   </span>
  </p>
  <pre><code class="language-plain" lang="plain">producer.send(record1);
 producer.send(record2);</code></pre>
  <p>
   如果此时由于某些原因(比如瞬时的网络抖动)导致record1没有成功发送，同时Kafka又配置了重试机制和max.in.flight.requests.per.connection大于1(默认值是5，本来就是大于1的)，那么重试record1成功后，record1在分区中就在record2之后，从而造成消息的乱序。很多某些要求强顺序保证的场景是不允许出现这种情况的。发送之后重发就会丢失顺序。
  </p>
  <p>
   鉴于producer的这两个问题，我们应该如何规避呢？？对于消息丢失的问题，很容易想到的一个方案就是：既然异步发送有可能丢失数据， 我改成同步发送总可以吧？比如这样：
  </p>
  <p>
   producer.send(record).get();
  </p>
  <p>
   这样当然是可以的，但是性能会很差，不建议这样使用。以下的配置清单应该能够比较好地规避producer端数据丢失情况的发生：(特此说明一下，软件配置的很多决策都是trade-off，下面的配置也不例外：应用了这些配置，你可能会发现你的producer/consumer 吞吐量会下降，这是正常的，因为你换取了更高的数据安全性)。
  </p>
  <p>
   block.on.buffer.full = True 尽管该参数在0.9.0.0已经被标记为“deprecated”，但鉴于它的含义非常直观，所以这里还是显式设置它为True，使得producer将一直等待缓冲区直至其变为可用。否则如果producer生产速度过快耗尽了缓冲区，producer将抛出异常。缓冲区满了就阻塞在那，不要抛异常，也不要丢失数据。
  </p>
  <p>
   acks=all 很好理解，所有follower都响应了才认为消息提交成功，即"committed"。
  </p>
  <p>
   retries = MAX 无限重试，直到你意识到出现了问题。
  </p>
  <p>
   max.in.flight.requests.per.connection = 1 限制客户端在单个连接上能够发送的未响应请求的个数。设置此值是1表示kafka broker在响应请求之前client不能再向同一个broker发送请求。注意：设置此参数是为了避免消息乱序。
  </p>
  <p>
   使用KafkaProducer.send(record, callback)而不是send(record)方法 自定义回调逻辑处理消息发送失败，比如记录在日志中，用定时脚本扫描重处理。
  </p>
  <p>
   callback逻辑中最好显式关闭producer：close(0) 注意：设置此参数是为了避免消息乱序（仅仅因为一条消息发送没收到反馈就关闭生产者，感觉代价很大）。
  </p>
  <p>
   unclean.leader.election.enable=False 关闭unclean leader选举，即不允许非ISR中的副本被选举为leader，以避免数据丢失。
   <span class="lake-fontsize-12" style="color: rgb(52, 73, 94)">
    replication.factor &gt;= 3 参考了Hadoop及业界通用的三备份原则。
   </span>
  </p>
  <p>
   min.insync.replicas &gt; 1 消息至少要被写入到这么多副本才算成功，也是提升数据持久性的一个参数。与acks配合使用保证replication.factor &gt; min.insync.replicas 如果两者相等，当一个副本挂掉了分区也就没法正常工作了。通常设置replication.factor = min.insync.replicas + 1即可。
  </p>
  <p>
   <strong>
    Consumer端
   </strong>
  </p>
  <p>
   consumer端丢失消息的情形比较简单：如果在消息处理完成前就提交了offset，那么就有可能造成数据的丢失。由于Kafka consumer默认是自动提交位移的，所以在后台提交位移前一定要保证消息被正常处理了，因此不建议采用很重的处理逻辑，如果处理耗时很长，则建议把逻辑放到另一个线程中去做。为了避免数据丢失，现给出两点建议：
  </p>
  <ul list="ud8d895e6">
   <li fid="u3f4e0adb">
    enable.auto.commit=False，关闭自动提交位移
   </li>
   <li fid="u3f4e0adb">
    在消息被完整处理之后再手动提交位移
   </li>
  </ul>
  <h2>
   V3.0
  </h2>
  <blockquote>
   <p>
    因为牛客上有的面经被发布者删除了，所有这部分内容是原来V3.0版本中有，但是现在4.0没有的
   </p>
  </blockquote>
  <h3>
   Kafka作为消息队列，它可解决什么样的问题？
  </h3>
  <p>
   问过的一些公司：Shopee(2021.07)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka是消息中间件的一种，它可以让合适的数据以合适的形式出现在合适的地方。Kafka的做法是提供消息队列，让生产者单往队列的末尾添加数据，让多个消费者从队列里面依次读取数据然后自行处理。
  </p>
  <p>
   Kafka主要用途是数据集成，或者说是流数据集成，以Pub/Sub形式的消息总线形式提供。但是，Kafka不仅仅是一套传统的消息总线，本质上Kafka是分布式的流数据平台，因为以下特性而著名：
  </p>
  <ul list="u15a2b487">
   <li fid="u2265a550">
    提供Pub/Sub方式的海量消息处理。
   </li>
   <li fid="u2265a550">
    以高容错的方式存储海量数据流。
   </li>
   <li fid="u2265a550">
    保证数据流的顺序。
   </li>
  </ul>
  <p>
   <strong>
    缓冲和削峰
   </strong>
   <span class="lake-fontsize-12">
    ：上游数据时有突发流量，下游可能扛不住，或者下游没有足够多的机器来保证冗余，kafka在中间可以起到一个缓冲的作用，把消息暂存在kafka中，下游服务就可以按照自己的节奏进行慢慢处理。
   </span>
  </p>
  <p>
   解耦和扩展性：项目开始的时候，并不能确定具体需求。消息队列可以作为一个接口层，解耦重要的业务流程。只需要遵守约定，针对数据编程即可获取扩展能力。
  </p>
  <p>
   冗余：可以采用一对多的方式，一个生产者发布消息，可以被多个订阅topic的服务消费到，供多个毫无关联的业务使用。
  </p>
  <p>
   健壮性：消息队列可以堆积请求，所以消费端业务即使短时间死掉，也不会影响主要业务的正常进行。
  </p>
  <p>
   异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。
  </p>
  <h3>
   Kafka的leader挂掉之后处理方法
  </h3>
  <p>
   问过的一些公司：京东(2021.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   当leader挂掉之后，会从ISR的follower中选举新的leader。
  </p>
  <h3>
   Kafka的ISR、OSR和ACK介绍，ACK分别有几种值？
  </h3>
  <p>
   问过的一些公司：快手，ebay
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    ISR
   </strong>
   <span class="lake-fontsize-12">
    （In-Sync Replicas）：副本同步队列
   </span>
  </p>
  <p>
   ISR是Leader维护了一个动态副本同步队列，是和leader保持同步的follower集合。
  </p>
  <p>
   <strong>
    OSR
   </strong>
   <span class="lake-fontsize-12">
    （Out-Sync Relipcas）：
   </span>
  </p>
  <p>
   当ISR中的follower长时间（replica.lag.time.max.ms参数设定）未向leader同步数据，则该follower将被踢出ISR，加入到OSR。
  </p>
  <p>
   <strong>
    ACK
   </strong>
   <span class="lake-fontsize-12">
    ：producer的消息发送确认机制
   </span>
  </p>
  <p>
   ack=0
  </p>
  <p>
   producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据
  </p>
  <p>
   ack=1
  </p>
  <p>
   producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；
  </p>
  <p>
   ack=-1
  </p>
  <p>
   producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。
  </p>
  <h3>
   Kafka如何尽可能保证数据可靠性？
  </h3>
  <p>
   可回答：Kafka如何保证消费者消费数据的可靠性
  </p>
  <p>
   问过的一些公司：星环科技，多益，ebay，招银网络x2，荣耀(2021.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、Topic分区副本
   </strong>
  </p>
  <p>
   在 Kafka 0.8.0 之前，Kafka 是没有副本的概念的，那时候人们只会用 Kafka 存储一些不重要的数据，因为没有副本，数据很可能会丢失。但是随着业务的发展，支持副本的功能越来越强烈，所以为了保证数据的可靠性，Kafka 从 0.8.0 版本开始引入了分区副本（详情请参见 KAFKA-50）。也就是说每个分区可以人为的配置几个副本（比如创建主题的时候指定 replication-factor，也可以在 Broker 级别进行配置 default.replication.factor），一般会设置为3。
  </p>
  <p>
   Kafka 可以保证单个分区里的事件是有序的，分区可以在线（可用），也可以离线（不可用）。在众多的分区副本里面有一个副本是 Leader，其余的副本是 follower，所有的读写操作都是经过 Leader 进行的，同时 follower 会定期地去 leader 上的复制数据。当 Leader 挂了的时候，其中一个 follower 会重新成为新的 Leader。通过分区副本，引入了数据冗余，同时也提供了 Kafka 的数据可靠性。
  </p>
  <p>
   Kafka 的分区多副本架构是 Kafka 可靠性保证的核心，把消息写入多个副本可以使 Kafka 在发生崩溃时仍能保证消息的持久性。
  </p>
  <p>
   <strong>
    2、ISR机制
   </strong>
  </p>
  <p>
   Kafka有两种副本数据同步策略（Kafka选择第二种）
  </p>
  <table class="lake-table" margin="True" style="width: 750px">
   <colgroup>
    <col width="250"/>
    <col width="250"/>
    <col width="250"/>
   </colgroup>
   <tbody>
    <tr>
     <td>
      <p>
       方案
      </p>
     </td>
     <td>
      <p>
       优点
      </p>
     </td>
     <td>
      <p>
       缺点
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p>
       半数以上完成同步，就发送ack
      </p>
     </td>
     <td>
      <p>
       延迟低
      </p>
     </td>
     <td>
      <p>
       选举新的leader时，容忍n台节点的故障，需要2n+1个副本
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p>
       全部完成同步，才发送ack
      </p>
     </td>
     <td>
      <p>
       选举新的leader时，容忍n台节点的故障，需要n+1个副本
      </p>
     </td>
     <td>
      <p>
       延迟高
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   Kafka选择了第二种方案，原因如下：
  </p>
  <ul list="u3f2c65c9">
   <li fid="uec9bb5eb">
    为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。
   </li>
   <li fid="uec9bb5eb">
    虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。
   </li>
  </ul>
  <p>
   为了防止Kafka在选择第二种数据同步策略时，因为某一个follower故障导致leader一直等下去，Leader维护了一个动态的in-sync replica set (ISR)。
  </p>
  <p>
   <strong>
    ISR
   </strong>
   <span class="lake-fontsize-12">
    ：同步副本，和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给生产者发送ack。
   </span>
  </p>
  <p>
   <strong>
    特殊情况
   </strong>
   <span class="lake-fontsize-12">
    ：
   </span>
  </p>
  <ul list="ud8e3b56c">
   <li fid="u5f48a074">
    如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定（默认：10s）。
   </li>
   <li fid="u5f48a074">
    如果Leader发生故障，就会从ISR中选举新的leader。
   </li>
  </ul>
  <p>
   <strong>
    3、ack应答机制
   </strong>
  </p>
  <p>
   为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662714065334-2b546947-4c6a-4328-a5cd-9de7c7cd7f15.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_21%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   Kafka为用户提供了三种可靠性级别（acks参数）：
  </p>
  <ul list="uf771c5ff">
   <li fid="u09d9b375">
    <strong>
     acks=0
    </strong>
    <span class="lake-fontsize-12">
     <br/>
    </span>
    <span class="lake-fontsize-12">
     producer不等待broker的ack，broker一接收到还没有写入磁盘就已经返回。
     <br/>
    </span>
    <span class="lake-fontsize-12">
     当
    </span>
    <strong>
     broker故障时
    </strong>
    <span class="lake-fontsize-12">
     ，有可能
    </span>
    <strong>
     丢失数据
    </strong>
    <span class="lake-fontsize-12">
     。
    </span>
   </li>
   <li fid="u09d9b375">
    <strong>
     acks=1
    </strong>
    <span class="lake-fontsize-12">
     <br/>
    </span>
    <span class="lake-fontsize-12">
     producer等待broker的ack，partition的leader落盘成功后返回ack。
     <br/>
    </span>
    <span class="lake-fontsize-12">
     如果在follower同步成功之前leader故障，那么将会
    </span>
    <strong>
     丢失数据
    </strong>
    <span class="lake-fontsize-12">
     。
    </span>
   </li>
   <li fid="u09d9b375">
    <strong>
     acks=-1(all)
    </strong>
    <span class="lake-fontsize-12">
     <br/>
    </span>
    <span class="lake-fontsize-12">
     producer等待broker的ack，partition的leader和follower（ISR中的所有follower）全部落盘成功后才返回ack。
     <br/>
    </span>
    <span class="lake-fontsize-12">
     如果在
    </span>
    <strong>
     follower同步完成后
    </strong>
    <span class="lake-fontsize-12">
     ，broker
    </span>
    <strong>
     发送ack之前leader发生故障
    </strong>
    <span class="lake-fontsize-12">
     ，此时kafka从ISR中重新选举一个leader，生产者没有收到ack重新发送一份到新leader上，则造成
    </span>
    <strong>
     数据重复
    </strong>
    <span class="lake-fontsize-12">
     。
     <br/>
    </span>
    <span class="lake-fontsize-12">
     如果
    </span>
    <strong>
     ISR中只剩一个leader
    </strong>
    <span class="lake-fontsize-12">
     时，此时
    </span>
    <strong>
     leader发生故障
    </strong>
    <span class="lake-fontsize-12">
     ，可能会造成
    </span>
    <strong>
     数据丢失
    </strong>
    <span class="lake-fontsize-12">
     。
     <br/>
    </span>
    <span class="lake-fontsize-12">
     如果
    </span>
    <strong>
     一个follower故障
    </strong>
    <span class="lake-fontsize-12">
     ，该节点被踢出ISR，只要ISR中所有节点都同步即可返回ack，
    </span>
    <strong>
     不影响
    </strong>
    <span class="lake-fontsize-12">
     。
    </span>
   </li>
  </ul>
  <p>
   <strong>
    4、故障处理：HW、LEO
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662714667682-097a890f-f4c7-4d28-999e-ebf893a50d2c.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_34%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <ul list="uf964e993">
   <li fid="u60af479d">
    LEO：每个副本的最后一个Offset
   </li>
   <li fid="u60af479d">
    HW：所有副本中最小的LEO
   </li>
  </ul>
  <p>
   1）follower故障
  </p>
  <p>
   follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并
   <strong>
    将log文件高于HW的部分截取掉
   </strong>
   <span class="lake-fontsize-12">
    ，从HW开始向leader进行同步。等
   </span>
   <strong>
    该follower的LEO大于等于该Partition的HW
   </strong>
   <span class="lake-fontsize-12">
    ，即follower追上leader之后，就可以重新加入ISR了。
   </span>
  </p>
  <p>
   2）leader故障
  </p>
  <p>
   leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会
   <strong>
    先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据
   </strong>
   <span class="lake-fontsize-12">
    。
   </span>
  </p>
  <p>
   注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。（是否丢数据是acks保证）
  </p>
  <h3>
   Kafka如何保证全局有序？
  </h3>
  <p>
   可回答：1）Kafka消费者怎么保证有序性？2）Kafka生产者写入数据怎么保证有序？3）Kafka可以保证数据的局部有序，如何保证数据的全局有序？4）Kafka消息的有序性
  </p>
  <p>
   问过的一些公司：快手x3，360x2，安恒信息，京东，京东(2021.07)，重庆富民银行(2021.09)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、设置Key值，指定分区
   </strong>
  </p>
  <p>
   kafka分区是存在K和V的，K就是分区，一般都是默认的，而默认的经常会发生一些我们并不像看到的结果，例如对同一数据进行多次操作不同分区会导致后进入先出，这就是因为跨分区导致的结果，因此我们要
   <strong>
    设置key用来进行hash取模来确定分区
   </strong>
   <span class="lake-fontsize-12">
    ，并且，这个再kafka源码是存在的，就在DefaultPartitioner.java中。
   </span>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662714733476-f97702f8-c3ee-4d89-9a5f-4e2d6c31b7b6.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_35%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   通过源码我们可以发现，再源码中默认的key为空，则系统会运算出一个partition，如果用这种方式，那么，就会导致分区内有序而分区无序，会导致数据无序，因此，
   <strong>
    要指定一个key值，也就是指定分区，这样的话，同一个数据发送到同一个分区，而多个分区依旧可以并行，同时实现数据有序和多分区并行
   </strong>
   <span class="lake-fontsize-12">
    。
   </span>
  </p>
  <p>
   如果数据是从MySQL传过来的话，一般来说都是会有自增长主键的，我们可以直接读取主键来作为运算的key值进行操作，这样就可以保证数据的操作唯一性了。
  </p>
  <p>
   <strong>
    2、
   </strong>
   <code>
    <strong>
     max.in.flight.requests.per.connection
    </strong>
   </code>
   <strong>
    设置为1
   </strong>
  </p>
  <p>
   <code>
    max.in.flight.requests.per.connection
   </code>
   <span class="lake-fontsize-12">
    设置为1,每个批次里面一次只写入一条消息到kafka里面，而这个参数，在kafka官网中是有说明的：
   </span>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662714758936-85457bad-23b9-441c-b8a7-7cbde8045e19.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_35%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   官网已经说的很明显了，如果设置打于1，那么每次重试可能存在从新排序（乱序）的情况，比如我设置为5，那么我传输的过程应该是1，2，3，4，5。
  </p>
  <p>
   但是如果网络波动导致3丢失，那么kafka自动重试只会拉取3，也就是排序就成了1，2，4，5，3。
  </p>
  <p>
   所以，记得将每个批次的消息数设置为1，防止出现重试乱序的情况。
  </p>
  <p>
   <strong>
    3、设置重试次数大于100次
   </strong>
  </p>
  <p>
   每一次kafka操作为了防止网络延时等问题，要设置重试次数大于100次
  </p>
  <h3>
   生产者消费者模式与发布订阅模式有何异同？
  </h3>
  <p>
   问过的一些公司：百度
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    1、点对点模式
   </strong>
   <span class="lake-fontsize-12">
    （一对一，消费者主动拉取数据，消息收到后消息清除）
   </span>
  </p>
  <p>
   消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。
  </p>
  <p>
   消息被消费以后，Queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。
   <span class="lake-fontsize-12" style="color: #F5222D">
    Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费
   </span>
   <span class="lake-fontsize-12">
    。
   </span>
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1665144381464-f877937e-000f-465f-852b-6ddaf64fc3d3.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_30%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    2、发布/订阅模式
   </strong>
   <span class="lake-fontsize-12">
    （一对多，消费者消费数据之后不会清除消息）
   </span>
  </p>
  <p>
   消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。
   <span class="lake-fontsize-12" style="color: #F5222D">
    和点对点模式不同，发布到topic的消息会被所有订阅者消费
   </span>
   <span class="lake-fontsize-12">
    。
   </span>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1665144672077-b2766e1f-254e-42f6-a1cf-183f56001048.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_32%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <h3>
   Kafka的offset管理
  </h3>
  <p>
   问过的一些公司：招银网络
  </p>
  <p>
   参考答案：
  </p>
  <p>
   消费者在消费的过程中需要记录自己消费了多少数据，即消费 Offset。Kafka Offset 是Consumer Position，与 Broker 和 Producer 都无关。每个 Consumer Group、每个 Topic 的每个Partition 都有各自的 Offset，如下图所示。
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662714910981-4d5ff978-5753-4757-939b-f95edcf93aa1.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_20%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   通常有如下几种 Kafka Offset 的管理方式：
  </p>
  <ul list="uea447af0">
   <li fid="u9f9356f5">
    Spark Checkpoint：在 Spark Streaming 执行Checkpoint 操作时，将 Kafka Offset 一并保存到 HDFS 中。这种方式的问题在于：当 Spark Streaming 应用升级或更新时，以及当Spark 本身更新时，Checkpoint 可能无法恢复。因而，不推荐采用这种方式。
   </li>
   <li fid="u9f9356f5">
    HBASE、Redis 等外部 NOSQL 数据库：这一方式可以支持大吞吐量的 Offset 更新，但它最大的问题在于：用户需要自行编写 HBASE 或 Redis 的读写程序，并且需要维护一个额外的组件。
   </li>
   <li fid="u9f9356f5">
    ZOOKEEPER：老版本的位移offset是提交到zookeeper中的，目录结构是 ：
    <code>
     /consumers/&lt;group.id&gt;/offsets/ &lt;topic&gt;/&lt;partitionId&gt;
    </code>
    <span class="lake-fontsize-12">
     ，但是由于 ZOOKEEPER 的写入能力并不会随着 ZOOKEEPER 节点数量的增加而扩大，因而，当存在频繁的 Offset 更新时，ZOOKEEPER 集群本身可能成为瓶颈。因而，不推荐采用这种方式。
    </span>
   </li>
   <li fid="u9f9356f5">
    Kafka自身的一个特殊Topic（__consumer_offsets）中：这种方式支持大吞吐量的Offset更新，又不需要手动编写 Offset 管理程序或者维护一套额外的集群，因而是迄今为止最为理想的一种实现方式。
   </li>
  </ul>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1665146199191-61f286f9-c379-4d73-9074-c03bdce0e651.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_31%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   另外几个与 Kafka Offset 管理相关的要点如下：
  </p>
  <ul list="u37d0842f">
   <li fid="ucc77aa46">
    Kafka 默认是定期帮你自动提交位移的（enable.auto.commit=True）。有时候，我们需要采用自己来管理位移提交，这时候需要设置 enable.auto.commit=False。
   </li>
   <li fid="ucc77aa46">
    属性 auto.offset.reset 值含义解释如下：
   </li>
  </ul>
  <ul data-lake-indent="1" list="u37d0842f">
   <li fid="u6b270395">
    earliest ：当各分区下有已提交的 Offset 时，从“提交的 Offset”开始消费；无提交的Offset 时，从头开始消费；
   </li>
   <li fid="u6b270395">
    latest ： 当各分区下有已提交的 Offse`t 时，从提交的 Offset 开始消费；无提交的 Offset时，消费新产生的该分区下的数据；
   </li>
   <li fid="u6b270395">
    none ： Topic 各分区都存在已提交的 Offset 时，从 Offset 后开始消费；只要有一个分区不存在已提交的 Offset，则抛出异常。
   </li>
  </ul>
  <pre><code class="language-xml" lang="xml">kafka-0.10.1.X版本之前: auto.offset.reset 的值为smallest,和,largest.(offest保存在zk中)；
kafka-0.10.1.X版本之后: auto.offset.reset 的值更改为:earliest,latest,和none (offest保存在kafka的一个特殊的topic名为:__consumer_offsets里面)；</code></pre>
  <h3>
   Kafka为什么同一个消费者组的消费者不能消费相同的分区？
  </h3>
  <p>
   问过的一些公司：网易
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka通过消费者组机制同时实现了发布/订阅模型和点对点模型。多个组的消费者消费同一个分区属于多订阅者的模式，自然没有什么问题；而在单个组内某分区只交由一个消费者处理的做法则属于点对点模式。其实这就是设计上的一种取舍，如果Kafka真的允许组内多个消费者消费同一个分区，也不是什么灾难性的事情，只是没什么意义，而且还会重复消费消息。
  </p>
  <p>
   通常情况下，我们还是希望一个组内所有消费者能够分担负载，让彼此做的事情没有交集，做一些重复性的劳动纯属浪费资源。就如同电话客服系统，每个客户来电只由一位客服人员响应。那么请问我就是想让多个人同时接可不可以？当然也可以了，技术上没什么困难，只是这么做没有任何意义罢了，既拉低了整体的处理能力，也造成了人力成本的浪费。
  </p>
  <p>
   总之，这种设计不是出于技术上的考量而更多还是看效率等非技术方面。
  </p>
  <h3>
   正在消费一条数据，Kafka挂了，重启以后，消费的offset是哪一个
  </h3>
  <p>
   问过的一些公司：招银网络
  </p>
  <p>
   参考答案：
  </p>
  <p>
   可以在Java API中设置
   <code>
    auto.offset.reset
   </code>
   <span class="lake-fontsize-12">
    的值（ConsumerConfig.AUTO_OFFSET_RESET_CONFIG）来进行指定。
   </span>
  </p>
  <p>
   先来看几个测试实验，结论在最后会给出
  </p>
  <p>
   <strong>
    auto.offset.reset值含义解释
   </strong>
  </p>
  <p>
   <code>
    earliest
   </code>
   <span class="lake-fontsize-12">
    ：当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
   </span>
  </p>
  <p>
   <code>
    latest
   </code>
   <span class="lake-fontsize-12">
    ：当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
   </span>
  </p>
  <p>
   <code>
    none
   </code>
   <span class="lake-fontsize-12">
    ：topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
   </span>
  </p>
  <p>
   案例测试如下：
  </p>
  <p>
   <strong>
    1、同分组测试
   </strong>
  </p>
  <p>
   <strong>
    1）测试一
   </strong>
  </p>
  <p>
   测试环境：Topic为lsztopic7，并生产30条信息。lsztopic7详情：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662715123968-088b7baf-d452-4b9d-a66b-9e6a4a892a63.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_33%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   创建组为“testtopi7”的consumer，将enable.auto.commit设置为False，不提交offset。依次更改auto.offset.reset的值。此时查看offset情况为：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662715139155-ecdbe01d-3a99-4440-9e02-d73fc7fbaa5b.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_45%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    测试结果：
   </strong>
  </p>
  <p>
   <strong>
    earliest
   </strong>
   <span class="lake-fontsize-12">
    ：客户端读取30条信息，且各分区的offset从0开始消费。
   </span>
  </p>
  <p>
   <strong>
    latest
   </strong>
   <span class="lake-fontsize-12">
    ：客户端读取0条信息。
   </span>
  </p>
  <p>
   <strong>
    none
   </strong>
   <span class="lake-fontsize-12">
    ：抛出NoOffsetForPartitionException异常。
   </span>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662715155188-0e0253d4-b37a-4239-9edc-2408dfa69f40.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_43%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    测试结论：
   </strong>
  </p>
  <pre><code class="language-xml" lang="xml">新建一个同组名的消费者时，auto.offset.reset值含义： 
earliest 每个分区是从头开始消费的。 
none 没有为消费者组找到先前的offset值时，抛出异常</code></pre>
  <p>
   <strong>
    2）测试二
   </strong>
  </p>
  <p>
   测试环境：测试场景一下latest时未接受到数据，保证该消费者在启动状态，使用生产者继续生产10条数据，总数据为40条。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662717545749-15ab3c6c-4bbc-45af-a77a-7e28d2135ecc.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_45%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    测试结果：
   </strong>
  </p>
  <p>
   <strong>
    latest
   </strong>
   <span class="lake-fontsize-12">
    ：客户端取到了后生产的10条数据
   </span>
  </p>
  <p>
   <strong>
    测试结论：
   </strong>
  </p>
  <pre><code class="language-xml" lang="xml">当创建一个新分组的消费者时，auto.offset.reset值为latest时，表示消费新的数据（从consumer创建开始，后生产的数据），之前产生的数据不消费。</code></pre>
  <p>
   <strong>
    3）测试三
   </strong>
  </p>
  <p>
   测试环境：在测试环境二，总数为40条，无消费情况下，消费一批数据。运行消费者消费程序后，取到5条数据。 即，总数为40条，已消费5条，剩余35条。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662717579159-6cd1dd7e-d43d-41bf-b9c0-9e1752daed03.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_45%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    测试结果：
   </strong>
  </p>
  <p>
   <strong>
    earliest
   </strong>
   <span class="lake-fontsize-12">
    ：消费35条数据，即将剩余的全部数据消费完。
   </span>
  </p>
  <p>
   <strong>
    latest
   </strong>
   <span class="lake-fontsize-12">
    ：
   </span>
  </p>
  <pre><code class="language-xml" lang="xml">消费9条数据，都是分区3的值。 
offset:0 partition:3 
offset:1 partition:3 
offset:2 partition:3 
offset:3 partition:3 
offset:4 partition:3 
offset:5 partition:3 
offset:6 partition:3 
offset:7 partition:3 
offset:8 partition:3</code></pre>
  <p>
   <strong>
    none
   </strong>
   <span class="lake-fontsize-12">
    ：抛出NoOffsetForPartitionException异常
   </span>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662717634159-6e69ae0b-8d32-470b-ad56-77dbadb09449.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_43%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    测试结论：
   </strong>
  </p>
  <pre><code class="language-plain" lang="plain">earliest：当分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费。 
latest：当分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据。 
none：当该topic下所有分区中存在未提交的offset时，抛出异常。</code></pre>
  <p>
   <strong>
    4）测试四
   </strong>
  </p>
  <p>
   测试环境：再测试三的基础上，将数据消费完，再生产10条数据，确保每个分区上都有已提交的offset。此时，总数为50，已消费40，剩余10条
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662717651840-1fdccccb-072e-4476-99e2-ddfc1eeadf94.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_45%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    测试结果：
   </strong>
  </p>
  <p>
   <strong>
    none
   </strong>
   <span class="lake-fontsize-12">
    ：
   </span>
  </p>
  <pre><code class="language-xml" lang="xml">消费10条信息，且各分区都是从offset开始消费 
offset:9 partition:3 
offset:10 partition:3 
offset:11 partition:3 
offset:15 partition:0 
offset:16 partition:0 
offset:17 partition:0 
offset:18 partition:0 
offset:19 partition:0 
offset:20 partition:0 
offset:5 partition:2</code></pre>
  <p>
   <strong>
    测试结论：
   </strong>
  </p>
  <pre><code class="language-xml" lang="xml">值为none时，topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常。</code></pre>
  <p>
   <strong>
    2、不同分组下测试
   </strong>
  </p>
  <p>
   <strong>
    1）测试五
   </strong>
  </p>
  <p>
   测试环境：在测试四环境的基础上：总数为50，已消费40，剩余10条，创建不同组的消费者，组名为testother7。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662717675659-9e017c62-b5c0-4e4e-ab9e-c2b262079258.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_45%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   测试结果：
  </p>
  <p>
   <strong>
    earliest
   </strong>
   <span class="lake-fontsize-12">
    ：消费50条数据，即将全部数据消费完。
   </span>
  </p>
  <p>
   <strong>
    latest
   </strong>
   <span class="lake-fontsize-12">
    ：消费0条数据。
   </span>
  </p>
  <p>
   <strong>
    none
   </strong>
   <span class="lake-fontsize-12">
    ：抛出异常
   </span>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662717692557-c829e8f5-a2bb-4ff6-885c-6f5c88571dd0.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_44%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   测试结论：
  </p>
  <pre><code class="language-xml" lang="xml">组与组间的消费者是没有关系的。 
topic中已有分组消费数据，新建其他分组ID的消费者时，之前分组提交的offset对新建的分组消费不起作用。</code></pre>
  <p>
   <strong>
    3、总结
   </strong>
  </p>
  <p>
   从上面的实验来看，基本上对auto.offset.reset不同的值进行了测试
  </p>
  <p>
   根据下面的实验数据来进行分析
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662717715853-fa9b4692-0513-46d3-a16a-ba4483b1bf9b.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_45%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   CURRENT-OFFSET：表示消费者消费了数据之后提交的offset，即消费者消费了的数据的偏移量。如果为unknown，则表示消费者未提交过offset。
  </p>
  <p>
   LOG-END-OFFSET：表示的是该分区的HW。
  </p>
  <p>
   LAG：表示延迟滞后，也就是生产者已经写到kafka集群了，然后有还没有被消费的数量，是logSize-currentOffset，logsize指的是消息总数。
  </p>
  <p>
   <strong>
    得出以下的一些结论：
   </strong>
  </p>
  <p>
   1）如果
   <strong>
    CURRENT-OFFSET不是为unknown
   </strong>
   <span class="lake-fontsize-12">
    （消费者以前消费过数据，提交过offset），重启消费者时earliest、latest、none都是会从CURRENT-OFFSET一直消费到LOG-END-OFFSET。也就是不会更新offset。
   </span>
  </p>
  <p>
   2）如果
   <strong>
    CURRENT-OFFSET为unknown
   </strong>
   <span class="lake-fontsize-12">
    ，重启消费者时earliest、latest、none才会展现出他们各自的不同：
   </span>
  </p>
  <ul list="u19a484bc">
   <li fid="uf1d2fdbe">
    <strong>
     earliest
    </strong>
    <span class="lake-fontsize-12">
     ：会从该分区当前最开始的offset消息开始消费(即从头消费)，如果最开始的消息offset是0，那么消费者的offset就会被更新为0.
    </span>
   </li>
   <li fid="uf1d2fdbe">
    <strong>
     latest
    </strong>
    <span class="lake-fontsize-12">
     ：只消费当前消费者启动完成后生产者新生产的数据。旧数据不会再消费。offset被重置为分区的HW。
    </span>
   </li>
   <li fid="uf1d2fdbe">
    <strong>
     none
    </strong>
    <span class="lake-fontsize-12">
     ：启动消费者时，该消费者所消费的主题的分区没有被消费过，就会抛异常。(一般新建主题或者用新的消费者组是使用这个就会抛异常。。宕机重启的话，使用这个就没问题。。这个的作用是什么？我猜测应该是用于在重启消费者时检查该消费者所消费的主题以及所属的消费者组的名称是否写错了，导致该消费者没有消费原来主题分区)
    </span>
   </li>
  </ul>
  <p>
   这里再强调一遍：
  </p>
  <p>
   在Java API中设置auto.offset.reset值（ConsumerConfig.AUTO_OFFSET_RESET_CONFIG）。
  </p>
  <h3>
   Kafka producer的写入数据过程？
  </h3>
  <p>
   问过的一些公司：流利说
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka的Producer发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量——RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662717890469-b9a5a664-269d-4adb-a2c3-cb968d32efc8.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_16%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   相关参数
  </p>
  <p>
   batch.size：只有数据积累到batch.size之后，sender才会发送数据。
  </p>
  <p>
   linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。
  </p>
  <h3>
   Kafka的ack机制，解决了什么问题？
  </h3>
  <p>
   问过的一些公司：端点数据(2021.07)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   ack机制保证了Producer在发送的数据能可靠的到达指定的Topic，为Producer提供了消息确认机制。 生产者往Broker的Topic中发送消息时，
   <strong>
    可以通过配置来决定有几个副本收到这条消息才算消息发送成功
   </strong>
   <span class="lake-fontsize-12">
    。可以在定义Producer时通过
   </span>
   <strong>
    acks
   </strong>
   <span class="lake-fontsize-12">
    参数指定，
   </span>
  </p>
  <p>
   这个参数支持以下三种值：
  </p>
  <p>
   <strong>
    ack=0
   </strong>
  </p>
  <p>
   意味着producer不等待broker同步完成的确认，继续发送下一条(批)信息。提供了最低的延迟。但是最弱的持久性，当服务器发生故障时，就很可能发生数据丢失。例如leader已经死亡，producer不知情，还会继续发送消息broker接收不到数据就会数据丢失。
  </p>
  <p>
   <strong>
    ack=1
   </strong>
  </p>
  <p>
   意味着producer要等待leader成功收到数据并得到确认，才发送下一条message。此选项提供了较好的持久性较低的延迟性。Partition的Leader死亡，follwer尚未复制，数据就会丢失。
  </p>
  <p>
   <strong>
    ack=-1
   </strong>
  </p>
  <p>
   意味着producer得到follwer确认，才发送下一条数据，持久性最好，延时性最差。
  </p>
  <h3>
   Kafka是如何进行数据备份的？
  </h3>
  <p>
   问过的一些公司：祖龙娱乐
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka的备份的单元是partition，也就是每个partition都都会有leader partiton和follow partiton。其中leader partition是用来进行和producer进行写交互，follow从leader副本进行拉数据进行同步，从而保证数据的冗余，防止数据丢失的目的。
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662719534048-d33b49e7-5691-4c9c-be6d-d365ddcde600.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_23%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <h3>
   Kafka里面存的数据格式是什么样的？
  </h3>
  <p>
   问过的一些公司：阿里云，祖龙娱乐
  </p>
  <p>
   参考答案：
  </p>
  <p>
   1、先介绍几个重要概念
  </p>
  <ul list="u11961626">
   <li fid="ue0de5822">
    <strong>
     Broker
    </strong>
    ：消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群；
   </li>
   <li fid="ue0de5822">
    <strong>
     Topic
    </strong>
    <span class="lake-fontsize-12">
     ：一类消息，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发；
    </span>
   </li>
   <li fid="ue0de5822">
    <strong>
     Partition
    </strong>
    <span class="lake-fontsize-12">
     ：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队；
    </span>
   </li>
   <li fid="ue0de5822">
    <strong>
     Segment
    </strong>
    <span class="lake-fontsize-12">
     ：每个partition又由多个segment file组成；
    </span>
   </li>
   <li fid="ue0de5822">
    <strong>
     offset
    </strong>
    <span class="lake-fontsize-12">
     ：每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset，用于partition唯一标识一条消息；
    </span>
   </li>
   <li fid="ue0de5822">
    <strong>
     message
    </strong>
    <span class="lake-fontsize-12">
     ：这个算是kafka文件中最小的存储单位，即是 a commit log。
    </span>
   </li>
  </ul>
  <p>
   kafka的message是以topic为基本单位，不同topic之间是相互独立的。每个topic又可分为几个不同的partition，每个partition存储一部的分message。topic与partition的关系如下：
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662719580278-f1da7399-2b18-40e0-919c-876f9e9aed00.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_11%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   其中，partition是以文件夹的形式存储在具体Broker本机上。
  </p>
  <p>
   2、partition中的数据文件
  </p>
  <p>
   1）segment中的文件
  </p>
  <p>
   对于一个partition（在Broker中以文件夹的形式存在），里面又有很多大小相等的segment数据文件（这个文件的具体大小可以在
   <code>
    config/server.properties
   </code>
   <span class="lake-fontsize-12">
    中进行设置），这种特性可以方便old segment file的快速删除。
   </span>
  </p>
  <p>
   partition中的segment file的组成：
  </p>
  <ul list="udcd0d681">
   <li fid="uc9057cc6">
    segment file组成：由2部分组成，分别为index file和data file，这两个文件是一一对应的，后缀”.index”和”.log”分别表示索引文件和数据文件；
   </li>
   <li fid="uc9057cc6">
    segment file命名规则：partition的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset,ofsset的数值最大为64位（long类型），20位数字字符长度，没有数字用0填充。如下图所示：
   </li>
  </ul>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662719608609-962359c1-7a0f-4aa5-93d0-352d55b9cdf6.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_19%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   关于segment file中index与data file对应关系图，这里我们选用网上的一个图片，如下所示：
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662719627293-4e660c99-94c7-44a5-a942-e3959deec818.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_16%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   segment的索引文件中存储着大量的元数据，数据文件中存储着大量消息，索引文件中的元数据指向对应数据文件中的message的物理偏移地址。以索引文件中的
   <code>
    3，497
   </code>
   <span class="lake-fontsize-12">
    为例，在数据文件中表示第3个message（在全局partition表示第368772个message），以及该消息的物理偏移地址为497。
   </span>
  </p>
  <p>
   注：Partition中的每条message由offset来表示它在这个partition中的偏移量，这个offset并不是该Message在partition中实际存储位置，而是逻辑上的一个值（如上面的3），但它却唯一确定了partition中的一条Message（可以认为offset是partition中Message的id）。
  </p>
  <p>
   2）message文件
  </p>
  <p>
   message中的物理结构为：
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662719653896-1a3d6802-fb83-4cb2-b7fb-ceb0ad4a37b5.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_9%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   参数说明：
  </p>
  <table class="lake-table" margin="True" style="width: 712px">
   <colgroup>
    <col width="199"/>
    <col width="513"/>
   </colgroup>
   <tbody>
    <tr>
     <td>
      <p style="text-align: left">
       关键字
      </p>
     </td>
     <td>
      <p style="text-align: left">
       解释说明
      </p>
     </td>
    </tr>
    <tr style="height: 94px">
     <td>
      <p style="text-align: left">
       8 byte offset
      </p>
     </td>
     <td>
      <p style="text-align: left">
       在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       4 byte message size
      </p>
     </td>
     <td>
      <p style="text-align: left">
       message大小
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       4 byte CRC32
      </p>
     </td>
     <td>
      <p style="text-align: left">
       用crc32校验message
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       1 byte “magic”
      </p>
     </td>
     <td>
      <p style="text-align: left">
       表示本次发布Kafka服务程序协议版本号
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       1 byte “attributes”
      </p>
     </td>
     <td>
      <p style="text-align: left">
       表示为独立版本、或标识压缩类型、或编码类型
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       4 byte key length
      </p>
     </td>
     <td>
      <p style="text-align: left">
       表示key的长度,当key为-1时，K byte key字段不填
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       K byte key
      </p>
     </td>
     <td>
      <p style="text-align: left">
       可选
      </p>
     </td>
    </tr>
    <tr>
     <td>
      <p style="text-align: left">
       value bytes payload
      </p>
     </td>
     <td>
      <p style="text-align: left">
       表示实际消息数据
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <h3>
   Kafka是如何清理过期文件的？
  </h3>
  <p>
   问过的一些公司：祖龙娱乐
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka将数据持久化到了硬盘上，我们可以配置一定的策略对数据进行清理，清理的策略有两个：
   <strong>
    删除
   </strong>
   <span class="lake-fontsize-12">
    和
   </span>
   <strong>
    压缩
   </strong>
   <span class="lake-fontsize-12">
    。
   </span>
  </p>
  <p>
   <strong>
    数据清理方式一：删除
   </strong>
  </p>
  <p>
   启用删除策略：
  </p>
  <pre><code class="language-java" lang="java">log.cleanup.policy=delete</code></pre>
  <p>
   直接删除，删除后的消息不可恢复。可配置以下两个策略：
  </p>
  <p>
   清理超过指定时间清理：
  </p>
  <pre><code class="language-java" lang="java">log.retention.hours=16</code></pre>
  <p>
   超过指定大小后，删除旧的消息：
  </p>
  <pre><code class="language-java" lang="java">log.retention.bytes=1073741824</code></pre>
  <p>
   为了避免在删除时阻塞读操作，采用了copy-on-write形式的实现，删除操作进行时，读取操作的二分查找功能实际是在一个静态的快照副本上进行的，这类似于Java的CopyOnWriteArrayList。
  </p>
  <p>
   <strong>
    数据清理方式二：压缩
   </strong>
  </p>
  <p>
   将数据压缩，只保留每个key最后一个版本的数据。
  </p>
  <p>
   首先在broker的配置中设置
   <code>
    log.cleaner.enable=True
   </code>
   <span class="lake-fontsize-12">
    启用cleaner，这个默认是关闭的。
   </span>
  </p>
  <p>
   在topic的配置中设置
   <code>
    log.cleanup.policy=compact
   </code>
   <span class="lake-fontsize-12">
    启用压缩策略。
   </span>
  </p>
  <p>
   压缩策略的细节
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662719698214-560287ed-eb07-46ca-9c16-b5e7b782b9ce.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_17%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   如上图，在整个数据流中，每个Key都有可能出现多次，压缩时将根据Key将消息聚合，只保留最后一次出现时的数据。这样，无论什么时候消费消息，都能拿到每个Key的最新版本的数据。压缩后的offset可能是不连续的，比如上图中没有5和7，因为这些offset的消息被merge了，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，比如，当试图获取offset为5的消息时，实际上会拿到offset为6的消息，并从这个位置开始消费。
  </p>
  <p>
   这种策略只适合特俗场景，比如消息的key是用户ID，消息体是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。
  </p>
  <p>
   压缩策略支持删除，当某个Key的最新版本的消息没有内容时，这个Key将被删除，这也符合以上逻辑。
  </p>
  <h3>
   Kafka如何保证数据的Exactly Once？
  </h3>
  <p>
   问过的一些公司：360，美团，360社招，中信信用卡中心，Shopee(2021.08)
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka可以通过两种机制来确保消息消费的精确一次：
  </p>
  <ul list="ue4094b20">
   <li fid="u5552fc35">
    幂等性（Idempotence）
   </li>
   <li fid="u5552fc35">
    事务（Transaction）
   </li>
  </ul>
  <p>
   <strong>
    1、幂等性：每个分区中精确一次且有序
   </strong>
  </p>
  <p>
   将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once语义。
  </p>
  <p>
   At Least Once可以保证数据不丢失，但是不能保证数据不重复；相对的，At Least Once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义。在0.11版本以前的Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。
  </p>
  <p>
   0.11版本的Kafka，引入了一项重大特性：
   <strong>
    幂等性
   </strong>
   <span class="lake-fontsize-12">
    。所谓的幂等性就是
   </span>
   <strong>
    指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条
   </strong>
   <span class="lake-fontsize-12">
    。
   </span>
   <strong>
    幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义
   </strong>
   <span class="lake-fontsize-12">
    。即：
   </span>
  </p>
  <p>
   At Least Once + 幂等性 = Exactly Once
  </p>
  <p>
   要启用幂等性，只需要将Producer的参数中enable.idompotence设置为True即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。
  </p>
  <p>
   <strong>
    2、事务：跨分区原子写入
   </strong>
  </p>
  <p>
   Kafka现在通过新的事务API支持跨分区原子写入。这将允许一个生产者发送一批到不同分区的消息，这些消息要么全部对任何一个消费者可见，要么对任何一个消费者都不可见。这个特性也允许你在一个事务中处理消费数据和提交消费偏移量，从而实现端到端的精确一次语义。下面是的代码片段演示了事务API的使用：
  </p>
  <pre><code class="language-java" lang="java">producer.initTransactions();
try {
  producer.beginTransaction();
  producer.send(record1);
  producer.send(record2);
  producer.commitTransaction();
} catch(ProducerFencedException e) {
  producer.close();
} catch(KafkaException e) {
  producer.abortTransaction();
}</code></pre>
  <p>
   上面的代码片段演示了你可以如何使用新生产者API来原子性地发送消息到topic的多个partition。值得注意的是，一个Kafka topic的分区中的消息，可以有些是在事务中，有些不在事务中。
  </p>
  <p>
   因此在消费者方面，你有两种选择来读取事务性消息，通过隔离等级“isolation.level”消费者配置表示：
  </p>
  <p>
   <code>
    read_commited
   </code>
   <span class="lake-fontsize-12">
    ：除了读取不属于事务的消息之外，还可以读取事务提交后的消息。
   </span>
  </p>
  <p>
   <code>
    read_uncommited
   </code>
   <span class="lake-fontsize-12">
    ：按照偏移位置读取所有消息，而不用等事务提交。这个选项类似Kafka消费者的当前语义。
   </span>
  </p>
  <p>
   为了使用事务，需要配置消费者使用正确的隔离等级，使用新版生产者，并且将生产者的“transactional.id”配置项设置为某个唯一ID。 需要此唯一ID来提供跨越应用程序重新启动的事务状态的连续性。
  </p>
  <h3>
   Kafka消费者怎么保证Exactly Once
  </h3>
  <p>
   问过的一些公司：快手 x 2
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Consumer端丢失数据主要体现在：拉取了消息，并提交了消费位移，但是在消息处理结束之前突然发生了宕机等故障。消费者重生后，会从之前已提交的位移的下一个位置重新开始消费，之前未处理完成的消息不会再次处理，即相当于消费者丢失了消息。
  </p>
  <p>
   解决Consumer端丢失消息的方法也很简单：将位移提交的时机改为消息处理完成后，确认消费完成了一批消息再提交相应的位移。这样做，即使处理消息的过程中发生了异常，由于没有提交位移，下次消费时还会从上次的位移处重新拉取消息，不会发生消息丢失的情况。
  </p>
  <p>
   具体的实现方法为，Consumer在消费消息时，关闭自动提交位移，由应用程序手动提交位移。
  </p>
  <h3>
   Kafka中的数据能彻底删除吗？
  </h3>
  <p>
   问过的一些公司：腾讯
  </p>
  <p>
   参考答案：
  </p>
  <p>
   如果用kafka-topics.sh的delete命令删除topic，会有两种情况：
  </p>
  <p>
   1）如果当前topic没有使用过即没有传输过信息：可以彻底删除。
  </p>
  <p>
   2）如果当前topic有使用过即有过传输过信息：并没有真正删除topic只是把这个topic标记为删除（marked for deletion）。
  </p>
  <p>
   要彻底把情况2中的topic删除必须把kafka中与当前topic相关的数据目录和zookeeper与当前topic相关的路径一并删除。
  </p>
  <h3>
   Kafka中的数据能彻底删除吗？
  </h3>
  <p>
   问过的一些公司：腾讯
  </p>
  <p>
   参考答案：
  </p>
  <p>
   如果用kafka-topics.sh的delete命令删除topic，会有两种情况：
  </p>
  <p>
   1）如果当前topic没有使用过即没有传输过信息：可以彻底删除。
  </p>
  <p>
   2）如果当前topic有使用过即有过传输过信息：并没有真正删除topic只是把这个topic标记为删除（marked for deletion）。
  </p>
  <p>
   要彻底把情况2中的topic删除必须把kafka中与当前topic相关的数据目录和zookeeper与当前topic相关的路径一并删除。
  </p>
  <h3>
   Kafka复制机制？
  </h3>
  <p>
   问过的一些公司：腾讯
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka 主题中的每个分区都有一个预写日志（write-ahead log），我们写入 Kafka 的消息就存储在这里面。这里面的每条消息都有一个唯一的偏移量，用于标识它在当前分区日志中的位置。如下图所示：
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/jpeg/12867669/1662720796969-3d9582be-03a9-426a-bf34-27d60b0e7d53.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_14%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   Kafka 中的每个主题分区都被复制了 n 次，其中的 n 是主题的复制因子（replication factor）。这允许 Kafka 在集群服务器发生故障时自动切换到这些副本，以便在出现故障时消息仍然可用。
   <strong>
    Kafka 的复制是以分区为粒度的，分区的预写日志被复制到 n 个服务器
   </strong>
   <span class="lake-fontsize-12">
    。 在 n 个副本中，一个副本作为 leader，其他副本成为 followers。顾名思义，producer 只能往 leader 分区上写数据（读也只能从 leader 分区上进行），followers 只按顺序从 leader 上复制日志。
   </span>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/jpeg/12867669/1662720817434-b026e344-f45e-400a-a34a-8f83b6e1c38c.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_23%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   日志复制算法（log replication algorithm）必须提供的基本保证是，如果它告诉客户端消息已被提交，而当前 leader 出现故障，新选出的 leader 也必须具有该消息。在出现故障时，Kafka 会从挂掉 leader 的 ISR 里面选择一个 follower 作为这个分区新的 leader ；换句话说，是因为这个 follower 是跟上 leader 写进度的。
  </p>
  <p>
   <strong>
    每个分区的 leader 会维护一个 in-sync replica（同步副本列表，又称 ISR）。当 producer 往 broker 发送消息，消息先写入到对应 leader 分区上，然后复制到这个分区的所有副本中
   </strong>
   <span class="lake-fontsize-12">
    。只有将消息成功复制到所有同步副本（ISR）后，这条消息才算被提交。由于消息复制延迟受到最慢同步副本的限制，因此快速检测慢副本并将其从 ISR 中删除非常重要。
   </span>
  </p>
  <h3>
   Kafka在哪些地方会有选举过程，使用什么工具支持选举？
  </h3>
  <p>
   问过的一些公司：字节
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka中的选举大致可以分为三大类：
   <strong>
    控制器的选举（先到先得）
   </strong>
   <span class="lake-fontsize-12">
    、
   </span>
   <strong>
    分区leader的选举（ISR）
   </strong>
   <span class="lake-fontsize-12">
    以及
   </span>
   <strong>
    消费者相关的选举
   </strong>
  </p>
  <p>
   <strong>
    1、控制器的选举
   </strong>
  </p>
  <p>
   在Kafka集群中会有一个或多个broker，其中有一个broker会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态等工作。比如当某个分区的leader副本出现故障时，由控制器负责为该分区选举新的leader副本。再比如当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有broker更新其元数据信息。
  </p>
  <p>
   Kafka Controller的选举是依赖Zookeeper来实现的，在Kafka集群中哪个broker能够成功创建/controller这个临时（EPHEMERAL）节点他就可以成为Kafka Controller。
  </p>
  <p>
   <strong>
    2、分区Leader的选举
   </strong>
  </p>
  <p>
   分区leader副本的选举由Kafka Controller负责具体实施。当创建分区（创建主题或增加分区都有创建分区的动作）或分区上线（比如分区中原先的leader副本下线，此时分区需要选举一个新的leader上线来对外提供服务）的时候都需要执行leader的选举动作。
  </p>
  <p>
   基本思路是按照AR（Assigned Repllicas：分区中的所有副本）集合中副本的顺序查找第一个存活的副本，并且这个副本在ISR集合中。一个分区的AR集合在分配的时候就被指定，并且只要不发生重分配的情况，集合内部副本的顺序是保持不变的，而分区的ISR集合中副本的顺序可能会改变。注意这里是根据AR的顺序而不是ISR的顺序进行选举的。这个说起来比较抽象，有兴趣的读者可以手动关闭/开启某个集群中的broker来观察一下具体的变化。
  </p>
  <p>
   还有一些情况也会发生分区leader的选举，比如当分区进行重分配（reassign）的时候也需要执行leader的选举动作。这个思路比较简单：从重分配的AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中。
  </p>
  <p>
   再比如当发生优先副本（preferred replica partition leader election）的选举时，直接将优先副本设置为leader即可，AR集合中的第一个副本即为优先副本。
  </p>
  <p>
   还有一种情况就是当某节点被优雅地关闭（也就是执行ControlledShutdown）时，位于这个节点上的leader副本都会下线，所以与此对应的分区需要执行leader的选举。这里的具体思路为：从AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中，与此同时还要确保这个副本不处于正在被关闭的节点上。
  </p>
  <p>
   <strong>
    3、消费者相关的选举
   </strong>
  </p>
  <p>
   组协调器GroupCoordinator需要为消费组内的消费者选举出一个消费组的leader，这个选举的算法也很简单，分两种情况分析。如果消费组内还没有leader，那么第一个加入消费组的消费者即为消费组的leader。如果某一时刻leader消费者由于某些原因退出了消费组，那么会重新选举一个新的leader，这个重新选举leader的过程又更“随意”了，相关代码如下：
  </p>
  <pre><code class="language-scala" lang="scala">private val members = new mutable.HashMap[String, MemberMetadata]
var leaderId = members.keys.head</code></pre>
  <p>
   解释一下这2行代码：在GroupCoordinator中消费者的信息是以HashMap的形式存储的，其中key为消费者的member_id，而value是消费者相关的元数据信息。leaderId表示leader消费者的member_id，它的取值为HashMap中的第一个键值对的key，这种选举的方式基本上和随机无异。总体上来说，消费组的leader选举过程是很随意的。
  </p>
  <p>
   用过Kafka的对partition.assignment.strategy（取值为RangeAssignor、RoundRobinAssignor、StickyAssignor等）这个参数都并不陌生。每个消费者都可以设置自己的分区分配策略，对消费组而言需要从各个消费者呈报上来的各个分配策略中选举一个彼此都“信服”的策略来进行整体上的分区分配。这个分区分配的选举并非由leader消费者决定，而是根据消费组内的各个消费者投票来决定的。
  </p>
  <h3>
   Kafka搭建过程要配置什么参数？
  </h3>
  <p>
   问过的一些公司：Bigo
  </p>
  <p>
   参考答案：
  </p>
  <p>
   Kafka的重要配置是在server.propertis文件中，具体如下：
  </p>
  <pre><code class="language-properties" lang="properties">#broker的全局唯一编号，不能重复
broker.id=0
#删除topic功能使能
delete.topic.enable=True
#处理网络请求的线程数量
num.network.threads=3
#用来处理磁盘IO的现成数量
num.io.threads=8
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600
#kafka运行日志存放的路径
log.dirs=/opt/module/kafka/logs
#topic在当前broker上的分区个数
num.partitions=1
#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1
#segment文件保留的最长时间，超时将被删除
log.retention.hours=168
#配置连接Zookeeper集群地址
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</code></pre>
  <p>
   基本上熟悉上面的一些参数就可以了，下面放一些更详细的，有需要可以看看。
  </p>
  <p>
   <strong>
    1、Kafka配置参数
   </strong>
  </p>
  <p>
   broker.id：broker的id，id是唯一的非负整数，集群的broker.id不能重复。
  </p>
  <p>
   log.dirs：kafka存放数据的路径。可以是多个，多个使用逗号分隔即可。
  </p>
  <p>
   port：server接受客户端连接的端口，默认6667
  </p>
  <p>
   zookeeper.connect：zookeeper集群连接地址。
  </p>
  <p>
   格式如：zookeeper.connect=server01:2181,server02:2181,server03:2181。
  </p>
  <p>
   如果需要指定zookeeper集群的路径位置，可以：zookeeper.connect=server01:2181,server02:2181,server03:2181/kafka/cluster。这样设置后，在启动kafka集群前，需要在zookeeper集群创建这个路径/kafka/cluster。
  </p>
  <p>
   message.max.bytes：server可以接受的消息最大尺寸。默认1000000。
  </p>
  <p>
   重要的是，consumer和producer有关这个属性的设置必须同步，否则producer发布的消息对consumer来说太大。
  </p>
  <p>
   num.network.threads：server用来处理网络请求的线程数，默认3。
  </p>
  <p>
   num.io.threads：server用来处理请求的I/O线程数。这个线程数至少等于磁盘的个数。
  </p>
  <p>
   background.threads：用于后台处理的线程数。例如文件的删除。默认4。
  </p>
  <p>
   queued.max.requests：在网络线程停止读取新请求之前，可以排队等待I/O线程处理的最大请求个数。默认500。
  </p>
  <p>
   host.name：broker的hostname
  </p>
  <p>
   如果hostname已经设置的话，broker将只会绑定到这个地址上；如果没有设置，它将绑定到所有接口，并发布一份到ZK
  </p>
  <p>
   advertised.host.name：如果设置，则就作为broker 的hostname发往producer、consumers以及其他brokers
  </p>
  <p>
   advertised.port：此端口将给与producers、consumers、以及其他brokers，它会在建立连接时用到； 它仅在实际端口和server需要绑定的端口不一样时才需要设置。
  </p>
  <p>
   socket.send.buffer.bytes：SO_SNDBUFF 缓存大小，server进行socket 连接所用，默认100*1024。
  </p>
  <p>
   socket.receive.buffer.bytes：SO_RCVBUFF缓存大小，server进行socket连接时所用。默认100 * 1024。
  </p>
  <p>
   socket.request.max.bytes：server允许的最大请求尺寸；这将避免server溢出，它应该小于Java heap size。
  </p>
  <p>
   num.partitions：如果创建topic时没有给出划分partitions个数，这个数字将是topic下partitions数目的默认数值。默认1。
  </p>
  <p>
   log.segment.bytes：topic partition的日志存放在某个目录下诸多文件中，这些文件将partition的日志切分成一段一段的；这个属性就是每个文件的最大尺寸；当尺寸达到这个数值时，就会创建新文件。此设置可以由每个topic基础设置时进行覆盖。默认1014
   <em>
    1024
   </em>
   <span class="lake-fontsize-12">
    1024
   </span>
  </p>
  <p>
   log.roll.hours：即使文件没有到达log.segment.bytes，只要文件创建时间到达此属性，就会创建新文件。这个设置也可以有topic层面的设置进行覆盖。默认24*7
  </p>
  <p>
   log.cleanup.policy：log清除策略。默认delete。
  </p>
  <p>
   log.retention.minutes和log.retention.hours：每个日志文件删除之前保存的时间。默认数据保存时间对所有topic都一样。
  </p>
  <p>
   log.retention.minutes 和 log.retention.bytes 都是用来设置删除日志文件的，无论哪个属性已经溢出。
  </p>
  <p>
   这个属性设置可以在topic基本设置时进行覆盖。
  </p>
  <p>
   log.retention.bytes：每个topic下每个partition保存数据的总量。
  </p>
  <p>
   注意，这是每个partitions的上限，因此这个数值乘以partitions的个数就是每个topic保存的数据总量。如果log.retention.hours和log.retention.bytes都设置了，则超过了任何一个限制都会造成删除一个段文件。
  </p>
  <p>
   注意，这项设置可以由每个topic设置时进行覆盖。
  </p>
  <p>
   log.retention.check.interval.ms：检查日志分段文件的间隔时间，以确定是否文件属性是否到达删除要求。默认5min。
  </p>
  <p>
   log.cleaner.enable：当这个属性设置为False时，一旦日志的保存时间或者大小达到上限时，就会被删除；如果设置为True，则当保存属性达到上限时，就会进行log compaction。默认False。
  </p>
  <p>
   log.cleaner.threads：进行日志压缩的线程数。默认1。
  </p>
  <p>
   log.cleaner.io.max.bytes.per.second：进行log compaction时，log cleaner可以拥有的最大I/O数目。这项设置限制了cleaner，以避免干扰活动的请求服务。
  </p>
  <p>
   log.cleaner.io.buffer.size：log cleaner清除过程中针对日志进行索引化以及精简化所用到的缓存大小。最好设置大点，以提供充足的内存。默认500
   <em>
    1024
   </em>
   <span class="lake-fontsize-12">
    1024。
   </span>
  </p>
  <p>
   log.cleaner.io.buffer.load.factor：进行log cleaning时所需要的I/O chunk尺寸。你不需要更改这项设置。默认512*1024。
  </p>
  <p>
   log.cleaner.io.buffer.load.factor：log cleaning中所使用的hash表的负载因子；你不需要更改这个选项。默认0.9
  </p>
  <p>
   log.cleaner.backoff.ms：进行日志是否清理检查的时间间隔，默认15000。
  </p>
  <p>
   log.cleaner.min.cleanable.ratio：这项配置控制log compactor试图清理日志的频率（假定log compaction是打开的）。
  </p>
  <p>
   默认避免清理压缩超过50%的日志。这个比率绑定了备份日志所消耗的最大空间（50%的日志备份时压缩率为50%）。更高的比率则意味着浪费消耗更少，也就可以更有效的清理更多的空间。这项设置在每个topic设置中可以覆盖。
  </p>
  <p>
   log.cleaner.delete.retention.ms：保存时间；保存压缩日志的最长时间；也是客户端消费消息的最长时间，与log.retention.minutes的区别在于一个控制未压缩数据，一个控制压缩后的数据；会被topic创建时的指定时间覆盖。
  </p>
  <p>
   log.index.size.max.bytes：每个log segment的最大尺寸。注意，如果log尺寸达到这个数值，即使尺寸没有超过log.segment.bytes限制，也需要产生新的log segment。默认10
   <em>
    1024
   </em>
   <span class="lake-fontsize-12">
    1024。
   </span>
  </p>
  <p>
   log.index.interval.bytes：当执行一次fetch后，需要一定的空间扫描最近的offset，设置的越大越好，一般使用默认值就可以。默认4096。
  </p>
  <p>
   log.flush.interval.messages：log文件“sync”到磁盘之前累积的消息条数。
  </p>
  <p>
   因为磁盘IO操作是一个慢操作，但又是一个“数据可靠性”的必要手段，所以检查是否需要固化到硬盘的时间间隔。需要在“数据可靠性”与“性能”之间做必要的权衡，如果此值过大，将会导致每次“发sync”的时间过长（IO阻塞），如果此值过小，将会导致“fsync”的时间较长（IO阻塞），导致”发sync“的次数较多，这也就意味着整体的client请求有一定的延迟，物理server故障，将会导致没有fsync的消息丢失。
  </p>
  <p>
   log.flush.scheduler.interval.ms：检查是否需要fsync的时间间隔。默认Long.MaxValue
  </p>
  <p>
   log.flush.interval.ms：仅仅通过interval来控制消息的磁盘写入时机，是不足的，这个数用来控制”fsync“的时间间隔，如果消息量始终没有达到固化到磁盘的消息数，但是离上次磁盘同步的时间间隔达到阈值，也将触发磁盘同步。
  </p>
  <p>
   log.delete.delay.ms：文件在索引中清除后的保留时间，一般不需要修改。默认60000。
  </p>
  <p>
   auto.create.topics.enable：是否允许自动创建topic。如果是True，则produce或者fetch 不存在的topic时，会自动创建这个topic。否则需要使用命令行创建topic。默认True。
  </p>
  <p>
   controller.socket.timeout.ms：partition管理控制器进行备份时，socket的超时时间。默认30000。
  </p>
  <p>
   controller.message.queue.size：controller-to-broker-channles的buffer尺寸，默认Int.MaxValue。
  </p>
  <p>
   default.replication.factor：默认备份份数，仅指自动创建的topics。默认1。
  </p>
  <p>
   replica.lag.time.max.ms：如果一个follower在这个时间内没有发送fetch请求，leader将从ISR重移除这个follower，并认为这个follower已经挂了，默认10000。
  </p>
  <p>
   replica.lag.max.messages：如果一个replica没有备份的条数超过这个数值，则leader将移除这个follower，并认为这个follower已经挂了，默认4000。
  </p>
  <p>
   replica.socket.timeout.ms：leader 备份数据时的socket网络请求的超时时间，默认30*1000
  </p>
  <p>
   replica.socket.receive.buffer.bytes：备份时向leader发送网络请求时的socket receive buffer。默认64*1024。
  </p>
  <p>
   replica.fetch.max.bytes：备份时每次fetch的最大值。默认1024*1024。
  </p>
  <p>
   replica.fetch.max.bytes：leader发出备份请求时，数据到达leader的最长等待时间。默认500。
  </p>
  <p>
   replica.fetch.min.bytes：备份时每次fetch之后回应的最小尺寸。默认1。
  </p>
  <p>
   num.replica.fetchers：从leader备份数据的线程数。默认1。
  </p>
  <p>
   replica.high.watermark.checkpoint.interval.ms：每个replica检查是否将最高水位进行固化的频率。默认5000.
  </p>
  <p>
   fetch.purgatory.purge.interval.requests：fetch 请求清除时的清除间隔，默认1000
  </p>
  <p>
   producer.purgatory.purge.interval.requests：producer请求清除时的清除间隔，默认1000
  </p>
  <p>
   zookeeper.session.timeout.ms：zookeeper会话超时时间。默认6000
  </p>
  <p>
   zookeeper.connection.timeout.ms：客户端等待和zookeeper建立连接的最大时间。默认6000
  </p>
  <p>
   zookeeper.sync.time.ms：zk follower落后于zk leader的最长时间。默认2000
  </p>
  <p>
   controlled.shutdown.enable：是否能够控制broker的关闭。如果能够，broker将可以移动所有leaders到其他的broker上，在关闭之前。这减少了不可用性在关机过程中。默认True。
  </p>
  <p>
   controlled.shutdown.max.retries：在执行不彻底的关机之前，可以成功执行关机的命令数。默认3.
  </p>
  <p>
   controlled.shutdown.retry.backoff.ms：在关机之间的backoff时间。默认5000
  </p>
  <p>
   auto.leader.rebalance.enable：如果这是True，控制者将会自动平衡brokers对于partitions的leadership。默认True。
  </p>
  <p>
   leader.imbalance.per.broker.percentage：每个broker所允许的leader最大不平衡比率，默认10。
  </p>
  <p>
   leader.imbalance.check.interval.seconds：检查leader不平衡的频率，默认300
  </p>
  <p>
   offset.metadata.max.bytes：允许客户端保存他们offsets的最大个数。默认4096
  </p>
  <p>
   max.connections.per.ip：每个ip地址上每个broker可以被连接的最大数目。默认Int.MaxValue。
  </p>
  <p>
   max.connections.per.ip.overrides：每个ip或者hostname默认的连接的最大覆盖。
  </p>
  <p>
   connections.max.idle.ms：空连接的超时限制，默认600000
  </p>
  <p>
   log.roll.jitter.{ms,hours}：从logRollTimeMillis抽离的jitter最大数目。默认0
  </p>
  <p>
   num.recovery.threads.per.data.dir：每个数据目录用来日志恢复的线程数目。默认1。
  </p>
  <p>
   unclean.leader.election.enable：指明了是否能够使不在ISR中replicas设置用来作为leader。默认True
  </p>
  <p>
   delete.topic.enable：能够删除topic，默认False。
  </p>
  <p>
   offsets.topic.num.partitions：默认50。由于部署后更改不受支持，因此建议使用更高的设置来进行生产（例如100-200）。
  </p>
  <p>
   offsets.topic.retention.minutes：存在时间超过这个时间限制的offsets都将被标记为待删除。默认1440。
  </p>
  <p>
   offsets.retention.check.interval.ms：offset管理器检查陈旧offsets的频率。默认600000。
  </p>
  <p>
   offsets.topic.replication.factor：topic的offset的备份份数。建议设置更高的数字保证更高的可用性。默认3
  </p>
  <p>
   offset.topic.segment.bytes：offsets topic的segment尺寸。默认104857600
  </p>
  <p>
   offsets.load.buffer.size：这项设置与批量尺寸相关，当从offsets segment中读取时使用。默认5242880
  </p>
  <p>
   offsets.commit.required.acks：在offset commit可以接受之前，需要设置确认的数目，一般不需要更改。默认-1。
  </p>
  <p>
   <strong>
    2、Kafka生产者配置参数
   </strong>
  </p>
  <p>
   boostrap.servers：用于建立与kafka集群连接的host/port组。
  </p>
  <p>
   数据将会在所有servers上均衡加载，不管哪些server是指定用于bootstrapping。
  </p>
  <p>
   这个列表格式：host1:port1,host2:port2,…
  </p>
  <p>
   acks：此配置实际上代表了数据备份的可用性。
  </p>
  <p>
   acks=0： 设置为0表示producer不需要等待任何确认收到的信息。副本将立即加到socket buffer并认为已经发送。没有任何保障可以保证此种情况下server已经成功接收数据，同时重试配置不会发生作用
  </p>
  <p>
   acks=1： 这意味着至少要等待leader已经成功将数据写入本地log，但是并没有等待所有follower是否成功写入。这种情况下，如果follower没有成功备份数据，而此时leader又挂掉，则消息会丢失。
  </p>
  <p>
   acks=all： 这意味着leader需要等待所有备份都成功写入日志，这种策略会保证只要有一个备份存活就不会丢失数据。这是最强的保证。
  </p>
  <p>
   buffer.memory：producer可以用来缓存数据的内存大小。如果数据产生速度大于向broker发送的速度，producer会阻塞或者抛出异常，以“block.on.buffer.full”来表明。
  </p>
  <p>
   compression.type：producer用于压缩数据的压缩类型。默认是无压缩。正确的选项值是none、gzip、snappy。压缩最好用于批量处理，批量处理消息越多，压缩性能越好。
  </p>
  <p>
   retries：设置大于0的值将使客户端重新发送任何数据，一旦这些数据发送失败。注意，这些重试与客户端接收到发送错误时的重试没有什么不同。
  </p>
  <p>
   允许重试将潜在的改变数据的顺序，如果这两个消息记录都是发送到同一个partition，则第一个消息失败第二个发送成功，则第二条消息会比第一条消息出现要早。
  </p>
  <p>
   batch.size：producer将试图批处理消息记录，以减少请求次数。这将改善client与server之间的性能。这项配置控制默认的批量处理消息字节数。
  </p>
  <p>
   client.id：当向server发出请求时，这个字符串会发送给server。目的是能够追踪请求源头，以此来允许ip/port许可列表之外的一些应用可以发送信息。这项应用可以设置任意字符串，因为没有任何功能性的目的，除了记录和跟踪。
  </p>
  <p>
   linger.ms：producer组将会汇总任何在请求与发送之间到达的消息记录一个单独批量的请求。通常来说，这只有在记录产生速度大于发送速度的时候才能发生。
  </p>
  <p>
   max.request.size：请求的最大字节数。这也是对最大记录尺寸的有效覆盖。注意：server具有自己对消息记录尺寸的覆盖，这些尺寸和这个设置不同。此项设置将会限制producer每次批量发送请求的数目，以防发出巨量的请求。
  </p>
  <p>
   receive.buffer.bytes：TCP receive缓存大小，当阅读数据时使用。
  </p>
  <p>
   send.buffer.bytes：TCP send缓存大小，当发送数据时使用。
  </p>
  <p>
   timeout.ms：此配置选项控制server等待来自followers的确认的最大时间。如果确认的请求数目在此时间内没有实现，则会返回一个错误。这个超时限制是以server端度量的，没有包含请求的网络延迟。
  </p>
  <p>
   block.on.buffer.full：当我们内存缓存用尽时，必须停止接收新消息记录或者抛出错误。
  </p>
  <p>
   默认情况下，这个设置为真，然而某些阻塞可能不值得期待，因此立即抛出错误更好。设置为False则会这样：producer会抛出一个异常错误：BufferExhaustedException， 如果记录已经发送同时缓存已满。
  </p>
  <p>
   metadata.fetch.timeout.ms：是指我们所获取的一些元素据的第一个时间数据。元素据包含：topic，host，partitions。此项配置是指当等待元素据fetch成功完成所需要的时间，否则会抛出异常给客户端。
  </p>
  <p>
   metadata.max.age.ms：以微秒为单位的时间，是在我们强制更新metadata的时间间隔。即使我们没有看到任何partition leadership改变。
  </p>
  <p>
   metric.reporters：类的列表，用于衡量指标。实现MetricReporter接口，将允许增加一些类，这些类在新的衡量指标产生时就会改变。JmxReporter总会包含用于注册JMX统计
  </p>
  <p>
   metrics.num.samples：用于维护metrics的样本数。
  </p>
  <p>
   metrics.sample.window.ms：metrics系统维护可配置的样本数量，在一个可修正的window size。这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口推出后，我们会擦除并重写最老的窗口。
  </p>
  <p>
   recoonect.backoff.ms：连接失败时，当我们重新连接时的等待时间。这避免了客户端反复重连。
  </p>
  <p>
   retry.backoff.ms：在试图重试失败的produce请求之前的等待时间。避免陷入发送-失败的死循环中。
  </p>
  <p>
   <strong>
    3、Kafka消费者配置参数
   </strong>
  </p>
  <p>
   group.id：用来唯一标识consumer进程所在组的字符串，如果设置同样的group id，表示这些processes都是属于同一个consumer group。
  </p>
  <p>
   zookeeper.connect：指定zookeeper的连接的字符串，格式是hostname：port, hostname：port…
  </p>
  <p>
   consumer.id：不需要设置，一般自动产生
  </p>
  <p>
   socket.timeout.ms：网络请求的超时限制。真实的超时限制是max.fetch.wait+socket.timeout.ms。默认3000
  </p>
  <p>
   socket.receive.buffer.bytes：socket用于接收网络请求的缓存大小。默认64*1024。
  </p>
  <p>
   fetch.message.max.bytes：每次fetch请求中，针对每次fetch消息的最大字节数。默认1024*1024
  </p>
  <p>
   这些字节将会督导用于每个partition的内存中，因此，此设置将会控制consumer所使用的memory大小。
  </p>
  <p>
   这个fetch请求尺寸必须至少和server允许的最大消息尺寸相等，否则，producer可能发送的消息尺寸大于consumer所能消耗的尺寸。
  </p>
  <p>
   num.consumer.fetchers：用于fetch数据的fetcher线程数。默认1
  </p>
  <p>
   auto.commit.enable：如果为真，consumer所fetch的消息的offset将会自动的同步到zookeeper。这项提交的offset将在进程挂掉时，由新的consumer使用。默认True。
  </p>
  <p>
   auto.commit.interval.ms：consumer向zookeeper提交offset的频率，单位是秒。默认60*1000。
  </p>
  <p>
   queued.max.message.chunks：用于缓存消息的最大数目，每个chunk必须和fetch.message.max.bytes相同。默认2。
  </p>
  <p>
   rebalance.max.retries：当新的consumer加入到consumer group时，consumers集合试图重新平衡分配到每个consumer的partitions数目。如果consumers集合改变了，当分配正在执行时，这个重新平衡会失败并重入。默认4
  </p>
  <p>
   fetch.min.bytes：每次fetch请求时，server应该返回的最小字节数。如果没有足够的数据返回，请求会等待，直到足够的数据才会返回。
  </p>
  <p>
   fetch.wait.max.ms：如果没有足够的数据能够满足fetch.min.bytes，则此项配置是指在应答fetch请求之前，server会阻塞的最大时间。默认100
  </p>
  <p>
   rebalance.backoff.ms：在重试reblance之前backoff时间。默认2000
  </p>
  <p>
   refresh.leader.backoff.ms：在试图确定某个partition的leader是否失去他的leader地位之前，需要等待的backoff时间。默认200
  </p>
  <p>
   auto.offset.reset：zookeeper中没有初始化的offset时，如果offset是以下值的回应：
  </p>
  <p>
   lastest：自动复位offset为lastest的offset
  </p>
  <p>
   earliest：自动复位offset为earliest的offset
  </p>
  <p>
   none：向consumer抛出异常
  </p>
  <p>
   consumer.timeout.ms：如果没有消息可用，即使等待特定的时间之后也没有，则抛出超时异常
  </p>
  <p>
   exclude.internal.topics：是否将内部topics的消息暴露给consumer。默认True。
  </p>
  <p>
   paritition.assignment.strategy：选择向consumer 流分配partitions的策略，可选值：range，roundrobin。默认range。
  </p>
  <p>
   client.id：是用户特定的字符串，用来在每次请求中帮助跟踪调用。它应该可以逻辑上确认产生这个请求的应用。
  </p>
  <p>
   zookeeper.session.timeout.ms：zookeeper 会话的超时限制。默认6000
  </p>
  <p>
   如果consumer在这段时间内没有向zookeeper发送心跳信息，则它会被认为挂掉了，并且reblance将会产生
  </p>
  <p>
   zookeeper.connection.timeout.ms：客户端在建立通zookeeper连接中的最大等待时间。默认6000
  </p>
  <p>
   zookeeper.sync.time.ms：ZK follower可以落后ZK leader的最大时间。默认1000
  </p>
  <p>
   offsets.storage：用于存放offsets的地点： zookeeper或者kafka。默认zookeeper。
  </p>
  <p>
   offset.channel.backoff.ms：重新连接offsets channel或者是重试失败的offset的fetch/commit请求的backoff时间。默认1000
  </p>
  <p>
   offsets.channel.socket.timeout.ms：当读取offset的fetch/commit请求回应的socket 超时限制。此超时限制是被consumerMetadata请求用来请求offset管理。默认10000。
  </p>
  <p>
   offsets.commit.max.retries：重试offset commit的次数。这个重试只应用于offset commits在shut-down之间。默认5。
  </p>
  <p>
   dual.commit.enabled：如果使用“kafka”作为offsets.storage，你可以二次提交offset到zookeeper(还有一次是提交到kafka）。
  </p>
  <p>
   在zookeeper-based的offset storage到kafka-based的offset storage迁移时，这是必须的。对任意给定的consumer group来说，比较安全的建议是当完成迁移之后就关闭这个选项
  </p>
  <p>
   partition.assignment.strategy：在“range”和“roundrobin”策略之间选择一种作为分配partitions给consumer 数据流的策略。
  </p>
  <p>
   循环的partition分配器分配所有可用的partitions以及所有可用consumer线程。它会将partition循环的分配到consumer线程上。如果所有consumer实例的订阅都是确定的，则partitions的划分是确定的分布。
  </p>
  <p>
   循环分配策略只有在以下条件满足时才可以
  </p>
  <p>
   1）每个topic在每个consumer实力上都有同样数量的数据流。
  </p>
  <p>
   2）订阅的topic的集合对于consumer group中每个consumer实例来说都是确定的。
  </p>
  <h3>
   Kafka的高水位和Leader Epoch
  </h3>
  <p>
   问过的一些公司：ebay
  </p>
  <p>
   参考答案：
  </p>
  <p>
   高水位(High Watermark)，
   <strong>
    通常被用在流式处理领域（比如Apache Flink、Apache Spark等），以表征元素或事件在基于时间层面上的进度
   </strong>
   <span class="lake-fontsize-12">
    。一个比较经典的表述为：流式系统保证在水位t时刻，创建时间（event time） = t'且t' ≤ t的所有事件都已经到达或被观测到。
   </span>
   <strong>
    在Kafka中，水位的概念反而与时间无关，而是与位置信息相关
   </strong>
   <span class="lake-fontsize-12">
    。严格来说，
   </span>
   <strong>
    它表示的就是位置信息，即位移（offset）
   </strong>
   <span class="lake-fontsize-12">
    。通俗的说下HW作用：
   </span>
   <strong>
    Kafka使用HW值来决定副本备份的进度
   </strong>
  </p>
  <p>
   Kafka分区下有可能有很多个副本(replica)用于实现冗余，从而进一步实现高可用。副本根据角色的不同可分为3类：
  </p>
  <ul list="ud6b9205d">
   <li fid="u7c303d2e">
    leader副本：响应clients端读写请求的副本
   </li>
   <li fid="u7c303d2e">
    follower副本：被动地备份leader副本中的数据，不能响应clients端读写请求。
   </li>
   <li fid="u7c303d2e">
    ISR副本：包含了leader副本和所有与leader副本保持同步的follower副本——如何判定是否与leader同步后面会提到
   </li>
  </ul>
  <p>
   每个Kafka副本对象都有两个重要的属性：LEO和HW。
   <strong>
    注意是所有的副本，而不只是leader副本。
   </strong>
  </p>
  <ul list="u0affbd17">
   <li fid="uf7a19fec">
    LEO：即日志末端位移(log end offset)，记录了该副本底层日志(log)中下一条消息的位移值。注意是下一条消息！也就是说，如果LEO=10，那么表示该副本保存了10条消息，位移值范围是[0, 9]。另外，leader LEO和follower LEO的更新是有区别的。
   </li>
   <li fid="uf7a19fec">
    HW：即上面提到的水位值。对于同一个副本对象而言，其HW值不会大于LEO值。小于等于HW值的所有消息都被认为是“已备份”的（replicated）。同理，leader副本和follower副本的HW更新也是有区别的。
   </li>
  </ul>
  <p>
   通过下图我们来了解下LEO和HW两者的关系：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662729373024-bf3994a1-8496-4725-972b-2f5b4e556750.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_21%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   上图中，HW值是7，表示位移是0
   <code>
    ~
   </code>
   <span class="lake-fontsize-12">
    7的所有消息都已经处于“已备份状态”（committed），而LEO值是15，那么8~14的消息就是尚未完全备份（fully replicated）——为什么没有15？因为刚才说过了，LEO指向的是下一条消息到来时的位移，故上图使用虚线框表示。我们总说consumer无法消费未提交消息。这句话如果用以上名词来解读的话，应该表述为：consumer无法消费分区下leader副本中位移值大于
   </span>
   <strong>
    分区HW
   </strong>
   <span class="lake-fontsize-12">
    的任何消息。这里需要特别注意
   </span>
   <strong>
    分区HW就是leader副本的HW值
   </strong>
   <span class="lake-fontsize-12">
    。
   </span>
  </p>
  <p>
   <strong>
    1、follower副本何时更新LEO？
   </strong>
  </p>
  <p>
   如前所述，follower副本只是被动地向leader副本请求数据，具体表现为follower副本不停地向leader副本所在的broker发送FETCH请求，一旦获取消息后写入自己的日志中进行备份。那么follower副本的LEO是何时更新的呢？Kafka有两套follower副本LEO：1. 一套LEO保存在follower副本所在broker的副本管理机中；2. 另一套LEO保存在leader副本所在broker的副本管理机中——换句话说，leader副本机器上保存了所有的follower副本的LEO。
  </p>
  <p>
   为什么要保存两套？这是因为Kafka使用前者帮助follower副本更新其HW值；而利用后者帮助leader副本更新其HW使用。下面我们分别看下它们被更新的时机。
  </p>
  <p>
   <strong>
    1）follower副本端的follower副本LEO何时更新？
   </strong>
  </p>
  <p>
   follower副本端的LEO值就是其底层日志的LEO值，也就是说每当新写入一条消息，其LEO值就会被更新(类似于LEO += 1)。当follower发送FETCH请求后，leader将数据返回给follower，此时follower开始向底层log写数据，从而自动地更新LEO值。
  </p>
  <p>
   <strong>
    2）leader副本端的follower副本LEO何时更新？
   </strong>
  </p>
  <p>
   leader副本端的follower副本LEO的更新发生在leader在处理follower FETCH请求时。一旦leader接收到follower发送的FETCH请求，它首先会从自己的log中读取相应的数据，但是在给follower返回数据之前它先去更新follower的LEO(即上面所说的第二套LEO)。
  </p>
  <p>
   <strong>
    2、follower副本何时更新HW？
   </strong>
  </p>
  <p>
   follower更新HW发生在其更新LEO之后，一旦follower向log写完数据，它会尝试更新它自己的HW值。具体算法就是比较当前LEO值与FETCH响应中leader的HW值，取两者的小者作为新的HW值。这告诉我们一个事实：如果follower的LEO值超过了leader的HW值，那么follower HW值是不会越过leader HW值的。
  </p>
  <p>
   <strong>
    3、leader副本何时更新LEO？
   </strong>
  </p>
  <p>
   和follower更新LEO道理相同，leader写log时就会自动地更新它自己的LEO值。
  </p>
  <p>
   <strong>
    4、leader副本何时更新HW值？
   </strong>
  </p>
  <p>
   前面说过了，leader的HW值就是分区HW值，因此何时更新这个值是我们最关心的，因为它直接影响了分区数据对于consumer的可见性 。以下4种情况下leader会尝试去更新分区HW——切记是尝试，有可能因为不满足条件而不做任何更新：
  </p>
  <ul list="u659a275e">
   <li fid="u3f0f02b9">
    副本成为leader副本时：当某个副本成为了分区的leader副本，Kafka会尝试去更新分区HW。这是显而易见的道理，毕竟分区leader发生了变更，这个副本的状态是一定要检查的！不过，本文讨论的是当系统稳定后且正常工作时备份机制可能出现的问题，故这个条件不在我们的讨论之列。
   </li>
   <li fid="u3f0f02b9">
    broker出现崩溃导致副本被踢出ISR时：若有broker崩溃则必须查看下是否会波及此分区，因此检查下分区HW值是否需要更新是有必要的。本文不对这种情况做深入讨论
   </li>
   <li fid="u3f0f02b9">
    producer向leader副本写入消息时：因为写入消息会更新leader的LEO，故有必要再查看下HW值是否也需要修改
   </li>
   <li fid="u3f0f02b9">
    leader处理follower FETCH请求时：当leader处理follower的FETCH请求时首先会从底层的log读取数据，之后会尝试更新分区HW值
   </li>
  </ul>
  <p>
   特别注意上面4个条件中的最后两个。它揭示了一个事实——当Kafka broker都正常工作时，分区HW值的更新时机有两个：leader处理PRODUCE请求时和leader处理FETCH请求时。另外，leader是如何更新它的HW值的呢？前面说过了，leader broker上保存了一套follower副本的LEO以及它自己的LEO。当尝试确定分区HW时，它会选出所有满足条件的副本，比较它们的LEO(当然也包括leader自己的LEO)，并选择最小的LEO值作为HW值。这里的满足条件主要是指副本要满足以下两个条件之一：
  </p>
  <ul list="u4d622a20">
   <li fid="uf48ca760">
    处于ISR中
   </li>
   <li fid="uf48ca760">
    副本LEO落后于leader LEO的时长不大于replica.lag.time.max.ms参数值(默认是10s)
   </li>
  </ul>
  <p>
   乍看上去好像这两个条件说得是一回事，毕竟ISR的定义就是第二个条件描述的那样。但某些情况下Kafka的确可能出现副本已经“追上”了leader的进度，但却不在ISR中——比如某个从failure中恢复的副本。如果Kafka只判断第一个条件的话，确定分区HW值时就不会考虑这些未在ISR中的副本，但这些副本已经具备了“立刻进入ISR”的资格，因此就可能出现分区HW值越过ISR中副本LEO的情况——这肯定是不允许的，因为分区HW实际上就是ISR中所有副本LEO的最小值。
  </p>
  <p>
   下面举个实际的例子。我们假设有一个topic，单分区，副本因子是2，即一个leader副本和一个follower副本。我们看下当producer发送一条消息时，broker端的副本到底会发生什么事情以及分区HW是如何被更新的。
  </p>
  <p>
   下图是初始状态，稍微解释一下：初始时leader和follower的HW和LEO都是0(严格来说源代码会初始化LEO为-1，不过这不影响之后的讨论)。leader中的remote LEO指的就是leader端保存的follower LEO，也被初始化成0。此时，producer没有发送任何消息给leader，而follower已经开始不断地给leader发送FETCH请求了，但因为没有数据因此什么都不会发生。值得一提的是，follower发送过来的FETCH请求因为无数据而暂时会被寄存到leader端的purgatory中，待500ms(replica.fetch.wait.max.ms参数)超时后会强制完成。倘若在寄存期间producer端发送过来数据，那么会Kafka会自动唤醒该FETCH请求，让leader继续处理之。
  </p>
  <p>
   因为FETCH请求发送和PRODUCE请求处理的时机会影响后面的一些内容。因此后续我们也将分两种情况来讨论分区HW的更新。
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662729566792-cdc3607d-65a2-470b-8e44-d4e343cd4870.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_20%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   <strong>
    第一种情况：follower发送FETCH请求在leader处理完PRODUCE请求之后
   </strong>
  </p>
  <p>
   producer给该topic分区发送了一条消息。此时的状态如下图所示：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662729588424-b49fffb5-4117-4e0d-bc96-20d8719d9406.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_24%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   如图所示，leader接收到PRODUCE请求主要做两件事情：
  </p>
  <ul list="ua5513414">
   <li fid="ue300e8a1">
    把消息写入写底层log（同时也就自动地更新了leader的LEO）
   </li>
   <li fid="ue300e8a1">
    尝试更新leader HW值（前面
    <strong>
     leader副本何时更新HW值
    </strong>
    <span class="lake-fontsize-12">
     一节中的第三个条件触发）。我们已经假设此时follower尚未发送FETCH请求，那么leader端保存的remote LEO依然是0，因此leader会比较它自己的LEO值和remote LEO值，发现最小值是0，与当前HW值相同，故不会更新分区HW值
    </span>
   </li>
  </ul>
  <p>
   所以，PRODUCE请求处理完成后leader端的HW值依然是0，而LEO是1，remote LEO是1。假设此时follower发送了FETCH请求(或者说follower早已发送了FETCH请求，只不过在broker的请求队列中排队)，那么状态变更如下图所示：
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662729605951-489d4bbc-2210-4cbd-8508-bbe3b9f6990f.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_20%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   本例中当follower发送FETCH请求时，leader端的处理依次是：
  </p>
  <ol list="u8c8b2145">
   <li fid="u5d1231db">
    读取底层log数据
   </li>
   <li fid="u5d1231db">
    更新remote LEO = 0（为什么是0？ 因为此时follower还没有写入这条消息。leader如何确认follower还未写入呢？这是通过follower发来的FETCH请求中的fetch offset来确定的）
   </li>
   <li fid="u5d1231db">
    尝试更新分区HW——此时leader LEO = 1，remote LEO = 0，故分区HW值= min(leader LEO, follower remote LEO) = 0
   </li>
   <li fid="u5d1231db">
    把数据和当前分区HW值（依然是0）发送给follower副本
   </li>
  </ol>
  <p>
   而follower副本接收到FETCH response后依次执行下列操作：
  </p>
  <ol list="u23e0fc72">
   <li fid="u07f41d0f">
    写入本地log（同时更新follower LEO）
   </li>
   <li fid="u07f41d0f">
    更新follower HW——比较本地LEO和当前leader HW取小者，故follower HW = 0
   </li>
  </ol>
  <p>
   此时，第一轮FETCH RPC结束，我们会发现虽然leader和follower都已经在log中保存了这条消息，但分区HW值尚未被更新。实际上，它是在第二轮FETCH RPC中被更新的，如下图所示：
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662729638859-f874f279-d253-4cac-9c14-d3645cdf6e03.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_25%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   上图中，follower发来了第二轮FETCH请求，leader端接收到后仍然会依次执行下列操作：
  </p>
  <ol list="uc9b9df80">
   <li fid="ua67dbd73">
    读取底层log数据
   </li>
   <li fid="ua67dbd73">
    更新remote LEO = 1（这次为什么是1了？ 因为这轮FETCH RPC携带的fetch offset是1，那么为什么这轮携带的就是1了呢，因为上一轮结束后follower LEO被更新为1了）
   </li>
   <li fid="ua67dbd73">
    尝试更新分区HW——此时leader LEO = 1，remote LEO = 1，故分区HW值= min(leader LEO, follower remote LEO) = 1。
    <strong>
     注意分区HW值此时被更新了！！！
    </strong>
   </li>
   <li fid="ua67dbd73">
    把数据（实际上没有数据）和当前分区HW值（已更新为1）发送给follower副本
   </li>
  </ol>
  <p>
   同样地，follower副本接收到FETCH response后依次执行下列操作：
  </p>
  <ol list="u627ed3c3">
   <li fid="u3222e372">
    <strong>
     写入本地log，当然没东西可写，故follower LEO也不会变化，依然是1
    </strong>
   </li>
   <li fid="u3222e372">
    <strong>
     更新follower HW——比较本地LEO和当前leader LEO取小者。由于此时两者都是1，故更新follower HW = 1
    </strong>
   </li>
  </ol>
  <p>
   producer端发送消息后broker端完整的处理流程就讲完了。此时消息已经成功地被复制到leader和follower的log中且分区HW是1，表明consumer能够消费offset = 0的这条消息。下面我们来分析下PRODUCE和FETCH请求交互的第二种情况。
  </p>
  <p>
   <strong>
    第二种情况：FETCH请求保存在purgatory中PRODUCE请求到来
   </strong>
  </p>
  <p>
   这种情况实际上和第一种情况差不多。前面说过了，当leader无法立即满足FECTH返回要求的时候(比如没有数据)，那么该FETCH请求会被暂存到leader端的purgatory中，待时机成熟时会尝试再次处理它。不过Kafka不会无限期地将其缓存着，默认有个超时时间（500ms），一旦超时时间已过，则这个请求会被强制完成。不过我们要讨论的场景是在寄存期间，producer发送PRODUCE请求从而使之满足了条件从而被唤醒。此时，leader端处理流程如下：
  </p>
  <ol list="u6a98c5b5">
   <li fid="ub20409f7">
    leader写入本地log（同时自动更新leader LEO）
   </li>
   <li fid="ub20409f7">
    尝试唤醒在purgatory中寄存的FETCH请求
   </li>
   <li fid="ub20409f7">
    尝试更新分区HW
   </li>
  </ol>
  <p>
   至于唤醒后的FETCH请求的处理与第一种情况完全一致，故这里不做详细展开了。
  </p>
  <p>
   以上所有的东西其实就想说明一件事情：
   <strong>
    Kafka使用HW值来决定副本备份的进度，而HW值的更新通常需要额外一轮FETCH RPC才能完成，故而这种设计是有问题的。
   </strong>
   <span class="lake-fontsize-12">
    它们可能引起的问题包括：
   </span>
  </p>
  <ul list="u82786a62">
   <li fid="ufbe21956">
    备份数据丢失
   </li>
   <li fid="ufbe21956">
    备份数据不一致
   </li>
  </ul>
  <p>
   我们来看下上述的两种情况：
  </p>
  <p>
   <strong>
    1）数据丢失
   </strong>
  </p>
  <p>
   如前所述，使用HW值来确定备份进度时其值的更新是在下一轮RPC中完成的。现在翻到上面使用两种不同颜色标记的步骤处思考下， 如果follower副本在蓝色标记的第一步与紫色标记的第二步之间发生崩溃，那么就有可能造成数据的丢失。我们举个例子来看下。
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662729671109-3f8c8e9e-7e58-479c-9d64-9eb78a4b601f.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_17%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   上图中有两个副本：A和B。开始状态是A是leader。我们假设producer端min.insync.replicas设置为1，那么当producer发送两条消息给A后，A写入到底层log，此时Kafka会通知producer说这两条消息写入成功。
  </p>
  <p>
   但是在broker端，leader和follower底层的log虽都写入了2条消息且分区HW已经被更新到2，但follower HW尚未被更新（也就是上面紫色颜色标记的第二步尚未执行）。倘若此时副本B所在的broker宕机，那么重启回来后B会自动把LEO调整到之前的HW值，故副本B会做日志截断(log truncation)，将offset = 1的那条消息从log中删除，并调整LEO = 1，此时follower副本底层log中就只有一条消息，即offset = 0的消息。
  </p>
  <p>
   B重启之后需要给A发FETCH请求，但若A所在broker机器在此时宕机，那么Kafka会令B成为新的leader，而当A重启回来后也会执行日志截断，将HW调整回1。这样，位移=1的消息就从两个副本的log中被删除，即永远地丢失了。
  </p>
  <p>
   这个场景丢失数据的前提是在min.insync.replicas=1时，一旦消息被写入leader端log即被认为是“已提交”，而延迟一轮FETCH RPC更新HW值的设计使得follower HW值是异步延迟更新的，倘若在这个过程中leader发生变更，那么成为新leader的follower的HW值就有可能是过期的，使得clients端认为是成功提交的消息被删除。
  </p>
  <p>
   <strong>
    2）leader/follower数据离散
   </strong>
  </p>
  <p>
   除了可能造成的数据丢失以外，这种设计还有一个潜在的问题，即造成leader端log和follower端log的数据不一致。比如leader端保存的记录序列是r1,r2,r3,r4,r5,....；而follower端保存的序列可能是r1,r3,r4,r5,r6...。这也是非法的场景，因为顾名思义，follower必须追随leader，完整地备份leader端的数据。
  </p>
  <p>
   我们依然使用一张图来说明这种场景是如何发生的：
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662729732127-c3f2a5d8-9a1b-405d-8777-89860cf6f818.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_15%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   这种情况的初始状态与情况1有一些不同的：A依然是leader，A的log写入了2条消息，但B的log只写入了1条消息。分区HW更新到2，但B的HW还是1，同时producer端的min.insync.replicas = 1。
  </p>
  <p>
   这次我们让A和B所在机器同时挂掉，然后假设B先重启回来，因此成为leader，分区HW = 1。假设此时producer发送了第3条消息(绿色框表示)给B，于是B的log中offset = 1的消息变成了绿色框表示的消息，同时分区HW更新到2（A还没有回来，就B一个副本，故可以直接更新HW而不用理会A）之后A重启回来，需要执行日志截断，但发现此时分区HW=2而A之前的HW值也是2，故不做任何调整。此后A和B将以这种状态继续正常工作。
  </p>
  <p>
   显然，这种场景下，A和B底层log中保存在offset = 1的消息是不同的记录，从而引发不一致的情形出现。
  </p>
  <p>
   <strong>
    关于上述问题的解决方案：
   </strong>
  </p>
  <p>
   造成上述两个问题的根本原因在于HW值被用于衡量副本备份的成功与否以及在出现failture时作为日志截断的依据，但HW值的更新是异步延迟的，特别是需要额外的FETCH请求处理流程才能更新，故这中间发生的任何崩溃都可能导致HW值的过期。鉴于这些原因，Kafka 0.11引入了leader epoch来取代HW值。Leader端多开辟一段内存区域专门保存leader的epoch信息，这样即使出现上面的两个场景也能很好地规避这些问题。
  </p>
  <p>
   所谓leader epoch实际上是一对值：(epoch，offset)。epoch表示leader的版本号，从0开始，当leader变更过1次时epoch就会+1，而offset则对应于该epoch版本的leader写入第一条消息的位移。因此假设有两对值：
  </p>
  <p>
   (0, 0)和(1, 120)
  </p>
  <p>
   则表示第一个leader从位移0开始写入消息；共写了120条[0, 119]；而第二个leader版本号是1，从位移120处开始写入消息。
  </p>
  <p>
   leader broker中会保存这样的一个缓存，并定期地写入到一个checkpoint文件中。
  </p>
  <p>
   当leader写底层log时它会尝试更新整个缓存——如果这个leader首次写消息，则会在缓存中增加一个条目；否则就不做更新。而每次副本重新成为leader时会查询这部分缓存，获取出对应leader版本的位移，这就不会发生数据不一致和丢失的情况。
  </p>
  <p>
   下面使用图的方式来说明下利用leader epoch如何规避上述两种情况
  </p>
  <p>
   <strong>
    1）规避数据丢失
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662729773560-821575e7-74ad-4512-8083-36a6ec078005.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_24%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   上图左半边已经给出了简要的流程描述，这里不详细展开具体的leader epoch实现细节（比如OffsetsForLeaderEpochRequest的实现），我们只需要知道每个副本都引入了新的状态来保存自己当leader时开始写入的第一条消息的offset以及leader版本。这样在恢复的时候完全使用这些信息而非水位来判断是否需要截断日志。
  </p>
  <p>
   <strong>
    2）规避数据不一致
   </strong>
  </p>
  <p>
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1662729791719-105dda0c-ead5-4c11-a117-6105dccfd655.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_25%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
  <p>
   同样的道理，依靠leader epoch的信息可以有效地规避数据不一致的问题。
  </p>
  <p>
   总结：0.11.0.0版本的Kafka通过引入leader epoch解决了原先依赖水位表示副本进度可能造成的数据丢失/数据不一致问题。有兴趣的读者可以阅读源代码进一步地了解其中的工作原理。
  </p>
  <h3>
   Kafka的分区器、拦截器、序列化器？
  </h3>
  <p>
   问过的一些公司：ebay
  </p>
  <p>
   参考答案：
  </p>
  <p>
   <strong>
    Kafka中，先执行拦截器对消息进行相应的定制化操作，然后执行序列化器将消息序列化，最后执行分区器选择对应分区
   </strong>
  </p>
  <p>
   <strong>
    拦截器 -&gt; 序列化器 -&gt; 分区器
   </strong>
  </p>
  <p>
   <strong>
    1、拦截器
   </strong>
  </p>
  <p>
   Kafka有两种拦截器：生产者拦截器和消费者拦截器
  </p>
  <p>
   生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规定过滤不符合要求的消息、修改消息内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。
  </p>
  <p>
   生产者拦截器的实现，主要是自定义实现 org.apache.kafka.clients.producer. ProducerInterceptor 接口。ProducerInterceptor接口包含3个方法：
  </p>
  <pre><code class="language-java" lang="java">ProducerRecord&lt;K, V&gt; onSend(ProducerRecord&lt;K, V&gt; var1);
void onAcknowledgement(RecordMetadata var1, Exception var2);
void close();</code></pre>
  <p>
   <strong>
    KafkaProducer 在将消息序列化和计算分区之前会调用生产者拦截器的onSend()方法来对消息进行相应的定制化操作
   </strong>
   <span class="lake-fontsize-12">
    。一般来说最好不要修改消息 ProducerRecord 的 topic、key 和 pritition 等信息，如果修改需要保证对其有准确判断，否则会出现与预想不一致的偏差。比如修改 key 不仅会影响分区的计算还会影响 broker 端日志压缩（Log Compaction）功能。
   </span>
  </p>
  <p>
   KafkaProducer 会在消息被应答（Acknowledgement）之前或消息发送失败时调用生产者拦截器的 onAcknowledgement() 方法，优先于用户设定的 Callback 之前执行。
   <strong>
    这个方法运行在 Producer的I/O线程中，所以这个方法的实现逻辑约简单越好，否则会影响消息的发送
   </strong>
   <span class="lake-fontsize-12">
    。
   </span>
  </p>
  <p>
   close()方法主要用于在关闭拦截器时执行一些资源的清理工作。在这3个方法中抛出的异常都会被捕获并记录到日志中，但并不会再向上传递。
  </p>
  <p>
   ProducerInterceptor接口与Protitioner 接口一样都有一个父接口Configurable。
  </p>
  <p>
   Kafka中不仅可以指定一个拦截器还可以指定多个拦截器形成一个拦截器链。拦截器链会根据配置时配置的拦截器顺序来执行（配置的时候，各个拦截器之间使用逗号隔开）。
  </p>
  <p>
   如果拦截器链中的某个拦截器的执行需要依赖上一个拦截器的输出，那么就有可能产生“副作用”。如果第一个拦截器因为异常执行失败，那么第二个也就不能继续执行。
   <strong>
    在拦截链中，如果某个拦截器执行失败，那么下一个拦截器会接着从上一个执行成功的拦截器继续执行
   </strong>
   <span class="lake-fontsize-12">
    。
   </span>
  </p>
  <p>
   <strong>
    2、序列化
   </strong>
  </p>
  <p>
   生产者需要用序列化器（Serializer）将key和value序列化成字节数组才可以将消息传入Kafka。消费者需要用反序列化器（Deserializer）把从Kafka中收到的字节数组转化成相应的对象。在代码清单3-1中，key和value都使用了字符字符串，对应程序中的序列化器也使用了客户端自带的 StringSerializer，除了字符串类型的序列化器，还有 ByteArray、ByteBuffer、Bytes、Double、Integer、Long 这几种类型，它们都实现了 org.apache.kafka.common.serialization.Serializer 接口，此接口有3个方法：
  </p>
  <pre><code class="language-java" lang="java">// 用来配置当前类
public void configure(Map&lt;String, ?&gt; configs, boolean isKey)
// 用来执行序列化操作
public byte[] serializer(String topic, T data)
// 用来关闭当前的序列化器
public void close()</code></pre>
  <p>
   一般情况下 close() 是个空方法，如果实现了此方法，则必须确保此方法的幂等性（一次和多次请求某一个资源对于资源本身应该具有同样的结果（网络超时等问题除外）。也就是说，其任意多次执行对资源本身所产生的影响均与一次执行的影响相同。），因为这个方法可能会被 KafkaProducer调用多次（Serializer和KafkaProducer 所实现的接口都继承了 Closeable 接口）。
  </p>
  <p>
   生产者使用的序列化和消费者使用的序列化是一一对应的，如果生产者使用了 StringSerializer，而消费者使用了另一种序列化器，那么是无法解析出相要数据的。
  </p>
  <p>
   如果Kafka客户端提供的几种序列化器都无法满足应用需求，可以选择如 Avro、JSON等通用的序列化工具实现，或者使用自定义类型的序列化器来实现。
  </p>
  <p>
   <strong>
    3、分区器
   </strong>
  </p>
  <p>
   消息在通过 send() 发往 broker 的过程中，有可能需要经过拦截器（Interceptor）、序列化器（Serializer）和分区器（Partitioner）的一系列作用之后才能被真正地发往 broker。拦截器一般不是必须的，而序列化器是必须的。消息经过序列化之后就需要确定它发往的分区，如果消息 ProducerRecord 中指定了 partition 字段，那么就不需要分区器的作用，因为 partition 代表的就是要发往的分区号。
  </p>
  <p>
   如果 ProducerRecode 中没有指定 partition 字段，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值。
   <strong>
    分区器的作用就是为消息分配分区
   </strong>
   <span class="lake-fontsize-12">
    。
   </span>
  </p>
  <p>
   Kafka 中提供的默认分区器是 org.apache.kafka.clients.producer.internals.DefaultPartitioner，它实现了 org.apache.kafka.clients.producer.Partitioner 接口，这个接口中定义了2个方法，具体如下所示。
  </p>
  <pre><code class="language-java" lang="java">public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);
public void close();</code></pre>
  <p>
   partition() 方法用来计算分区号，返回值为ini类型。方法中的参数分别表示主题、键、序列化后的键、值、序列化后的值以及集群的元数据信息，通过这些信息可以实现分区器。close()方法在关闭分区器的时候回收一些资源。
  </p>
  <p>
   Partitioner接口还有一个父接口 org.apache.kafka.common.Configurable，该接口只有一个方法
  </p>
  <pre><code class="language-java" lang="java">void configure(Map&lt;String, ?&gt; var1);</code></pre>
  <p>
   Configurable 接口中的 configure() 方法主要是用来获取配置信息及初始化数据。
  </p>
  <p>
   在默认分区器 DefaultPartitioner 的实现中，close()是空方法，而 partition() 方法中定义了主要的分区分配逻辑。如果 key 不为 None，那么默认分区器会对 key 进行哈希（采用 MurmurHash2 算法，具备高运算性能及低碰撞率）根据最终得到的哈希值，与分区的数量取模运算得到分区编号来匹配分区，相同key得到的哈希值是一样的，所以当key一致，分区数量不变的情况下，会将消息写入同一个分区（注意：在不改变主题分区数量的情况下，key 与分区之间的映射可以保持不变。不过，一旦主题增加了分区，那么就难以保证key与分区的映射关系）。如果，key 是 None，那么消息会以轮询的方式写入分区。（注意：如果 key 不为None，那么计算得到的分区号会是所有分区中的一个。如果 key 为 None 并且有可用的分区的时候，那么计算得到的分区号仅为可用分区中的任意一个。）
  </p>
  <p>
   除了使用 Kafka 提供的默认分区器进行分配，还可以使用自定义的分区器，只需要和 DefaultPartitioner 一样 实现 Partitioner 接口即可。默认的分区器在 key 为 None 不会选择不可用的分区，我们通过自定义分区器来实现。
  </p>
  <h3>
   Kafka的生成者客户端有几个线程？
  </h3>
  <p>
   问过的一些公司：昆仑万维
  </p>
  <p>
   参考答案：
  </p>
  <p>
   2个，主线程和Sender线程。
  </p>
  <p>
   <strong>
    主线程
   </strong>
   <span class="lake-fontsize-12">
    负责创建消息，然后通过分区器、序列化器、拦截器作用之后缓存到累加器RecordAccumulator中。
   </span>
  </p>
  <p>
   <strong>
    Sender线程
   </strong>
   <span class="lake-fontsize-12">
    负责将RecordAccumulator中消息发送到kafka中。
   </span>
  </p>
  <p>
   <br/>
  </p>
  <p style="text-align: center">
   <img src="https://cdn.nlark.com/yuque/0/2022/png/12867669/1661077543035-a9c9e813-6e34-4f60-bd43-2a020969b404.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_10%2Ctext_5YWs5LyX5Y-377ya5pen5pe25YWJ5aSn5pWw5o2u%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10"/>
  </p>
 </body>
</html>