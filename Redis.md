## Redis
* redis是内存数据存储引擎，可以用作数据库、缓存、消息队列和分布式锁等
* redis有String、Hash、Set、Zset、Bitmap、Hyperloglog、GEO、Stream
* 数据操作是原子性的
* redis支持事务、持久化、Lua脚本、多种集群方式、发布/订阅模式、内存淘汰机制、过期删除机制等等
### Redis和Memcache有什么区别
* 都是高性能内存数据库，一般用来做缓存，都有过期替换策略
* Redis数据结构更丰富，Memcache只有key-value
* Redis支持持久化，Memcache断电就没了
* Redis支持集群化，Memcache依靠客户端实现往集群里写入数据
* Redis支持订阅模型、Lua脚本、事务等
### Redis作为MySQL的缓存
* MySQL QPS很难突破1w，Redis可以轻松突破10w
* 请求从Redis读取数据，没有再从MySQL加载数据到Redis
* 缓存一致性问题通过先修改，再删除，部分解决
### Redis数据类型的应用场景
* String：缓存对象、常规计数、分布式锁、共享session信息等
* List：消息队列（生产者需要自行实现全局唯一ID、不能以消费组的形式消费数据）
* Set：聚合计算的场景，比如点赞、关注、抽奖活动等
* Zset：排序场景，排行榜、热搜、电话和姓名排序等
* Bitmap：二值状态统计的场景、用户登录状态、签到、连续签到的人数
* HyperLoglog：海量数据统计、百万级网页uv计数
* GEO：LBS
* Stream：消息队列，相比List，自动生成全局唯一ID，可以用消费组的形式消费数据
### Redis数据类型的实现
* String基于SDS
* * 动态简单字符串，不仅可以保存文本还可以保存二进制
* * 获取字符串长度的代价是O（1）
* * SDS API是安全的，拼接字符串不会造成缓冲区溢出
* * String有三种类型，分别是int、raw和embstr
* * * int就是redis数据结构的指针指向一个long类型的位置
* * * 如果小于32字节（随着不断变化，3.0-4.0是39，5.0是44）就用embstr
* * * embstr是redis的数据结构后面紧跟着一个SDS的头，然后指针指向存储位置
* * * raw是redis的数据结构指针指向SDS的头，SDS再指向一个存储位置
* * * embstr比raw少一片区域，分配和释放的时候都只需要两次，会快一点。并且由于位置离得近，CPUcache更好命中
* * * 但是embstr实际上是只读的，修改的时候redis会把它换成raw再搞（因为redis数据结构和SDS都需要重新分配空间）
* * string的应用场景
* * * 缓存对象：整个JSON，将key分离常user:ID:属性，比如key是user:1:name value是xiaolin
* * * 常规计数，由于redis操作是原子性的，所以可以计数
* * * 分布式锁，如果key存在就插入，释放的时候需要判断是否是加锁的用户，用Lua脚本
* * * 共享session信息，session保存会话换一个服务器就不行了，可以用redis专门来保存所有服务器的session
* List基于双向链表或压缩列表
* * 小于512个，列表每个元素的值都小于64字节，使用压缩列表
* * 其他使用双向链表
* * redis3.2之后List底层都是quicklist实现的
* * 应用场景
* * * 消息队列：消息保序、处理重复消息和保证消息可靠性
* * * List本身是先进先出的，可以保序，生产者LPUSH，消费者RPOP
* * * Redis为了防止消费者一直轮询，使用BRPOP命令，阻塞式读取，没读到就阻塞
* * * List不会生成全局唯一ID，需要自己生成
* * * List读了一个消息，List就不会留存了，如果消费者宕机了，没法重新读
* * * List有BRPOPLPUSH命令，阻塞读取后放到另一个List备份
* * * List没法实现多个消费者消费同一条信息，也不支持消费组
* Hash是压缩列表或哈希表
* * 哈希类型元素小于512个，每个都小于64字节，使用压缩列表
* * 其他用哈希表
* * redis 7.0之后压缩列表被listpack取代了
* * 应用场景
* * * HASH类型像用户ID、属性、值
* * * 一般对象用string和JSON，频繁变化的用Hash存储比如购物车
* * * 添加商品HSET cart:{用户id} {商品id} 1
* * * 添加数量HINCRBY cart:{用户id} {商品id} 1
* * * 商品总数HLEN cart:{用户id}
* * * 删除商品HDEL cart:{用户id} {商品id}
* * * 获取所有商品 HGETALL cart:{用户id}
* * * 里面存储的是商品id，还需要另外的查询id对应的完整商品信息
* Set是哈希表和整数集合
* * 元素个数小于128个，每个都小于64字节，用整数集合
* * 其他用哈希表
* * 7.0以后压缩列表被listpack取代
* * * Set适合去重，计算集合的交集、差集、并集
* * * 差集、并集、交集计算复杂度高，容易阻塞
* * * 点赞，保障用户只点一个赞
* * * SADD article:1 uid:1
* * * SREM article:1 uid:1
* * * SCARD article:1
* * * 共同关注：SINTER，求交集
* * * 抽奖：重复可以用SRANDMEMBER，不重复用SPOP
* Zset用压缩列表或跳表
* * 和Set一样，比set多一个排序属性score
* * 一个key对很多个value，每个value一个score
* * * 可以计算并集和交集，没有差集，ZUNIONSTORE，ZINTERSTORE
* * * 排行榜，用点赞数做score，ZSCORE查看score
* * * ZREVRANGE倒序获取有序集合key从start下标到stop下标的元素
* * * ZRANGEBYSCORE返回集合中指定分数区间内的成员，从低到高排序
* * * 电话、姓名排序ZRANGEBYLEX - + 或者[start (end代表开闭区间、ZREVRANGEBYLEX，
* Bitmap
* * 底层用string实现，保存成二进制字节数组
* * SETBIT key offset value value只能是0或1
* * GETBIT key offset
* * BITCOUNT key start end
* * 可以用BITOP AND/OR/XOR/NOT result key1 keyn...
* * BITPOS key value返回key中第一次出现value的位置
* * * 签到统计：SETBIT uid:sign:100:202206 2 1记录6月3号已签到
* * * GETBIT获取是否签到，BITCOUNT uid:sign:100:202206获取六月签到次数
* * * 可以用BITPOS获取首次打卡时间
* * * 判断用户登录状态
* * * 统计连续签到用户总数
* * * 用BITOP AND来把用户七天的打卡情况统计起来，再用BITCOUNT来统计总数
* * * 第一步对于所有用户统计每一天的bitmap，一亿个用户也就12MB内存
* HyperLogLog
* * 提供不精确的去重计数
* * 标准误差率是0.81%
* * 每个HyperLogLog键只需要12KB内存，可以计算2^64个不同元素的基数，非常省空间
* * 就三个命令
* * * PFADD key elemnent [element ...]添加指定元素
* * * PFCOUNT key [key ...]统计基数估算值
* * * PFMERGE destkey sourcekey [sourcekey ...]合并多个HyperLogLog
* * 应用就是百万级网站的UV计数，把uid就加进去，统计即可，有误差
* GEO用ZSET实现
* * GEOADD key longitude latitude member
* * GEOPOS key member返回所有指定名称的位置
* * GEODIST key member1 member2返回距离
* * GEORADIUS key longitude latitude radius获取指定范围内的地质位置集合
* * 滴滴叫车，可以用这个来确定范围内的车
* Stream
* XADD插入消息，保证有序，自动生成全唯一ID，毫秒数和毫秒内的序号
* XLEN查询消息长度
* XREAD用于读取信息，可以按ID读取数据，XREAD STREAMS mymq ID从ID开始读取后面的所有消息
* * 阻塞读要用XREAD BLOCK time STREAM mymq超时就返回
* XDEL根据消息ID删除消息
* DEL删除整个stream
* XRANGE读取区间消息
* XREADGROUP按消费组形式读取消息
* XPENDING查询消费组内所有消费者是否已读取但尚未确认的消息
* XACK用于向消息队列确认消息处理已完成
* XGROUP创建消费组
* * XGROUP CREATE mymq group1 0-0从第一条消息开始读取
* * XREADGROUP GROUP group1 consumer1 streams mymq > 从第一条尚未被消费的消息开始读取
* * 消息队列中的消息一旦被消费组里的一个消费者读取了，就不能被该消费组内其他消费者读取了
* * 如果不同消费组开始读取消息的位置一样，那么不同消费组可以消费同一条消息
* * 消费者可以用XPENDING查看已读取但是没有确认处理完成的消息
* Stream的消息队列
* * 中间件也就是Stream可能会丢消息，因为如果AOF刷盘不及时，就丢了，主从复制可能丢
* * 如果消息太多，stream有内存上限，超过了就删除旧消息，因此是有可能丢失的
* * 发布订阅机制不用来做消息队列，因为其没有基于数据类型实现，不能持久化，发后既忘无法消费历史消息，消费端有消息积压的时候，超过32M或者60s内保持在8M以上就强行断开
### Redis数据类型具体实现
* redisDB指向dict，dict指向两个dictht，一般用第一个，rehash采用第二个
* dictht指向一个dictEntry*，这是个哈希表节点的结构，里面存放了哈希表数组，数组的每个元素是指向哈希表节点结构的指针
* dictEntry就是哈希表节点的结构，存放了void* key和void* value，key是string，value不确定
* 这些数据类型是一个redisObject结构，有type、encoding、void* ptr三个属性，其ptr指向底层数据结构
* SDS
* * C语言字符串长度获取是O(n)，二进制数据中间也有\0，并且不会记录自身缓冲区大小
* * SDS记录了长度len，alloc分配的空间长度，flags表示不同类型SDS，buf[]字符数组
* * 这样长度是O(1)并且二进制安全，由于alloc缓冲区不会溢出
* * 扩容规则是：所需的sds长度唱过1MB，就翻倍扩容即2倍的newlen，小于1MB就扩容为newlen+1MB（newlen是旧长度和新的需要的长度之和），如果够了就不分配
* * sdshdr5、sdshdr8、sdshdr16、sdshdr32、sdshdr64是len和alloc数据类型不一样，节省空间
* list
* * 双向链表，底层listnode也是自己实现的
* * 在list里还有dup、free、match方法，并且维护了len长度
* * list还维护了head和tail，节点的值类型是void*可以通过方法设置不同的match、dup、free
* * 链表无法很好利用CPU缓存，并且由于都有头，内存开销比较大
* 压缩列表
* * 是节约内存开发的
* * 是由连续内存块组成的顺序型数据结构，类似于数组
* * 头有三个字段
* * * zlbytes，记录压缩列表占用内存字节数
* * * zltail，记录压缩列表尾部节点距离其实地址多少字节
* * * zllen，记录压缩列表包含的节点数量
* * 尾部有zlend，标记列表的结束点，固定值是0XFF
* * 压缩列表不适合保存过多的元素
* * 节点构成
* * * prelen 记录前一个节点长度，用于从后向前遍历
* * * encoding记录当前节点实际数据的类型和长度，字符串、整数
* * * data记录实际数据
* * 根据数据大小和类型进行不同空间大小分配
* * 如果前一个节点长度小于254bytes，prelen空间为1字节
* * 如果前一个大于254bytes，prelen使用5字节
* * encoding大小和数据是字符串还是整数、以及字符串长度有关
* * 整数就1字节，字符串根据长度是1/2/5字节
* * 压缩列表需要连锁更新，因为前面的如果长度超了，后面的也需要重新分配空间
* * 压缩列表如果保存的元素增加或者变大都会重新分配内存，只适用与小数据，节点够连锁更新也还好
* Hash表
* * 链式哈希解决哈希冲突
* * 哈希表数组，大小、大小掩码、节点数量是其属性
* * 数组的每个元素是个指向哈希表节点的指针
* * 哈希表节点是一个key指针，union键值对中的值（不同类型），next下一个节点形成链表
* * 哈希冲突就用next指针指向下一个
* * 如果分配到同一个哈希桶，就用next串成单向链表
* * 如果链表太长了就不行了，redis进行rehash
* * * 把哈希表2空间分配成1的两倍大
* * * 把1的数据迁移到2中
* * * 释放1的空间，把2变成1，给新的2创建新的哈希表
* * * 迁移的过程就是rehash，如果表太大会阻塞
* * * 渐进rehash，在rehash期间，每次哈希元素进行增删改查的时候，redis除了执行对应操作，还顺序将哈希表1中索引位置的key-value迁移到哈希表2上
* * * 这样客户端全部访问仪表就把所有key-value都迁移了
* * * 这期间hash会在两个表中找，1没找到，就去2找
* * rehash触发条件：负载因子=哈希表已保存的节点数量/哈希表大小
* * * 当负载因子大于等于1，并且redis没有执行bgsave或者bgrewriteaof的时候进行rehash
* * * 负载因子大于等于5，不管有没有RDB和AOF都rehash
* 整数集合
* * 连续的内存空间，结构为encoding，length和数组
* * encoding决定了数组类型，contents数组无所谓
* * 如果新加入的元素比以前的大，比如原来16bits，后来32bits，那么就要升级
* * 把contents数组按照新元素扩充，将之前的元素转换成新的大小，然后放在应该在的位置
* * 不支持降级，只能升级
* 跳表
* * O(logn)复杂度查找，Zset用了跳表，是和哈希表组合使用的，但是哈希表只用来常数复杂度获取权重
* * 跳表是多层有序链表，空间换时间，每一层节点数量不一样，高层节点节点指向下层节点
* * 跳表数据结构：
* * * 元素值ele、权重值score、后向指针、level数组保存每层上的前向指针和跨度
* * * level数组就是有好几层的节点，其level数组就有好几个元素，前向指向下一个当前层节点，span记录跨度（跨过了几个节点）
* * * 利用跨度就可以知道节点的排位，从高层级也能直接得到
* * * 跳表中包括了头尾节点、长度、以及跳表的最大层数
```C
typedef struct zskiplistNode {
    //Zset 对象的元素值
    sds ele;
    //元素权重值
    double score;
    //后向指针
    struct zskiplistNode *backward;
  
    //节点的level数组，保存每层上的前向指针和跨度
    struct zskiplistLevel {
        struct zskiplistNode *forward;
        unsigned long span;
    } level[];
} zskiplistNode;

typedef struct zskiplist {
    struct zskiplistNode *header, *tail;
    unsigned long length;
    int level;
} zskiplist;
```
* * 跳表查询过程
* * * 从头结点最高层查起，逐一遍历每一层
* * * 如果在某一层判断权重等于，SDS类型数据小于，那就下一个
* * * 如果是权重小于那就会访问下一个节点
* * * 如果下一个是空，那就访问下一层
* * 跳表层数设置，相邻两层理想是2：1
* * redis创建的时候随机生成每个节点的层数，不严格限制2：1
* * 如果随机数小于0.25，增加一层，下一层再随机
* * 这样最大限高64，头结点直接64层或者32层，插入的时候如果有高层，从头直接找就行（logN复杂度）
* * 为啥不用平衡树，AVL红黑树
* * * 树不是很内存密集，节点个数设定上，跳表更灵活一些
* * * 范围查找的时候跳表比平衡术操作简单
* * * 算法实现难度上，跳表比平衡术简单一些
* quicklist
* * 双向链表和压缩列表的组合，链表的每一个元素是压缩列表
* * 添加的时候先看压缩列表放不放得下，不行再新建一个quicklistNode
* * 没有完全解决连锁更新问题
* listpack
* * 代替压缩列表，还没完全代替
* * 还是连续内存空间，还是不同编码方式保存不同大小的数据
* * 头部有两个字段，listpack总字节数和listpack元素数量
* * 节点是encoding，data和len，encoding和data的长度
* * 这样没有前一个元素的大小问题，就不用连锁更新了，因为前一个的大小变化不会影响后一个的编码方式
* * 同样还是可以额从后向前遍历的
### Redis线程模型
* Redis单线程指的是接受客户端请求、解析请求、进行数据读写操作、发送数据到客户端时主线程完成的
* redis会启动后台进行，2.6以前俩，处理关闭文件、AOF刷盘；4.0之后加了一个，lazyfree异步释放Redis内存
* 删除大key的时候，使用del命令会使得主线程阻塞，要用unlink
* 主线程会把BIO_CLOSE_FILE队列放要关闭的任务，BIO_AOF_FSYNC放要AOF刷盘的任务，BIO_LAZY_FREE放要释放内存的任务，后台线程从队列里面取出任务，进行处理
* 主线程再6.0以前的模型：
* * 注册I/O多路复用epoll，把socket都注册进去
* * epoll_wait等待，发现任务以后进行分发
* * 连接事件用accept，再注册socket
* * 读事件用read，解析命令、执行命令、添加到发送队列，执行结果加入缓存
* * 写事件，调用write，如果一轮没法玩，注册一个写事件函数到epoll，epoll发现可以写的时候下一轮接着发（每次循环都会写，处理epoll发现的事件后要写一轮）
* Redis使用单线程模型是因为CPU不是Redis性能表现的瓶颈所在，更多情况受内存和网络I/O的限制
* 并且可维护性高，多线程可能增加程序复杂性、同时存在线程切换、加锁、解锁等带来的性能损耗
* Redis6.0之后的多线程模型
* * 采用多个I/O线程处理网络请求，因为网络I/O硬件设备的提升使得其瓶颈可能再网络I/O上
* * 可以将I/O性能提高1倍以上
* * 推荐4核CPU，2-3个I/O线程（主线程也算）
* * redis 6.0之后默认额外开启6个线程，前三个一样：关闭文件、AOF刷盘、Lazyfree，还有三个io_thd分担网络I/O压力
### Redis为什么这么快
* redis都在内存中完成，采用了高效的数据模型
* redis使用单线程模型，避免了多线程之间的竞争，省去了线程切换的时间和性能开销
* redis使用I/O多路复用机制处理大量的socket，
### Redis持久化
* AOF日志
* * 每执行一次写操作之后，就把命令以追加的方式写入一个文件里
* * \*3的意思是命令有3个部分，$3是这个部分有几个字符
* * 先执行命令再写入日志到数据库
* * * 这样可以避免额外的检查开销，如果是错的就不用写日志了
* * * 不会阻塞当前的写操作
* * * 但是数据可能会丢失（写日志之前断电）
* * * 可能阻塞其他操作（主线程写AOF文件，还是会阻塞后续操作）
* * AOF写操作有三种写回策略
* * * always：总是，每次操作之后都刷盘
* * * everysec，每秒把写命令输刷盘
* * * No，redis不管，操作系统决定啥时候刷盘
* * AOF重写
* * * 如果AOF文件太大了，就会重写
* * * 读取每一个键值对，把其写为一个AOF命令，用新的代替旧的文件
* * * 使用后台子进程bgrewriteaof来实现，这样可以不用阻塞主进程
* * * 子进程有数据副本，并且由于cow，父子进程数据安全，其副本不会变，父进程还可以继续操作
* * * 如果父进程太大，创建子进程可能会阻塞；父进程对大key写的时候可能会阻塞
* * * 主进程执行期间的把命令写道AOF缓冲区，并且写到AOF重写缓冲区
* * * 完事了把新的覆盖旧的就行了（期间还是要写旧的，防止失败）
* RDB快照
* * 把某一时刻的内存数据以二进制写入磁盘
* * 有save和bgsave，前者阻塞，后者不阻塞
* * 可以用命令save num1 num2来定期执行，num1秒内对数据库至少进行了num2次修改就快照
* * 子进程进行RDB的时候还是可以操作的
* 混合持久化
* * 再AOF文件重写的时候，fork的子进程会先RDB写入AOF文件，然后主线程处理的操作命令会被记录再重写缓冲区，写完以后以增量的形式写在AOF后面
* * 之后AOF文件就变成了前一部分是RDB，后面是AOF
* * 优点：可以结合RDB的优点，快速启动，又可以结合AOF有点，丢失少
* * 缺点：AOF文件加入了RDB可读性差，兼容不了redis4.0之前的版本
* 大key对持久化的影响
* * 对AOF
* * * 如果是always，那么写大key的时候会阻塞，磁盘写慢
* * * 如果是everysec，那么异步执行，不会卡主进程
* * * 如果是No那就无所谓了
* * 对于AOF重写和RDB的影响
* * * 如果页表很大，那么fork的时候会阻塞
* * * 写中断的时候，也就是父进程对大key做了修改，如果太大，物理内存复制耗时会阻塞主进程
* * * Linux开启了内存大页影响redis性能，因为redis写中断复制内存的时候一个页至少2MB
### Redis集群
* redis主从复制
* * 主服务器可以读写，从服务器只有读取，
* * 由于从服务器复制过程是异步的，因此不是强一致性
* * replicaof(5.0之前是slaveof)命令，后面跟ip地址和端口号
* * 从服务器建立链接，发视频美好psync命令，包含主服务器的runID和复制进度offset
* * 主服务收到以后响应复制方式，全量还是增量
* * 然后全量就后台bgsave，然后发送RDB文件，发的时候的写命令保存在repalication buffer里
* * 发完把replication buffer的命令发给从服务器，后面的写操作也发过去
* * 从服务器收到RDB以后把数据清空，直接读RDB
* * 命令转播是长连接，避免TCP握手开销
* * 生成RDB会忙于fork可能阻塞，传输RDB会占带宽，从服务器可以被replicaof
* * 增量复制，如果网络断开了，后来有连上了
* * 那么从服务器会发送psync，主服务器检查repl_backlog_buffer里面是否还有offset的数据了
* * 如果有就continue，增量复制，把缓冲区里的发过去，没有就全量复制
* * 主服务器复制的时候不仅会写命令发送给从服务器，还会写到repl_backlog_buffer里
* * 发的时候repl_backlog_buffer里的数据放到replication buffer里去发送
* * 为了避免网络中断引起的全量同步，可以把repl_backlog_buffer设大一点
* * 主节点每10s给从节点发ping，从节点每1s给主节点发replconf ack{offset}上报偏移量
* * 过期的key主节点发一个del过去
* * repl_backlog_buffer这个是只有一个缓冲区，满了就覆盖起始位置
* * replication buffer是一个从节点一个，满了就断开连接，删除缓存，重新连接就全量复制
* * 主从复制是异步的，可能会不一致，为此需要保障网络良好，并且可以使用外部程序监控复制进度
* * 主从切换可能导致数据丢失，调整延迟的时间间隔，脑裂哨兵（少于x个，延迟大于x秒不干活）
* 哨兵模式
* * 哨兵可以监控主从节点的故障，从而控制其故障转移
* * 哨兵每1s给所有主从节点发ping，判断是否正常运行，没有pong就标记为主观下线
* * 哨兵发现以后会发起投票，如果收到的主观下线大于quorum（配置文件）了，那么哨兵就认为客观下线了
* * 投票的时候只能投一次，候选者拿到的票大于N/2并且大于quorum就是leader
* * * 客观下线了之后，哨兵的leader在已下线主节点的从节点中挑一个从节点，升级成主节点
* * * 选的时候看网络情况，然后就是优先级、复制进度、ID号，发送SLAVEOF no one
* * * 发了命令之后，开始每秒一次发INFO命令，看它是否升级成主节点了
* * * 发现成主节点了，就让其他从节点以这个节点为新的主节点，SLAVEOF newID port
* * * 将新的主节点的IP和信息以发布/订阅机制发送给客户端，哨兵向某一个频道发布新的IP地址和端口
* * * 继续监听主节点，当旧的主节点重新上限的时候，将其设置为新主节点的从节点
* * 哨兵集群的构建只需要设置主节点的名字、主节点的IP和端口号以及quorum值
* * 这是因为哨兵会把自己的ip地址和端口号发送到__sentinel__:hello频道上，不同的哨兵从这个频道直接获取哨兵的IP和端口，建立连接
* * 哨兵每10s向主节点发一个INFO，主节点返回节点列表，哨兵就可以发INFO给从节点
* 切片模式
* * 一个服务器无法处理，就用多个服务器，对数据库进行切片
* * redis cluster方案中，一个切片集群有16538个哈希槽
* * 根据key得到一个hash值，然后模16538，得到映射
* * 哈希槽的分配可以是平均或者手动指定等
* 脑裂
* * 主节点和哨兵失联，哨兵认为其挂了，新指定一个节点
* * 但是主节点还是可以更客户端通信，于是还在读写
* * 这时候新选了一个主节点，就有俩主节点了
* * 这样重新连回来的时候就会把数据覆盖掉了
* * 解决方法：必须有至少x个从节点可以通信才写数据，t时间内必须有x个回信
* * 没法完全解决，需要抽屉原理才行
### Redis过期删除策略
* Redis过期删除策略和内存淘汰策略是不一样的
* * 过期删除是对于一个key，可以设置一个过期时间，过期的key需要删除
* * 过期字典是一个哈希表
* * 删除策略是Redis会把带有过期时间的key放在过期字典里，惰性删除加定期删除
* * * 惰性删除就是访问的时候再查看key是否在过期字典里，如果在就看是否过期，如果过期了就删除
* * * 惰性删除的优势是CPU友好，不需要定期轮询，只有访问的时候才检查
* * * 缺点是对内存不友好，如果一个key过期了又一直不访问，就一直在内存里，占内存
* * 定期删除是每隔一段时间随机从数据库中取出一定数量的key，删除其中过期的
* * * 具体是从过期字典里随机抽取20个，删除过期的key，如果过期的key大于5个（25%）就继续抽20个
* * * 优势是可以删除过期的不被访问的key，同时通过随机20个，可以减少操作对CPU的影响
* * * 缺点是难以确定具体的时间长度和频率，如果时间长了，占内存，时间短了占CPU
* * * redis每秒进行10次定期删除，10Hz
* * * 定期删除循环不超过25ms，防止循环过度线程卡死
* 持久化时过期键
* * RDB文件生成的时候会看是否过期，过期的不生成
* * RDB文件读取的时候看是不是主服务器，主服务器会判断，从服务器不会，不过主从复制的时候从服务器会清空数据
* * AOF文件持久化的时候如果过期了，AOF还会持有，删除的时候会在后面追加一个DEL
* * AOF重写阶段不会写入过期的键
* 主从模式对过期键的处理
* * 从库不处理，依靠主库的AOF文件
### Redis内存淘汰策略
* redis最大内存在redis.conf里通过maxmemory设置，64位默认是0，不限制，32位默认是3G（交给操作系统来限制内存）
* noeviction 3.0之后的默认模式，不会淘汰内存
* volatile-random 随机淘汰设置了过期时间的任意键
* volatile-ttl 优先淘汰更快过期的键
* volatile-lru 淘汰所有设置了过期时间的键里最久未使用的 3.0之前默认
* volatile-lfu 淘汰所有设置了过期时间的键中使用次数最少得，4.0之后新增
* allkeys-random 所有键随机
* allkeys-lru 所有键最久未使用
* allkeys-lfu 所有键最少使用次数
* LRU问题：链表数据空间的占用，数据被访问的时候需要移动链表，性能占用
* Redis LRU算法，是给每个对象结构体加上了一个最后访问时间，内存淘汰的时候随机访问5个，淘汰最久未使用的那个
* LRU无法解决缓存污染的问题，一下读了太多数据，这些数据在内部停留很久
* Redis中，LRU算法下对象头的24bits是用来记录key访问时间戳的
* LFU算法下，对象头24bits分成两个部分，前16bits是访问时间戳，后8bits是访问次数
* 访问字数不是单纯的访问次数，而是访问频次，logc会随着时间而衰减
* 每次访问key的时候会先做一个衰减运算，衰减值跟前后访问时间差有关系，差值越大衰减值越大
* 衰减之后会增加，增加也是根据概率增加的，logc越大的key，logc越难增加
* lfu-decay-time控制衰减，越大越慢，lfu-log-factor越大增长越慢
### Redis缓存设计
* 缓存雪崩
* 如果有大量请求没有命中，到达后端数据库，会导致数据库压力骤增，严重的会宕机形成一系列连锁反应，造成整个系统崩溃
* 原因可能是：大量的key同时过期，热点数据过期（缓存击穿），大量不存在的key访问（缓存穿透），redis宕机
* 解决方案：
* * 设置在过期时间上加上一个随机偏移量，或者设置成不过期
* * 对于缓存击穿，可以使用互斥锁方案，保障只有一个业务线程请求后端资源，其他阻塞；或者热点数据不过期，后台定期更新；或者热点数据快过期了就通知后台数据更新缓存，更新过期时间
* * 对于缓存击穿，可以在API入口处限制判断缓存请求参数是否合理，发现有穿透的现象给其加一个空值，利用布隆过滤器快速判断是否数据不存在
* * 如果redis宕机，可以用服务熔断或请求限流机制，直接返回错误
* * 建立高可用redis集群
* 动态缓存热点数据策略
* * 通过数据最新访问时间来做排名，并过滤掉不常访问的数据，只留下经常访问的数据。
* * 利用Zset构建排序队列，根据访问时间更新队列信息，越近访问越靠前
* * 系统定期过滤掉最后两百个商品，再从数据库随机读200个
* * 每次请求到达的时候先从队列里获取商品ID，如果命中根据ID从另一个缓存结构读取实际商品信息
* * Redis可以用zadd或者zrange完成排序队列和获取200个商品的操作
* 缓存更新策略
* * cache aside
* * * 读：没有就从数据库读，更新缓存
* * * 写：先写数据库，再删缓存
* * * 适合读多写少，无法保证缓存一致性
* * * 如果先删缓存，再更新数据库，可能会再删除缓存之后，更新数据库之前读到旧值，更新缓存，导致不一致
* * * 先改后删也有可能，可能读到旧值，更新数据库，删除缓存，然后又旧值写回缓存
* * * 延迟双删，也就是说先删一次，等一会再删一次
* * * 如果删除失败了咋办
* * * 重试机制：引入消息队列，把删除消息加入队列，删除失败就重新读消息，超时就报错
* * * 订阅MySQL binlog再操作缓存，binlog操作成功了，会拿到消息，然后再执行缓存删除
* * * lease的使用方式：
* * * 1. 缓存miss就给分配一个lease，如果数据被删除，那么lease也失效，这样就不会不一致了。因为后续写入或被阻止
* * * 2. 惊群，热门key的写入会导致缓存大量不命中（缓存穿透），每个key 10秒一个lease，拿到的可以和数据库要数据，其他的只能等着或者拿过期数据
* * write through
* * * 用户只和缓存打交道
* * * 读：没有就缓存自己从数据库读，更新缓存
* * * 写：改缓存，缓存写数据库
* * * 双写一致性，redis和memcache没有和数据库打交道的功能
* * write back
* * * 读：没有就从数据库读，更新缓存
* * * 写：改缓存，标记为脏
* * * 分布式场景没法用
* * * 双写是保证不了一致性的，但是如果对于缓存命中要求高的话，可以通过分布式锁来保证操作的顺序，或者加上较短的过期时间

### Redis实现延迟队列
* 利用Zset
* Zset的Score设定成时间，然后通过Zadd加入内存生产消息
* 在利用zrangebyscore查询符合条件的所有待处理任务，通过循环执行任务即可
### Redis大key如何处理
* 大key是value大的key
* string类型值大于10kb，或其他的大于5000个
* 大key的问题
* * 客户端超时阻塞，也能为处理大key费时间
* * 引发网络阻塞，流量太大，大key带来大流量
* * 阻塞工作线程，del大key会阻塞
* * 内存分布不均，集群模式在slot分片下，出现数据和查询倾斜问题
* 找大key的方法
* * redis-cli --bigkeys，在从节点上好，因为会阻塞，只能返回每种类型最大的key，对于集合类型只统计个数，不统计真实大小
* * 使用SCAN命令
* * * 对于string直接用STRLEN
* * * 对于集合类型，如果知道平均大小（业务层面），知道集合个数就行了，LLEN、HLEN、SCARD、ZCARD
* * * 不知道的话用MEMORY USAGE，查询键值对的内存空间（4.0版本）
* * 用rdbtools查询，通过解析redis rdb文件来找
* 删除大key的方法
* * 删除需要释放内存，一下释放太多会让空闲内存链表操作很久，导致主线程阻塞
* * 分批次删除：
* * * 对于HASH，用hscan，每次获取100个，然后hdel删除1个
* * * 对于LIST，用ltrim每次少删一点
* * * 对于大SET，用sscan，每次srem一个键
* * * 对于大ZSET，zremrangebyrank，每次删除一部分
* * 异步删除
* * * 用unlink来代替del
* * * 开启lazyfree-lazy-(eviction, expire, server-del, flush)机制，对于内存满了、过期了、server处理已存在的键、从节点复制，不是del而是lazyfree
### Redis管道有什么用
* 客户端批处理技术，一次处理多个redis命令
* 解决多个命令执行时的网络等待和redis没啥关系
### Redis事务支持回滚吗
* 没有回滚机制
* DISCARD是放弃事务，把暂存的命令队列清空
* redis并不一定保证原子性
* 作者认为这种错误大多是编程错误，支持回滚性能代价太大了
### Redis怎么实现分布式锁
* set命令的NX参数，表示KEY不存在才插入
* 加上EX PX可以表示过期时间
* 锁需要区分不同客户端，因此需要设置一个unique_value来标识客户端
* SET lock_key unique_value NX PX 10000 
* 解锁就是del lock_key，需要保证是加锁的客户端
* 可以用Lua脚本保障解锁的原子性，只有value满足才解锁
* 优点：
* * 性能高效
* * 实现方便
* * 避免单点故障
* 缺点
* * 超时时间不好设置：太长了影响性能，太短了事务还没执行完
* * * 合理的设置时间：给锁设置一个守护线程，当快过期了，续约，主线程结束了销毁就行
* * 主从复制时异步的，导致分布式锁不太可靠
* * * 如果主节点宕机了，从节点也没同步到锁，那就有两个客户端能获得锁了
* redis解决集群模式下分布式锁的可靠性问题
* Redlock，官方推荐至少5个主节点，互不相关
* 客户端与这些节点分别要锁，只要大于一半就成功了
* 首先获取时间t1，然后向N个节点加锁，需要设置等待超时时间
* 一旦获得了一半以上N/2 + 1个节点的锁，计算一个当前时间t2
* 如果t2-t1大于锁的过期时间，就加锁成功
### redis发布订阅机制
* 发布者（客户端）使用pubulish向某个频道发布消息
* 订阅者（客户端）通过subscribe订阅某个频道
* 具体实现有一个列表，里面是channel，每个channel后面有一个链表，表示订阅了的客户端
* 基于模式的发布订阅实现的时候有一个pubsub_patterns属性，这个是一个链表，保存了所有和模式相关的信息，每个模式底下都会放一个列表装这个模式对应的client