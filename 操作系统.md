## 操作系统知识点
### CPU与Cache
* Memory hierarchy: L1和L2是CPU独占的，L3是CPU共享的。
* 读写速度：L1 - 2~4 ticks、L2 - 10~20 ticks、L3 - 20~60 ticks、DRAM - 200-300 ticks、SSD - 2000-300000 ticks、机械硬盘 - DRAM的10万倍
* Cache一次加载64 bytes的数据
* 多个内存块对应同一个Cache位置，通过组标记来确定
* Cache有组标记和数据块两个部分，内存地址有组标记、索引、偏移量，来Cache里找数据
* Linux线程绑定CPU：sched_setaffinity方法
* Cache写回：先写到Cache里，如果Cache被替换了，再写回内存
* * 发生写操作时，首先判断是否在Cache里，如果在，则直接写，标记为脏
* * 否则就先看那个位置的Cache是否为脏
* * 脏的话就写回内存，再读取数据放在Cache里，再写，再标记为脏
* * 否则直接读数据，写，标记为脏
* Cache读的时候，考虑到写回，如果在Cache直接读；否则判断当前Cache是否是脏，如果脏要先写回内存，再从内存读。
* 总线嗅探：某个CPU改变了Cache内的值，需要通知其他CPU
* 不同的CPU核直接要保证缓存一致性：MESI
* * Modify、Exclusive、Shared、Invalidated
* * M：读：不变；写：不变；远读：先写自己的，变S；远写：先写自己的，变I
* * E：读：不变；写：不变；远读：变S；远写：变I
* * S：读：不变；写：变M；远读：不变；远写：变I
* * I：读：如果别的核心没有这个Cache，则从内存读，变E。如果其他有，且为M，则M的Cache先写进内存，本地再读，再都变成S。如果其他有，且为S或E，则本地从内存读，都变成S；写：如果其他核心没有这个Cache，则从内存读，再写，变M。如果其他核心有，且为M，则其他核心先写如内存，本地再从内存读，再写，本地变M，其他变I。如果其他核心有，且为S或E，则从内存读，再写，再变M，其他变I；远读：不变；远写：不变
* 伪共享：两个变量a和b都在一个Cache里，但是在不同的CPU里，修改A的时候会让B也失效，修改B的时候会让A也失效。
* Linux解决方法：__cacheline_aligned_in_smp 在CacheLine中对齐，也就是让a和b在不同的cache中，就可以解决了
* JAVA Disruptor的RingBuffer，用8个Long填充，使得不管怎么Cache，这些不用改的变量都不会被其他经常改的变量替换出Cache
* Linux线程和进程是一样的struct task_struct
* 调度方式：Deadline、Realtime、Fair
* * 前两种是根据优先级来，Deadline根据里Deadline近的优先，他俩优先级可以插队，
* * 第三种常用，虚拟执行时间vruntime，选择vruntime小的任务，vruntime=运行时间 · C · nice / 权重
* * CPU先执行前两种，再执行后面的，nice值是-20到20，都是普通任务。
* * 前两种优先级0-99，普通100-139
* 软中断：Linux为了中断处理快，把中断分为上下两个部分，上部分快速处理，下半部分延迟处理；上半部分屏蔽中断，下半部分不屏蔽；
* 查看软中断top后按1；查看软中断使用次数：/proc/softirqs
* 硬件中断和软件终端：硬件中断是硬件向CPU发出的中断，有固定的编号和硬件中断处理程序。软件终端是操作系统约定好的（没有也可以，通过中断向量表），由用户程序可以发出软件中断，CPU进入内核态，将中断号作为参数给内核程序，内核根据终端号进行处理，并返回结果。
* 异常也叫软件中断
* 信号和中断的对比：
* * 信号和中断都是异步通信方式，暂停当前程序去执行处理程序，返回原来的断点，可以被屏蔽
* * 中断有优先级，有优先级的中断可以嵌套执行；信号没有优先级，信号处理程序是用户态执行的；中断处理程序是内核态执行的；中断处理比较及时；信号处理延迟长
* * 信号发出：通过系统调用或者一些硬件中断；接收到信号根据信号进行处理；

### 内存管理
* 虚拟内存：隔离进程所使用的地址空间，充分利用物理内存，可以使运行内存超过物理内存大小，可以通过标记位管理使用权限。
* * 使用虚拟内存的意义：隔离各个进程使用的内存空间，更加安全；扩大了可用的程序内存空间，由于局部性原理，可以只分配使用到的内存；简化编程人员并发控制的考虑；
* 内存分段：内存碎片、内存交换效率低（碎片导致需要经常swap一大块内存）
* 内存分页：把虚拟和物理内存空间切成一段段固定尺寸的大小，默认4KB
* 页表：CPU通过页表将虚拟地址转换为物理地址，在CPU中的MMU上完成
* * 页内会有内部碎片问题（实际需求不足一页）
* * 内存交换效率高：一次只要swap几页或者几十页
* * 页表项结构：全局页目录项PGD、上层页目录项PUG、中间页目录项PMG、页表项PTE、偏移量offset
* * TLB CPU内常用页表项的缓存
* * Linux实际是段页式，但是段基本没用，每个程序段都是从0开始
* 用户空间内存布局：代码段、数据段（初始化的全局变量或静态变量）、BSS段（未初始化的全局变量或静态变量）、堆、文件映射、栈
* * Linux中每个进程或线程都使用task_struct来管理，其中有mm_struct来描述内存结构
* * 线程和进程的区别在于是否共享地址空间，在Linux调度过程中是不区分的
* * 内核线程的mm结构指向NULL
* * 内核通过vm_area_struct描述虚拟内存区域，比如代码段、数据段、BSS等，通过双向链表连起来
* * 同时内核还实现了红黑树管理的VMA，查找效率高
* Linux内核内存空间
* * 1G中有896M直接映射区，减去偏移量都是物理地址ZONE_NORMAL（物理内存区域）
* * 其中由于历史原因，前16M让内核用来为DMA分配内存ZONE_DMA（物理内存区域）
* * ZONE_HIGHMEM高端内存（物理内存区域）
* * 内核内存中NORMAL映射区上面有个8M的空洞
* * 再上面是使用vmalloc的动态映射区，映射到物理内存的高端内存上
* * 在上面是永久映射区，建立与物理内存的长期映射关系
* * 在上面是固定映射区，可以自由映射到高端地址上，但是其虚拟地址是固定的，而物理地址可以改变，主要目的是启动时在内存模块加载之前使用内存
* * 最顶上是临时映射区主要用来从内核拷贝数据
* * 在64位系统中。顶上128T，最底下8T是空洞，然后64T直接映射区，1T空洞，32Tvmalloc映射区，1T空洞，1T虚拟内存映射区（物理页面描述符），在上面有空洞，然后512M代码段（也是减去一个值就是物理内存），然后空到顶
* malloc()：封装了sbrk和mmap，大于128kb的用mmap，小的用sbrk
* * 分配时会预先分配更大的空间：并且会在空间头上加一个16 bytes的描述
* * free sbrk中分配的内存，不会立刻立刻释放；mmap中的会立刻释放
* * sbrk分配的内存会产生内存碎片，mmap会频繁释放导致效率低下
* * free会根据内存段头部16 bytes的内存
* 内存回收
* * 后台内存回收kswapd（异步），直接内存回收（同步）
* * 内存阈值，页高阈值、页低阈值、最小阈值
* * 文件页和匿名页相比，文件页回收代价更小
* * 保护进程不被OOM，调整校准值oom_score_adj，-1000-1000
* NUMA，Non-uniform Memory Access
* * 每个node有自己独立资源，memory和IO等
* * 可以设置回收内存的时候是否选择只回收本地内存
* 预读失效
* * 预读：Linux读取文件的时候会使用预读机制，read 4 KB会读进来16 KB，MySQL也有预读，从磁盘加载页的时候会加载相邻的页，从而减少磁盘I/O
* * 如果使用LRU管理一个链表，预读失效会导致预读的页占据了LRU的前部
* * 利用区分冷热数据的方法
* * Linux使用活跃链表和非活跃链表，预读的页放在非活跃链表的头部，当其被真正访问，才会放在活跃链表头部
* * InnoDB使用LRU链表上的young区域和old区域来划分，预读放在old区域
* 缓存污染
* * 如果读一次就放到活跃链表或young区域的头部的话，大文件访问会把整个LRU活跃链表污染
* * Linux在内存页被第二次访问的时候才从非活跃链表中取到活跃链表头部
* * InnoDB在第二次访问后判断和第一次访问的时间间隔，如果小于一秒则还是old区域，如果大于1秒则升级到young区域
* Linux物理内存
* * 4KB作为物理内存页的大小：是磁盘块的整数倍，太小需要的管理数据结构占据内存太大，太大磁盘交换效率太低；
* * 匿名页的反向映射：释放物理页的时候要去找对应的PTE，结束绑定，没有反向映射的时候很费时，后来多了俩字段，总之就是关联了
* * 内存页有active列表和inactive列表，回收的时候先回收inactive链的末尾。这俩列表都有两个，分别对应匿名页和文件页，内存还有一个结构保存了不会被换出的页
* * 复合页，通过两个或更多个物理上连续的内存页组装成的页，首页对应的struct page中设置为PG_head，并保留一些复合页的信息：析构函数、分配阶、复合页的引用计数、反向映射个数；尾页都通过compound_head指向首页
* slad对象池
### 进程管理
* 并发：一段时间内多个进程在运行；并行：同一时间有多个进程在运行
* 进程状态：创建状态，就绪状态，运行状态，阻塞状态，结束状态
*  * 进程创建后加入就绪队列
*  * 被调度后进入运行状态，时间片用完回到就绪状态
*  * 运行时等待事件到达阻塞状态
*  * 阻塞状态事件完成进入就绪状态
*  * 运行完到结束状态
*  * 大量阻塞状态的进程，可能会换出到磁盘，等再次运行的时候再换入物理内存，因此存在阻塞挂起和就绪挂起两个状态
* 上下文切换：保存CPU寄存器和程序计数器
*  * 进程切换包括虚拟内存、栈、全局变量等用户空间的资源，还包括内核堆栈、寄存器等内核空间的资源
* 线程是进程中的执行流程，有独立的寄存器和栈
*  * 一个进程可以有多个线程，线程可以并发执行，线程可以共享地址和文件等资源
*  * 一个线程崩溃，会导致其所属进程的所有线程崩溃（JAVA不会）
*  * 线程创建速度比进程快，不用处理资源信息，终止时间也更快
*  * 进程切换比线程切换慢，线程不需要切换页表，TLB失效了
*  * 线程之间共享数据更快，因为共享内存和文件资源
*  * 线程上下文切换，如果属于同一个进程，则切换栈和寄存器，否则和进程一样
*  * 用户线程，内核线程，轻量级进程
*  * 用户级线程由用户程序调度，无需用户态和内核态的转换，速度快；但是没有操作系统的参与，如果发生系统调用而阻塞，则全部线程阻塞，另外没有打断线程的特权，而且时间片较少，协程
*  * 内核线程由操作系统线程一对一管理，系统调用的阻塞不会影响其他内核线程，CPU时间多；需要内核态和用户态的多次转换，创建销毁都速度慢
*  * 轻量级进程是内核支持的用户线程，一个进程可以有多个LWP，和内核线程一一对应
*  * 轻量级进程和用户线程可以有1：1，1：N，M：N三种形式和用户线程与内核线程的对应一样；1：1创建开销大，1：N系统调用阻塞，M：N充分利用资源
*  进程调度
*  调度时机：从就绪态到运行态；从运行态到阻塞态；从运行态到结束态；从阻塞态到就绪态（高优先级抢占）；
*  调度原则
*  * 提高CPU利用率，发生I/O事件时让出CPU
*  * 提高系统吞吐率，权衡长任务和短任务的完成数量
*  * 减小任务周转时间，不要让进程等待太久
*  * 就绪队列中进程等待的时间越短越好
*  * 交互式任务响应时间应当尽量短
*  多级反馈队列
*  * 设置多个队列，队列优先级由高到低，时间片越来越长
*  * 新进程放在第一个队列末尾，按照先来先服务原则
*  * 在第一个队列时间片没有完成就放到下一个，以此类推
*  * 高优先级队列为空才调度低优先级队列
*  * 兼顾了长短任务，有较好响应时间
*  * 如果程序运行的时候有更高优先级的队列入队，则停止当前运行程序，放到当前队列末尾，执行高优先级任务，轮到这个队列的时候也只会分配上次未完成的时间片 
*  进程通信方式
*  管道
*  * linux [ | ]表示一个管道，是匿名管道，通信是单向的，mkfifo可以创建一个命名管道
*  * 管道是内核中的一段缓存，pipe系统调用传入两个指针，其会返回两个文件描述符，父子进程通过fork复制管道进行通信。命名管道可以在不相关的进程之间通信
*  消息队列
*  * 消息队列是保存在内核中的消息链表，发送方和接收方约定好消息体的格式，每个消息体都是固定大小的存储块，进程读取了消息体，内核就删除。
*  * 消息队列通信不及时，并且不适合大文件传输，消息体有大小限制，并且存在用户态和内核态之间消息拷贝的开销
*  共享内存
*  * 把不同的虚拟地址映射到同一块物理地址上
*  * 进程之间产生冲突
*  信号量：整形计数器
*  * PV操作，实现进程之间的互斥和同步
*  * P操作给信号量减一，如果小于零，则资源已占用需要阻塞，如果大于等于0，则资源可使用
*  * V操作把信号量加上1，如果小于等于0，则有阻塞的进程，唤醒它们，如果大于0则没有阻塞的进程
*  * P操作在开始之前，V操作在结束之后，信号量初始化为1代表互斥信号量，保证只有一个进程在使用资源
*  * 如果初始化为0，是同步信号量，给读进程开始前进行P操作，写进程结束后进行V操作，那么读进程一定在写进程之后进行
*  信号
*  * 唯一的异步通信机制，用户进程可以对信号执行默认操作，或者执行自己定义的信号处理程序，或者忽略信号
*  Socket
*  * 设置通信类型为本地，绑定一个本地文件进行通信
*  多线程冲突
*  临界区：访问共享资源的代码片段
*  互斥和同步：互斥是同一时间只有一个线程在临界区执行，同步是在一些关键点上，线程之间需要互相等待与互通消息
*  锁
*  * 原子操作：要么不执行，要么都执行
*  * test-and-set，如果是，则设置为
*  * 忙等待锁和非忙等待锁，
*  * 自旋锁：一直等待，直到获取到；没有抢占式调度不可使用
*  * 互斥锁：没拿到锁就让出CPU
*  信号量
*  * PV操作，互斥初始化为1，同步初始化为0
*  生产者消费者问题
*  * 生产者把数据放在缓冲区内，消费者从缓冲区读数据，任何时刻只有一个生产者或者消费者访问缓冲区
*  * 既需要互斥又需要同步
*  * 三个信号量：互斥：Mutex初始化为1，同步：fullBuffer消费者询问缓冲区是否有数据，初始化为0；同步：emptyBuffer生产者询问缓冲区是否有空位，初始化为n
*  哲学家就餐问题
*  * 五个叉子，五个哲学家，需要两个叉子吃饭
*  * 如果每个叉子都一个信号量，执行P操作，则都卡在这里，死锁了
*  * 如果只有一个信号量，则可以吃饭，同时只有一个哲学家吃饭
*  * 让偶数哲学家先拿左边的，奇数先拿右边的，那么不会死锁
*  * 记录每个哲学家的状态，进餐、思考、想进餐，只有在左右邻居没有进餐的时候才能进餐
*  读者写者问题
*  * wMutex写操作互斥信号量，初始化为1；rCount读操作个数，初始化为0；rCountMutex对rCount的互斥修改，初始值为1；读的时候给wMutex上锁，所有读结束解锁；这是读者优先，会写饥饿
*  * rMutex读操作互斥信号量，初始化为1；wMutex写操作互斥信号量，初始化为1；rCount读操作计数，初始化为0，rCountMutex控制rCount互斥修改，初始化为1，wCount写操作计数初始化为0，wCountMutex控制wCount互斥修改，初始化为1；读的时候多一个p操作rMutex，写的时候要像读一样计数，如果是第一个写的要P操作rMutex；这是写优先，全写完才读，读饥饿
*  * 与读优先相比，加一个信号量，写需要先P操作这个信号量，读也需要再rCountMutex之前P操作这个信号量，结束之后V操作；这样，读操作可以多个进行，而写操作进来以后不会有更多的读操作进来，并且后续读操作和写操作还是按顺序的
*  死锁
*  * 两个线程互相等待对方释放锁，就会发生死锁
*  * 死锁的发生需要满足四个条件：
*  * 互斥条件：多个线程不能同时持有一个资源
*  * 持有并等待条件：线程A在等待资源2的时候不会释放自己的资源1
*  * 不可剥夺条件：在自己使用完之前不能被其他线程获取
*  * 环路等待条件：两个线程获取资源的顺序构成了环路
*  死锁检查工具：Java jstack；Linux pstak+gdb
*  * 观察调用堆栈卡住的位置
*  避免死锁发生
*  * 资源有序分配，获取锁的顺序固定
*  互斥锁有线程上下文切换的代价，如果临界区代码很短，使用自旋锁比互斥锁要好 
*  自选锁使用CAS，CAS是compare and switch，是原子指令，CAS是乐观锁，先锁，有问题再撤回
*  读写锁：读优先锁、写优先锁、读写公平锁，上面有
*  悲观锁，认为访问前要加锁；乐观锁，先改，有冲突在放弃
*  一个进程能创建多少个线程
*  * 32位系统，一个线程10M的话，3G就是300多个
*  * 64位系统，按理来说1000多万个，但是有系统限制
*  * 最大线程数14553，pid数32768，VMA最多65530个
*  线程崩溃了进程一定崩溃吗
*  * 线程由于内存地址共享，一个线程越界，那么就会产生内存不确定性，故让他们都崩溃
*  * 不过进程崩溃其实是通过信号，SGISEGV等信号让进程崩溃，因此可以通过注册信号处理函数来实现进程不崩溃
*  * JVM定义了自己的信号处理函数，因此线程崩溃了其他线程也不会崩溃
*  页面替换算法
*  * 最佳页面替换算法
*  * 置换在未来最长时间不访问的页面（理想算法，用来作比较）
*  * 先进先出
*  * 最久未使用，开销大，需要一个大链表
*  * 时钟页面替换算法：环形链表，如果访问位是0就替换，如果是1，就设为0，然后下一个
*  * 最不常用算法：访问计数器，代价太大
*  磁盘调度算法
*  先来先服务
*  最短寻道时间：实时性不好，会饥饿
*  扫描算法，一个方向完事再下一个方向，中间磁道好
*  循环扫描算法：一直向一个方向，到头再从头开始
*  LOOK和C-LOOK扫描和循环扫描的改进，到最远就反向，LOOK反向响应，C-LOOK反向不响应
### 文件系统
* inode
* * 保存在磁盘里，记录文件元信息，编号、大小、访问权限、创建时间、修改时间、磁盘位置等
* dentry
* * 记录文件名字、索引节点指针，其他目录层级关系，内核在内存中使用的数据结构
* 虚拟文件系统：操作系统为用户提供统一的文件接口：磁盘文件系统、内存的文件系统、网络的文件系统
* 文件描述符：操作系统为进程的文件表项分配的标识
* 文件存储方式：连续、链表
* * 连续有碎片问题；隐式链表有可能中间断了；显示链表不适用大磁盘
* 索引：在磁盘中创建索引，由索引指向数据块，也可以多级索引、链式索引
* Unix文件用三级目录，inode里有一级索引、二级索引、三级索引，Ext4做出了改动
* 空闲管理
* * 空闲表：小的空闲区问题
* * 空闲链表：不能随机访问，指针空间
* * 位图：二进制
* 磁盘结构：Ext2
* * 引导块，后面是多个块组
* * 块组内有超级块、块组描述符表、数据位图、inode位图、inode列表、数据块
* * 超级块记录了inode总数、块总个数、每个块的inode数、块数等；
* * 块组描述符表记录了每个块组的状态、inode数据、空闲等
* * 有全局信息重复，因为可以恢复、使文件和管理数据接近，减少磁头寻道和旋转
* * 后续版本使用了稀疏技术
* * 里面还有log用来恢复文件写入，commit之后的才会redo
* * 内核使用pagecache，write会先写在pagecache上，写磁盘的时候先写log，如果写完了会commit；这时候如果断电重启，会发现有commit的log，于是会redo文件写入
* * 如果没有commit，那么重启时不会redo
* * ext3提供异步系统调用、维护一些transaction的信息，可以把多个系统调用打包成一个transaction、并且提供并行可以同时多个系统调用
* ext文件系统用哈希表存储目录
* 硬链接多个目录项的索引节点指向一个inode，删除所有硬链接，才删除文件
* 软连接重新创建一个inode，但是这inode指向的是那个链接的inode，目标文件删除了也不会删除软连接
* 非阻塞I/O和异步I/O的区别是，非阻塞操作系统告诉用户程序数据准备好了，这时候用户程序开始处理，从内核态拷贝到用户态；异步则是操作系统直接拷贝到用户区域，用户程序再开始读
* 进程写文件，进程崩溃数据会丢失吗
* * pagecache操作系统为了减少磁盘I/O提供的cache，可以加速读写，
* * pagecache需要占据额外物理内存，使用DMA维护pagecache，DMA多做一次数据拷贝，命中率低的时候浪费，大文件缓存污染
### 网络系统
* DMA
* 如果所有数据传输都要通过中断来CPU处理的话，代价很大。
* DMA设备可以直接搬运数据，不需要CPU参与，DMA直接把数据从磁盘搬到内存。现在每个I/O设备都有DMA
* 零拷贝
* mmap+write
* * mmap直接把内核缓冲区里的数据映射到用户空间，就省去了内核态到用户态的数据拷贝，但是系统调用还是2次
* sendfile
* * sendfile直接把数据从内核缓冲区拷贝到socket缓冲区中，只有两次内核态和用户态的切换，三次数据拷贝
* * linux2.4之后，sendfile直接利用网卡的SG-DMA控制器把数据从内核缓冲区拷贝到网卡，只发送描述符和数据长度到socket缓冲区，这样只有一次系统调用，并且只有两次拷贝，拷贝都由DMA直接完成
* kafka和nginx都使用了零拷贝
* 大文件传输不适用pagecache，也就不用零拷贝，使用异步I/O和直接I/O结合的方式代替零拷贝
* 大文件pagecache预读失效，缓存污染，占据太多空间，因此使用直接I/O，并且用异步I/O解决阻塞问题
* 内核统一使用sk_buff结构体来表示各层网络包，其给头部留足空间，通过移动指针来增加或剥离协议头，这样就省去了多次拷贝的CPU效率损失
* 线程池：提前创建若干个线程，当新连接建立时，将已连接的socket放在一个队列里，线程池从队列里取出已连接socket进行处理，从而避免线程频繁创建销毁
* I/O多路复用，让一个进程处理多个请求，方法：select/poll/epoll
* select：将已连接的socket放在一个文件描述符集合中，调用select函数将文件描述符集合拷贝到内核中，内核通过遍历的方式检查是否有事件发生
* 内核发现有事件发生，就将其socket标记为可读可写，拷贝回用户空间，用户遍历集合，找到可读可写的socket
* select使用bitsmap表示文件描述符集合，其最大支持0~1023共1024个文件描述符
* poll不再使用bitsmap，而是使用链表，但是还是需要拷贝和遍历
* epoll使用红黑树来跟踪进程所有待检测的文件描述符，把需要监控的socket通过epoll_ctl加入内核红黑树中，一次只需要传入一个待检测的socket
* epoll采用事件驱动方式，内核里维护了一个链表记录就绪事件，某个socket事件发生，内核通过回调函数将其加入就绪事件列表中
* 用户调用epoll_wait函数的时候，只会返回有事件发生的文件描述符的个数
* epoll支持边缘触发和水平触发两种模式，边缘触发模式服务器端只会从epoll_wait中苏醒一次，水平触发模式服务器端不断从epoll_wait中苏醒，直到内核缓冲区数据被read函数读完结束
* 水平触发模式，没必要一次执行尽可能多的读写操作
* 边缘触发模式，要循环读取，如果文件读取是阻塞的，有可能就会阻塞在I/O操作上，所以边缘触发模式和非阻塞I/O搭配
* 边缘触发模式比水平触发模式效率高，使用I/O多路复用要搭配非阻塞I/O来使用
* 在连接少且都十分活跃的情况下，select和poll可能比epoll好，因为epoll通知机制需要很多函数回调
* reactor：对事件进行反应，来了一个事件，Reactor就有相应的反应
* reactor也叫dispatcher，I/O多路复用监听事件，收到事件后，根据事件类型分配给某个进程/线程
* reactor由两个部分组成，主要是reactor负责监听和分发事件；处理资源池负责处理事件
* 单reactor单线程、单reactor多线程、多reactor多线程（多reactor单线程没用）
* 单reactor单线程：reactor通过I/O多路复用监听事件，收到事件后进行分发，看是acceptor还是handler
* 缺点是无法利用CPU多核性能，handler处理的时候无法处理连接（单线程），业务如果较长就会造成相应的延迟，只适合业务处理很快的场景（redis 6.0之前）
* 单reactor多线程：reactor监听I/O多路复用，分发给acceptor或多个handler，handler分发给子线程处理业务，handler只接收发送
* 能利用CPU多核性能，但是单线程reactor容易成为性能瓶颈
* 多reactor多线程：主线程reactor只负责判断是建立连接还是业务逻辑，是业务逻辑就交给子线程，子线程有reactor，监听I/O多路复用，并且调用handler处理
* Netty和Memcache采用了多Reactor多线程的方式
* Proactor是异步网络模式
* 非阻塞还是要调用read，让内核拷贝数据到用户态；异步完全不需要，aio_read
* reactor是非阻塞网络模式，proactor是异步网络模式
* Linux对于本地可以aio_read，对于网络没有，windows有
* 一致性哈希
* 一版的哈希算法，如果节点数量发生了变化，那么在系统扩容或者缩容的时候，就要迁移映射关系的数据
* 一致性哈希是对2^32进行取模运算，进行两次运算，第一次对于存储节点计算哈希值，对数据进行存储访问时计算哈希值
* 将存储节点和数据都映射到一个首尾相连的哈希还上，映射值按顺指针找到的第一个节点，就是存储的节点
* 移除一个节点，只影响该节点在哈希环顺时针相邻的后继节点，一致性哈希不保证均匀
* 虚拟节点，将虚拟节点映射到哈希环上，再将虚拟节点映射到真实节点上
* 节点数量多了分布的就相对均匀了，删除节点会有不同节点共同分担系统的变化，还是尽量均匀的
* Ngnix使用的每个权重为1的真实节点有160个虚拟节点
* 使用netstat查看端口使用情况，-aptn，或者-tunlp | grep 端口号
* netstat -ntlp查看所有tcp端口
* 使用lsof -i :端口号，需要root权限
